Detection and Handling of Corner Cases in Autonomous Driving
==========

## List of Publications
##### \* denotes equal contribution

&nbsp;
&nbsp;
### WIP | Quantification of Actual Road User Behavior on the Basis of Given Traffic Rules
02/2022 • Daniel Bogdoll*, Moritz Nekolla*, Tim Joseph, and J. Marius Zöllner

[PDF]() | [arXiv]() | [Code]() | [Proceeding]()

:heavy_check_mark: Accepted at [IEEE Intelligent Vehicles Symposium (IV)](https://iv2022.com/)

<details>
  <summary markdown="span">BibTeX Citation</summary>
  
  ```
  @article{Bogdoll_Compressing_2021_NeurIPS,
      author    = {Bogdoll, Daniel and Jestram, Johannes and Rauch, Jonas and Scheib, Christin and Wittig, Moritz and Z\"{o}llner, J. Marius},
      title     = {{Compressing Sensor Data for Remote Assistance of Autonomous Vehicles using Deep Generative Models}},
      journal   = {NeurIPS Conference on Neural Information Processing Systems Workshop on Machine Learning for Autonomous Driving (ML4AD)},
      year      = {2021},
  }
  ```
</details>

<details>
  <summary>
    :red_circle: Single-Blind
    :orange_circle: Medium Quality Reviews
</summary>
  
### Reviewer 1

##### 
</details>

&nbsp;
&nbsp;
### WIP | ad-datasets: a meta-collection of data sets for autonomous driving
02/2022 • Daniel Bogdoll*, Felix Schreyer*, and J. Marius Zöllner

[PDF]() | [arXiv]() | [Code](https://ad-datasets.com) | [Proceeding]()

:heavy_check_mark: Accepted at [VEHITS International Conference on Vehicle Technology and Intelligent Transport Systems](https://vehits.scitevents.org/)

<details>
  <summary markdown="span">BibTeX Citation</summary>
  
  ```
  @article{Bogdoll_Compressing_2021_NeurIPS,
      author    = {Bogdoll, Daniel and Jestram, Johannes and Rauch, Jonas and Scheib, Christin and Wittig, Moritz and Z\"{o}llner, J. Marius},
      title     = {{Compressing Sensor Data for Remote Assistance of Autonomous Vehicles using Deep Generative Models}},
      journal   = {NeurIPS Conference on Neural Information Processing Systems Workshop on Machine Learning for Autonomous Driving (ML4AD)},
      year      = {2021},
  }
  ```
</details>

<details>
  <summary>
    :orange_circle: Double-Blind
    :orange_circle: Medium Quality Reviews
</summary>
  
### Reviewer 1

##### 
</details>

:x: Previous version rejected at [AAMAS International Conference on Autonomous Agents and Multiagent Systems](https://aamas2022-conference.auckland.ac.nz/)

<details>
  <summary>
    :orange_circle: Double-Blind
    :green_circle: High Quality Reviews
</summary>
  
### Reviewer 1

##### Overall recommendation	
-1: (weak reject)
  
##### Summary
The submission deals with a meta-collection of data-sets for autonomous driving, i.e., a collection of pointers to data-sets collected by others. In addition to the mere pointers, technical information on the 3rd party data-sets is provided, e.g., duration, size, sensors, etc.

##### Detailed comments
The submission deals with a meta-collection of data-sets for autonomous driving, i.e., a collection of pointers to data-sets collected by others. In addition to the mere pointers, technical information on the 3rd party data-sets is provided, e.g., duration, size, sensors, etc. This meta-collection has a lot of value to the community as it provides a fast and easy access to the many data-sets for autonomous driving that exist. But I am missing some clear academic insights that would motivate the publication of a paper on it at AAMAS. The submission is a mere description of the tool, which in addition is focused on basic technical parameters of the data-sets. What is lacking are academic aspects related to important research questions in autonomous driving. For example, how "flat" is the data-set, i.e., how much change in the z-coordinate, respectively roll and pitch is in there? How big is the "overlap" between consecutive sensor readings, i.e., the relation between sensor range, update rates, and vehicle speed? As mentioned, I consider the main idea and the contribution to the community as very relevant. It is definitely useful to have a white paper on it that can be cited, e.g., using arXiv. But I am not convinced that the submission is a good match for AAMAS.

##### Rebuttal question	

##### Reason for the recommendation
This collection of links to 3rd party data-sets generated by others is useful for the autonomous driving community, but it is not well suited as an AAMAS paper.

### Reviewer 2
  
##### Overall recommendation	
-2: (reject)

##### Summary
This paper describes the creation of a website to allow users to browse through all relevant autonomous driving datasets.

##### Detailed comments
It is difficult to assess this paper because the central offering of this paper is a website which we the reviewers do not have access to.

The website idea is indeed useful and can be kept up to date by the authors. However, due to the pace of AV research, this paper will likely be out of date within a year of being published. In my opinion, having the website as the sole contribution of this work is not enough to warrant publication. The paper could be improved by adding a detailed review of existing dataset papers and proposing future directions / needs in this area.

One important question in the field is to what extent datasets and benchmarks can be used as an indicator for progress in autonomous driving research. What is missing from current datasets? What could be done better? Is it better to rely on datasets or simulation environments? Machine learning algorithms assume that samples are IID (independent and identically distributed) and that the training distribution matches the test distribution, does this make sense in autonomous driving?

How can we decide which datasets are relevant to self-driving and which are not? There are datasets out there that were originally designed for unmanned ground vehicles, but could certainly be used to develop algorithms that would benefit autonomous vehicles. Should these be included? What about the older computer vision datasets that still have some relevance?

The authors should review the abstract and introduction and make the following improvements:
- Keep the abstract short and to the point, motivation can be condensed to 1 sentence here.
- In the introduction, the authors can condense their justification for the importance of autonomous vehicles to 1-2 sentences.
- I'm not sure I agree with the motivation. Do researchers need a tool to find datasets beyond simply using search engines themselves? Recent dataset papers like Waymo include tables that compare what they offer against other datasets, is this not sufficient? Moreover, there exist more general research tools like www.connectedpapers.com which allow users to find new papers based on a given paper as a query.

For the related work section, instead of summarizing other papers and websites that have tables or summaries of autonomous driving datasets, this paper's related work should summarize all the datasets out there itself. The summary could follow historical trends in datasets and/or be divided by task.

I'm not sure that the headings "approach" and "evaluation" are appropriate for those sections. Perhaps divide this into "selection process", "metrics", and "comparison".

"size" should not be it's own section, this information can be contained in a table.

I would like to see the number of tables and figures greatly expanded to include more statistics that help give the readers a better picture of what datasets are out there and how they compare.

##### Rebuttal question	

##### Reason for the recommendation
In my opinion, having the website as the sole contribution of this work is not enough to warrant publication. The paper could be improved by adding a detailed review of existing dataset papers and proposing future directions / needs in this area.

  
### Reviewer 3
  
#####  Overall recommendation	
-3: (strong reject)

##### Summary
The paper presents a tool that collects a number of datasets for self-driving cars that were identified through manual search over research papers and social media. Such a tool has been enriched with search and filtering capabilities.

##### Detailed comments
The paper discusses a potentially useful tool for performing research in the self-driving car space.

However, there are a number of major issues that makes the publication not suitable for AAMAS:
- the paper does not present any novelty, as the implementation is standard.
- it is not clear how the paper plans to keep the dataset up-to-date in a sustainable way: currently, the datasets have been added manually and the paper expects the community to contribute. Note that the paper should discuss clearly why previous datasets are not usable anymore.
- the number of properties are quite straightforward and, while important, they do not provide any insight on what datasets to use for testing.

The tool might be more useful if there is an actual automated analysis of the datasets that will provide insights on their diversity for example.

==== After rebuttal
Thanks for providing a response and recognizing the limitations -- please consider the comments to enhance the contributions. Thanks also for providing the link.

##### Rebuttal question	

##### Reason for the recommendation
As discussed in the detailed comments, while potentially a useful tool, what proposed in the paper does not display novelty, actual sustainable effort, nor actionable information.
  
### Meta Review
The paper describes the creation of a web site containing a list of data sets collected by others. This is helpful, but the list alone does not comprise enough content for this to be an acceptable paper. It is also unclear how the resource described provides a contribution to the community beyond existing web-based resources. As it stands, the analysis and value of the resource is limited. The domain is autonomous driving. The content is not sufficient for an academic paper and has limited value to the AAMAS community.
</details>

&nbsp;
&nbsp;
### Compressing Sensor Data for Remote Assistance of Autonomous Vehicles using Deep Generative Models
11/2021 • Daniel Bogdoll*, Johannes Jestram*, Jonas Rauch*, Christin Scheib*, Moritz Wittig*, and J. Marius Zöllner

<img width="100%" src="https://user-images.githubusercontent.com/19552411/150287374-0c4b3081-62b9-46f2-8d32-cb733cd7c762.png">

[PDF](https://arxiv.org/pdf/2111.03201.pdf) | [arXiv](https://arxiv.org/abs/2111.03201) | [Code](https://github.com/daniel-bogdoll/deep_generative_models)

:heavy_check_mark: Accepted at [NeurIPS Workshop on Machine Learning for Autonomous Driving](https://ml4ad.github.io/)

<details>
  <summary markdown="span">BibTeX Citation</summary>
  
  ```
  @article{Bogdoll_Compressing_2021_NeurIPS,
      author    = {Bogdoll, Daniel and Jestram, Johannes and Rauch, Jonas and Scheib, Christin and Wittig, Moritz and Z\"{o}llner, J. Marius},
      title     = {{Compressing Sensor Data for Remote Assistance of Autonomous Vehicles using Deep Generative Models}},
      journal   = {NeurIPS Conference on Neural Information Processing Systems Workshop on Machine Learning for Autonomous Driving (ML4AD)},
      year      = {2021},
  }
  ```
</details>

<details>
  <summary>
    :orange_circle: Double-Blind
    :orange_circle: Medium Quality Reviews
</summary>
  
### Reviewer 1

##### What is your overall impression? How does the paper align with machine learning towards autonomous driving?
The paper addresses the important problem of compressing sensor data from autonomous vehicles. They leverage pre-existing neural compression models involving VAEs and GANs. They further present a comprehensive study of the performance of these models for image data for downstream tasks such as detection.

The writing is clear and the experiments are convincing. The authors also analyse examples for various driving datasets. As such, the paper aligns really well with the topic of the workshop and will be of broad interest to the community.

##### Are the task, theory or method new? Is it clear how this work differs from previous work?
Yes

##### Does this address a difficult task in a better way than previous work?
Yes
  
##### Is the writing clear?
Yes
  
##### Are claims well supported by theoretical analysis or experimental results?
Yes
  
##### Are the authors honest about evaluating both the strengths and the weaknesses of the work?
Yes
  
##### Are others likely to use the ideas or build on them?
Yes
  
##### What could the authors do to improve clarity, theory, or experiment?
Figure 2 could be better presented with a finer x-axis. It will be useful to see where the inflection points occur for JPEG and the neural methods. Additionally, the experiments should be conducted for a larger section of the dataset and failure modes identified.

##### Your overall assessment?
Accept
  
### Reviewer 2

##### What is your overall impression? How does the paper align with machine learning towards autonomous driving?
  
The authors apply established VAE- and GAN-based models to compress image and lidar data in an automotive context. These approaches are compared with each other and with JPEG compression, considering different evaluation scores and visualizations. The paper is easy to understand and the presentation of (most of) the results is clear (for details see below).

They argue for the relevance in the field of autonomous driving (AD) as self-driving systems will not be perfect in the near future, thus rendering human assistance necessary in complicated traffic situations. Considering remote operators, large amounts of sensor data need to be transmitted. While this motivation for compression is overall plausible, one might further detail how such remote-assistance scenarios look like and which requirements they pose (see below).

##### Are the task, theory or method new? Is it clear how this work differs from previous work?
No
  
##### Does this address a difficult task in a better way than previous work?
No
  
##### Is the writing clear?
Yes
  
##### Are claims well supported by theoretical analysis or experimental results?
Yes
  
##### Are the authors honest about evaluating both the strengths and the weaknesses of the work?
Yes
  
##### Are others likely to use the ideas or build on them?
Yes
  
##### What could the authors do to improve clarity, theory, or experiment?
conceptual aspects:
- you could describe the circumstances/scenarios of "remote assistance" in more detail (will a remote operator actually take over during a critical situation? will the vehicle conduct an emergency stop and the remote operator will only take over once a safe state is reached? consider not only fps rates for practical feasibility, but also the latency introduced by sending a signal to the remote operator and back
- you could add safety-specific aspects to your evaluation, e.g. by paying special attention to critical image areas (e.g. driveable areas) and / or the detection of vulnerable road users

technical aspects:
- introduce abbreviations "SAE" and "VA"
- on p.4, l.141, make more clear that GANs will not be used for processing lidar data in subsection 4.2
- on p.5, l.180 it is stated that "JPEG is worse than the generative approaches", however, for not too small bit rates it yields competitive results (see Figure 2)
- better explain how Figure 4 is constructed (was a smoothing routine (e.g. KDE) applied to the relative-error histogram on the x-axis? are the values of the pdfs color-coded by shades of blue/orange? are there no relative-error values above 200%? relative-error distributions are not that easy to interpret: for n_orig = 1, n_recon = 0 leads to a relative error of -1, for n_orig=10, one missed object leads to only -0.1, i.e. the relative-error distribution encodes the objects-per-image distribution)
- on p.6, l.223: "more objects" instead of "more images"
- the sentence "the VAE approach was trained and tested on KITTI lidar data" (p.7, l.236) could be interpreted that separate models are used for image and lidar data compression, in the caption of Figure 1 it is however stated that "the pre-processed lidar data passes through the same encoder as RGB data"
- provide details on the CARLA data sample (seemingly) used in subsection 4.3. How do the KITTI-based plots in Figure 2 look like for this data set?

##### Your overall assessment?
Weak Accept
  
### Reviewer 3

  ##### What is your overall impression? How does the paper align with machine learning towards autonomous driving?
I do not think that there are significant contributions in this paper and the direction isn't clear. The paper applied existing, learning-based, compression methods onto autonomous driving RGB images and point cloud data, all of which are results that can be obtained from the original description of the work.
While the authors mention a potential application in remote assistance of autonomous vehicles, this idea wasn't clearly emphasized.

##### Are the task, theory or method new? Is it clear how this work differs from previous work?
No

##### Does this address a difficult task in a better way than previous work?
No

##### Is the writing clear?
Yes

##### Are claims well supported by theoretical analysis or experimental results?
Yes

##### Are the authors honest about evaluating both the strengths and the weaknesses of the work?
Yes

##### Are others likely to use the ideas or build on them?
No

##### What could the authors do to improve clarity, theory, or experiment?
Using learning-based methods for low-latency compression for teleoperated vehicles is an interesting topic. However, the ideas presented here are not novel and doesn't really add to the exploration of this topic. I recommend that the authors review other publications in this area, potentially focusing on some of the specific points (non-exhaustive) below:
1) Latency reduction (and comparison with classical/established video compression method)
2) Since the goal is for remote operation/assistance, there should also be some mention on acceptable latency
3) Bitrate reduction/robustness to packet losses/adaptive bitrates (again perhaps in comparison to established compression methods)

##### Your overall assessment?
Reject
</details>

&nbsp;
&nbsp;
### Description of Corner Cases in Automated Driving: Goals and Challenges
09/2021 • Daniel Bogdoll*, Jasmin Breitenstein*, Florian Heidecker*, Maarten Bieshaar, Bernhard Sick, Tim Fingscheidt, and J. Marius Zöllner

<img width="100%" src="https://user-images.githubusercontent.com/19552411/152381191-25bea065-3f4b-4fae-ac3c-76111cf9083f.png">

[PDF](https://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Bogdoll_Description_of_Corner_Cases_in_Automated_Driving_Goals_and_Challenges_ICCVW_2021_paper.pdf) | [arXiv](https://arxiv.org/abs/2109.09607) | [Proceeding](https://openaccess.thecvf.com/ICCV2021_workshops/ERCVAD)

:heavy_check_mark: Accepted at [ICCV Workshop on Embedded and Real-World Computer Vision in Autonomous Driving](https://www.ki-deltalearning.de/event/ercvad2021)

<details>
  <summary markdown="span">BibTeX Citation</summary>
  
  ```
  @InProceedings{Bogdoll_Description_2021_ICCV,
    author    = {Bogdoll, Daniel and Breitenstein, Jasmin and Heidecker, Florian and Bieshaar, Maarten and Sick, Bernhard and Fingscheidt, Tim and Z\"{o}llner, Marius},
    title     = {{Description of Corner Cases in Automated Driving: Goals and Challenges}},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    year      = {2021}
}

  ```
</details>

<details>
  <summary>
    :orange_circle: Double-Blind
    :orange_circle: Medium Quality Reviews
</summary>
  
### Reviewer 1

##### Comments to authors* Please assess the contribution of this submission (if any). Please provide comments to the authors to help them improve their work, or to explain your recommendation.
The authors want to show the challenges and goals of machine interpretable definitions of corner cases for automated driving by presenting six research questions.

Main feedback
- The contribution beyond state-of-the-art is not clear. Roughly 2.75 pages of 5 (excluding references) are filled with introduction and related work. Additionally, the remaining pages read like state-of-the-art since any valuable information is either a reference to other papers or unproven. The paper only contains general ideas without any indication on how to address the problems that need to be solved (e.g. how to keep the corner case description so flexible to represent any corner case, but rigid enough to be useful for machine interpretation, how to generate the descriptions, etc.).
- The authors cite reference 7,8 and especially 16 a lot, these papers are from one research group. Reference 8 and 16 are only available on arXiv, so they are not published and not peer-reviewed. Reference 16 is cited 14 times within the paper. The left image in figure 2 is the exact same as in reference 16 except for the car color, but it is not cited.
- Abstract contains little more information about the content of the paper than the title, i.e. information on the process and the results

Other notes:
- Paper ID is missing
- Chapter 1: Contribution is not mentioned / unclear. It is claimed that corner case description can improve the performance of ML models, but this remains unproven
- Line 50-51: Should be elaborated more.
- Line 117-120: Missing reference
- The relevance of listing different frameworks for defining street scenarios (chapter 2.1.1-2.1.4) in not apparent
- Line 264: Why quotes?
- Line 285: "CCD can ... act as a trigger during fleet recordings" should be elaborated a bit more to convey the usefulness. Is this intended as live interpretation of the current driving scenario and saving either an annotation or the corner case description?
- Chapter 3 & 4: Would be the contribution of the paper, but contain many unproven assumptions, like "we can automatically describe CC and generate them", "With CCD as an extension, this becomes possible, improving upon existing approaches drastically", etc. This is worded like it already working, but it is never presented/proven.
- Figure 2: As written above. The left image in figure 2 is the exact same as in reference 16 except for the car color, but it is not cited neither exists any indication that the generation of the image was part of the current work.
- Line 426: "To improve this situation, first, a CCD is necessary." The wording should be more humble. This is a very theoretical paper without showing any working solution.

##### Overall quality assessment* Select one option from the list
Reject
  
### Reviewer 2
  
##### Comments to authors* Please assess the contribution of this submission (if any). Please provide comments to the authors to help them improve their work, or to explain your recommendation.
Positive: The paper is well written and structured. It gives a good overview over the research area of corner cases and existing state of the art. The core message is formulated research questions for further work.

Negative: I would like to see a larger and more application focussed section 4 with empirical examples of failure due to CC, or graphs/media to visually underline the text. The high occurence of abbreviations makes the text difficult to read.

##### Overall quality assessment* Select one option from the list
Accept

### Reviewer 3
##### Comments to authors* Please assess the contribution of this submission (if any). Please provide comments to the authors to help them improve their work, or to explain your recommendation.
This paper propose an approach for corner case detection guided by best-practice. The general idea is very relevant and is certainly worthy of further exploration. However, the authors essentially describes a research outline to the problem and provides very abstract solutions only. The potential of corner case descriptions (CCD) for three challenging situations becomes apparent to the reader, but the solutions still seem to be very vague.

##### Overall quality assessment* Select one option from the list
Low Botherline
</details>


&nbsp;
&nbsp;
### Taxonomy and Survey on Remote Human Input Systems for Driving Automation Systems
09/2021 • Daniel Bogdoll, Stefan Orf, Lars Töttel, and J. Marius Zöllner

<img width="100%" src="https://user-images.githubusercontent.com/19552411/150287894-21e34522-03d8-4d34-98b5-4d630b3d1644.png">

[PDF](https://arxiv.org/pdf/2109.08599) | [arXiv](https://arxiv.org/abs/2109.08599) | [Proceeding]()

:heavy_check_mark: Accepted at [FICC Future of Information and Communication Conference](https://saiconference.com/FICC)

<details>
  <summary markdown="span">BibTeX Citation</summary>
  
  ```
  @article{Bogdoll_Taxonomy_2021_arXiv,
      author    = {Bogdoll, Daniel and Orf, Stefan and T\"{o}ttel, Lars and Z\"{o}llner, J. Marius},
      title     = {{Taxonomy and Survey on Remote Human Input Systems for Driving Automation Systems}}, 
      journal   = {arXiv preprint:2109.08599},
      year      = {2021}
  }
  ```
</details>

<details>
  <summary>
    :orange_circle: Double-Blind
    :red_circle: Low Quality Reviews
</summary>
  
### Reviewer 1

##### Detailed Comments
Interesting work! I would like to share your research findings with other colleagues later at the forthcoming event, FICC 2022. After all, this paper should be accepted.

##### Please rate your satisfaction with the basic sections (introduction, conclusion, works cited, etc.) ?
Fair

##### The material is ordered in a way that is logical, clear, and easy to follow?
Good

##### The writer adequately summarizes and discusses the topic?
Fair

##### The writer makes some contribution of thought to the paper or merely summarizes data or publications?
Good

##### The writer introduces and documents sources adequately and appropriately?
Good

##### The formatting of the manuscript is in accordance to the prescribed paper format?
Fair

##### The paragraphs and sentences are cohesive (flow together smoothly without disruption in the train of thought)?
Good

##### Are there any grammar, punctuation, or spelling errors?
Moderate editing of English style and the paper format changes are required including minor spell check.

### Reviewer 2

##### Detailed Comments
• The "Introduction" section needs to be elaborated with more details.
• The paper is missing a short paragraph to introduce the rest of the paper's contents at the end of the Introduction section. This paragraph is important, enabling the readers to understand what the following content will be and arouse their interest to continue reading the paper.
• Describe an incremental solution and show the results.
• Limitation of the study must be highlighted.
• Add portion of discussion to share your thoughts.
• This work can be published only after careful review by the authors as those mentioned above.

##### Please rate your satisfaction with the basic sections (introduction, conclusion, works cited, etc.) ?
Fair

##### The material is ordered in a way that is logical, clear, and easy to follow?
Fair

##### The writer adequately summarizes and discusses the topic?
Fair

##### The writer makes some contribution of thought to the paper or merely summarizes data or publications?
Fair

##### The writer introduces and documents sources adequately and appropriately?
Fair

##### The formatting of the manuscript is in accordance to the prescribed paper format?
Good

##### The paragraphs and sentences are cohesive (flow together smoothly without disruption in the train of thought)?
Good

##### Are there any grammar, punctuation, or spelling errors?
No

</details>

&nbsp;
&nbsp;
### KIGLIS: Smart Networks for Smart Cities
05/2021 • Daniel Bogdoll, Patrick Matalla, Christoph Füllner, Christian Raack, Shi Li, Tobias Käfer, Stefan Orf, Marc René Zofka, Finn Sartoris, Christoph Schweikert, Thomas Pfeiffer, André Richter, Sebastian Randel, and Rene Bonk

<img width="100%" src="https://user-images.githubusercontent.com/19552411/152385032-b1dc7720-7b22-4ead-b1cd-0b3665edf4fc.png">

[PDF](https://arxiv.org/pdf/2106.04549.pdf) | [arXiv](https://arxiv.org/abs/2106.04549) | [Proceeding](https://ieeexplore.ieee.org/abstract/document/9562826)

:heavy_check_mark: Accepted at [IEEE International Smart Cities Conference (ISC2)](https://attend.ieee.org/isc2-2021/)

<details>
  <summary markdown="span">BibTeX Citation</summary>
  
  ```
  @InProceedings{Bogdoll_KIGLIS_2021_ISC2,
    author    = {Bogdoll, Daniel and Matalla, Patrick and F\"{u}llner, Christoph and Raack, Christian and Li, Shi and K\"{a}fer, Tobias and Orf, Stefan and Zofka, Marc Ren\'{e} and Sartoris, Finn and Schweikert, Christoph and Pfeiffer, Thomas and Richter, Andr\'{e} and Randel, Sebastian and Bonk, Rene},
    title     = {{KIGLIS: Smart Networks for Smart Cities}},
    booktitle = {IEEE International Smart Cities Conference (ISC2)},
    year      = {2021}
}

  ```
</details>

<details>
  <summary>
   :red_circle: Single-Blind
   :orange_circle: Medium Quality Reviews
</summary>
  
### Reviewer 1

##### Comments to the author: Summarize the strengths and weaknesses of the paper. Provide a rationale for your rating, and suggested improvements (if appropriate).
The authors introduce the smart networks for smart cities, including the smart city services and architectures. Then, AI based smart network applications are presented to show the further directions. This paper is well written and organized.
 
##### Familiarity: Rate your familiarity with the topic of the paper.
Truly expert in this area of research (1)
 
##### Relevance to the track and timeliness: Rate the importance and timeliness of the topic addressed in the paper within its area of research.
Excellent (1)

##### Technical content and scientific rigour: Rate the technical content of the paper, its scientific rigour and novelty.
Excellent work and outstanding technical content. (1)

##### Quality of presentation: Rate the paper organization, the clearness of text and figures, the completeness and accuracy of references.
Excellent. (1)
 
##### Overall evaluation: Please judge whether the paper should be accepted or rejected
Accept (1)
 
### Reviewer 2
 
##### Comments to the author: Summarize the strengths and weaknesses of the paper. Provide a rationale for your rating, and suggested improvements (if appropriate).
In this paper, the authors present early results on the process of collecting smart city requirements for communication networks, which will lead towards reference infrastructure and architecture solutions. They also suggest directions in which artificial intelligence will improve smart city networks. It will be more interesting if more research results are provided and the novelty over existing schemes can be introduced more clearly.
 
##### Familiarity: Rate your familiarity with the topic of the paper.
Very limited expertise (4)
 
##### Relevance to the track and timeliness: Rate the importance and timeliness of the topic addressed in the paper within its area of research.
Good (2)
 
##### Technical content and scientific rigour: Rate the technical content of the paper, its scientific rigour and novelty.
Marginal work and simple contribution. (4)
 
##### Quality of presentation: Rate the paper organization, the clearness of text and figures, the completeness and accuracy of references.
Excellent. (1)
 
##### Overall evaluation: Please judge whether the paper should be accepted or rejected
Borderline paper (3)
 
### Reviewer 3
 
##### Comments to the author: Summarize the strengths and weaknesses of the paper. Provide a rationale for your rating, and suggested improvements (if appropriate).
 
Authors present KIGLIS research project which addresses the optimization of fiber-optic networks using artificial intelligence, considering 3 research directions, namely (1) AI for improving digital signal processing (2) AI for traffic management, and (3) AI for infrastructure planning. They enumerate multiple services affected by the enhancements as well as typical network architectures. The paper is well-written, Authors focus on network infrastruture while other aspects could be also described such as big data processing infrastructure,
 
##### Familiarity: Rate your familiarity with the topic of the paper.
Working in this area of research (2)
 
##### Relevance to the track and timeliness: Rate the importance and timeliness of the topic addressed in the paper within its area of research.
Good (2)
 
##### Technical content and scientific rigour: Rate the technical content of the paper, its scientific rigour and novelty.
Solid work of notable importance. (2)
 
##### Quality of presentation: Rate the paper organization, the clearness of text and figures, the completeness and accuracy of references.
Excellent. (1)
 
##### Overall evaluation: Please judge whether the paper should be accepted or rejected
Accept (1)
</details>


&nbsp;
&nbsp;
### Reliving the Dataset: Combining the Visualization of Road Users' Interactions with Scenario Reconstruction in Virtual Reality
05/2021 • Lars Töttel, Maximilian Zipfl, Daniel Bogdoll, Marc René Zofka, and J. Marius Zöllner

<img width="100%" src="https://user-images.githubusercontent.com/19552411/152392764-0a5553a7-cba0-4ccc-aef0-9bde156f614e.png">

[PDF](https://arxiv.org/pdf/2105.01610.pdf) | [arXiv](https://arxiv.org/abs/2105.01610) | [Proceeding]()

:heavy_check_mark: Accepted at [International Conference on Intelligent Transportation Engineering (ICITE)](http://www.icite.org/icite2021.html)

<details>
  <summary markdown="span">BibTeX Citation</summary>
  
  ```
  @article{Toettel_Reliving_2021_arXiv,
    author    = {T\"{o}ttel, Lars and Zipfl, Maximilian and Bogdoll, Daniel and Zofka, Marc Ren\'{e} and Z\"{o}llner, J. Marius},    
    title     = {{Reliving the Dataset: Combining the Visualization of Road Users' Interactions with Scenario Reconstruction in Virtual Reality}}, 
    journal   = {arXiv preprint:2105.01610},
    year      = {2021}
}
  ```
</details>

<details>
  <summary>
    :red_circle: Single-Blind :red_circle: Low Quality Review
</summary>
  
### Reviewer 1

##### Expertise: Explain your rating by discussing the strengths and weaknesses of the submission, contributions, and the potential impacts of the paper. Include suggestions for improvement and publication alternatives, if appropriate. Be thorough. Be fair. Be courteous. Your evaluation will be forwarded to the authors during the rebuttal period.
1. Thank you for the opportunity to review the paper, Reliving the Dataset: Combining the
Visualization of Road Users’ Interactions with Scenario Reconstruction in Virtual Reality. This paper
proposed a combination of the analysis of interactive driving scenarios from naturalistic motion
datasets, using various criticality measures, with 2 types of visualization approaches.
2. The paper is well written and organized. The Technical contend of the paper is very sound and
strong. There are high quality images and illustrations to enhance the readers’ understanding. The
list of reference is also extensive with a good range. I will recommend the paper to be strongly
accept to the conference.
3. The OA and SA are well described in the paper.
4. The use of VR to recreate and reconstruct automated driving related datasets for close up
scenarios analysis is very useful for multiple perspectives analysis. This allows the user to be
immersed in the almost real yet safe environment.
5. I look forward to reading the follow up studies on automated vehicles, non-automated vehicles
and other road users.
  
##### Recommendation to Editors
Strong Accept
</details>

:x: Previous version rejected at [IEEE VIS](http://ieeevis.org/)

<details>
  <summary>
    :orange_circle: Double-Blind :green_circle: High Quality Reviews
</summary>
  
### Reviewer 1

##### The Review

This paper presents a visual analytics approach to analyzing automated driving's
safety-critical scenes. Automated driving is emerging as a powerful platform for
transportation services in the near future. Thus, tackling its safety issues would
be a valuable research topic.
The proposed approach allows end-users to detect and explore criticality scenes
from multiple perspectives,  including 2D visualizations for objective analyses
and VR-based close-up subjective analyses. Overall, using VR technologies to
construct multi-perspective scene visualizations is intriguing, which can help
users understand driving corner-cases more straightforwardly and even obtain more
profound insights.

However, the following issues significantly reduce the quality of this paper.
1. The proposed visual analytics (VA) approach needs further clarification for its
end-users and analytical task details. Who will benefit from the proposed visual
analytics workflow? What are the deeper insights immersive VR views can provide
but conventional 2D views cannot? How can this information improve end users'
decision-making? I don't locate answers to these questions in the manuscript.
2. It is vague regarding how the proposed VR-based views can handle the
scalability issue. For example, Section 4.1's third paragraph mentions using a
threshold to control the number of participants in a visualization. How is the
threshold set? Can end users interactively adjust it? In addition, 3D
visualization suffers some challenges, e.g., occlusion of objects. Does the
proposed approach have this issue?
3. The presented visual analytics framework lacks details regarding user
interactions. The evaluation part (Section 5) reveals that the proposed approach
is a mixture of 2D and 3D visualizations. Do the two sides host in the same system
or reside in separate systems? How do the two sides communicate end users'
interactions and data exploration activities? More details or supplementary
materials (e.g., a video) are strongly recommended to help readers better
understand the interactivities between the proposed visual analytics approach and
users.
4. The evaluation (Section 5) is not sufficient to support this paper's novelty,
as it uses only one single dataset to conduct a case study. Thus,  it is hard to
predict whether the proposed approach is generic to other automated driving
datasets. More efforts are required, e.g., domain user feedbacks.
5. This paper's writing needs significant improvements, including but not limited
to the following:
5.1 The concepts of some terms (e.g, RViz, V2X) do not have a literature reference
or sentences to explain.
5.2 Some sentences are vague. For example, there is a sentence in Section 3.2, "On
basis of the road topology given by the HD map, traffic participants can be
brought into mutual relation." What does "mutual relation" mean?
5.3 Section 3.6 describes the implementation of VR scenes. Section 4.2 mainly
introduces the realization of the proposed VR-based visualization. Why does
Section 3.6 is separated from Section 4.2?
6. Regarding the literature, the related work section misses some VR-based visual
analytics work, including but not limited to:
B. Lee, D. Brown, B. Lee, C. Hurter, S. Drucker and T. Dwyer, "Data
Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality," in
IEEE Transactions on Visualization and Computer Graphics, vol. 27, no. 2, pp.
1095-1105, Feb. 2021, doi: 10.1109/TVCG.2020.3030435.
F. Yang, J. Qian, J. Novotny, D. Badre, C. Jackson and D. Laidlaw, "A Virtual
Reality Memory Palace Variant Aids Knowledge Retrieval from Scholarly Articles,"
in IEEE Transactions on Visualization and Computer Graphics, doi:
10.1109/TVCG.2020.3009003.
M. Chen, K. Gaither, N. W. John and B. Mccann, "An Information-Theoretic Approach
to the Cost-benefit Analysis of Visualization in Virtual Environments," in IEEE
Transactions on Visualization and Computer Graphics, vol. 25, no. 1, pp. 32-42,
Jan. 2019, doi: 10.1109/TVCG.2018.2865025.

##### Overall Rating

Reject. The paper is not ready for publication in IEEE VIS. The
work may have some value but the paper requires major revisions or additional work
that are not possible within the conference review cycle to meet the quality
standard. Without them I will not be able to recommend accept.

##### Justification

The main contributions and strengths are:

1. The research topic is quite intriguing.
2. The presented 2D-and-3D mixed visual analytics framework provides a
comprehensive data analysis environment.
3. VR-based visual analytics provides a close-up inspection of critical scenes.


The most important issues or flaws are:

1. This paper is vague regarding end-users, characterization of the problem space,
and particular analytic tasks.
2. The evaluation section is not sufficient to support its claims.
3. The paper writing requires significant improvements.

##### Expertise

Knowledgeable

##### Confidence

Very confident

### Reviewer 2

##### The Review

This work presents an interesting application, close-up scenario analysis using VR
to reconstruct automated driven scenes. The authors claim this is the work of such
a work. While I am not familiar with this application, I agree that the chosen set
up is very appropriate for this application.

The main paper describes the system components, including the analysis framework,
criticality measures, scenario recreation in VR. The evaluation provides a summary
of using TTC, RSS and SFF in analysis. The conclusion from the example is that the
critical scene resulted from two vehicles driving in parallel and very close to
each other.

The visualization component is rather limited in this work. Based on the space
time cubes, vehicles/humans/bikes are visualized as boxes/spheres and the travel
paths are rendered as lines in the scene visualization. These visual
representations are appropriate. However, this is the most relevant portion to
visualization.

Based on the technical contribution of the paper, I would suggest the authors to
consider VR conferences, which are more suitable than visualization conferences.

##### Overall Rating

Reject. The paper is not ready for publication in IEEE VIS.The
work may have some value but the paper requires major revisions or additional work
that are not possible within the conference review cycle to meet the quality
standard. Without them I will not be able to recommend accept.

##### Justification

This work presents an interesting application. The system is well constructed and
described.

However, the visualization component follows the concept of space time cube
directly. I think this work is more suitable for VR conferences or a short paper
for the upcoming short paper deadline.

##### Expertise

Knowledgeable

##### Confidence

Very confident

### Reviewer 3

##### The Review

This paper proposed a novel pipeline to analyse automated driving dataset. The
pipeline contains of two steps: objective analysis and subjective analysis.
Objective analysis aims at visualizing the criticality analysis result from the
semantic scene graph generated from the dataset. Subjective analysis then allow
analytics to immerse into the reconstructed historical scene for more in-depth
analysis. The authors evaluated their methodology by providing a case study of
finding reasons behind a critical situation.

Positive
-       The author proposed a novel framework combining visual analytics with VR
technology to provide both objective and subjective analytics, which is a timely
topic.
-       The paper is easy to read.

Negative
-       The contribution is a bit limited for the VIS community. The authors
proposed a novel framework for automated driving dataset by involving a subjective
analysis using VR after the visual analytics, which seems to be a simple
combinatorial approach of visual analytics and VR. I suggest the authors should do
more in-depth analysis in either visual analytics or VR. For example, what is the
pain point in visual analytics for automated driving dataset and what is the
challenge of recreating a historical scene in VR. Besides, blending visual
analytics and VR, not just treating them as a separation component, could be
another direction. For instance, it would be interesting to see how to present the
time-space cube in the VR traffic scene for the analytics to do in-depth
subjective analysis, while still be able to refer to the objective data.
-       The evaluation is not sufficient. The authors only presented one case
study in the paper to evaluate the usefulness of the pipeline, which is hard to
tell the usefulness in a more general sense. I suggested that the authors could
evaluated their pipeline with domain experts to see if the proposed pipeline is
useful. Moreover, the case study did not cover all the functionality of the
proposed system. The Spatio-Temporal Scenario Visualization in Sec 3.4.2 is not
presented in the case study, which its usefulness cannot be evaluated. I suggest
the authors to find a case with all functionalities or provide more case studies
to cover the complete set of function proposed in the paper.

To conclude, this paper presented a novel framework for analyzing the automated
driving dataset, which is an emerging topic. However, the paper currently did not
provide sufficient contribution to the VIS community and evaluation for the
usefulness of the proposed methodology. As a result, I am on the side of rejecting
this paper by giving a rating of 2.

##### Overall Rating

Reject. The paper is not ready for publication in IEEE VIS. The
work may have some value but the paper requires major revisions or additional work
that are not possible within the conference review cycle to meet the quality
standard. Without them I will not be able to recommend accept.

##### Justification

The paper presents a novel framework combining objective analysis using
visualization and subjective analysis using VR for driving dataset. It is a timely
research topic which combines automated driving and VR.

But the paper does not have sufficient contribution and also the evaluation is not
solid.

##### Expertise

Knowledgeable

##### Confidence

Very confident

### Reviewer 4

##### The Review

This paper presents an unnamed VR visualization system for reconstructing
automated driving scenarios using naturalistic motion traffic data. Per their
abstract, the motivation for the authors' implementation is to "detect critical
scenarios and generate knowledge" to the end of improving automated driving
models. The contribution of this paper is a novel one, and I am impressed with the
engineering that has gone into the system architecture, which is clearly laid out
in Sections 3 and 4 with sufficient detail regarding the models, calculations, and
development tools used in its construction. Section 3 in general in particular is
very well-reasoned and does an excellent job of justifying and explaining the
models, measures, and data transformations implemented in the system. These
elements of the system are sufficiently validated in Section 5 using measures
consistent with the discussion of criticality measures throughout the paper, which
adds cohesion to the submission.

However, I have significant concerns about this paper, and I do not believe that
the revisions that would be required are achievable within the conference review
cycle.

The authors' application of their selection of criticality measures to reconstruct
a scenarios in a VR replay view from multiple perspectives specifically for the
use-case of automated driving is, to the best of this reviewer's knowledge, indeed
a novel one, as the authors state in the second-to-last paragraph of the
INTRODUCTION Section. However, it is not the first VR visualization system for
semantic, relational, temporal data: See Hoobler, et al's "Visualizing Competitive
Behaviors in Multi-User Virtual Environments" (2004); Einsfeld, et al's "Modified
Virtual Reality for Intuitive Semantic Information Visualization" (2008); Halpin,
et al's "Redgraph" (2008); or Cailhol, et al's "multi-layer approach for
interactive path planning" (2014). Additionally, while the publications by Han, et
al (2020) and Park, et al (2021) noted below, along with van den Berg, et al's
"Virtualized Traffic" (2009), don't exactly overlap with this submission, they
would be worth a read by the authors.

That none of the items listed above are cited by the authors speak to my most
significant concern with this submission: A lack of grounding in prior research.

This may sound cantankerous, but I believe it needs to be said: VIS is not just a
place to submit rejected CVPR papers that have an interface component. It is the
premier venue for visualization research. The authors have cited virtually no
literature from the visualization community, and their absence is reflected
throughout the entire paper. The authors should take some time to draw lessons
from existing visualization research--particularly the extremely relevant,
emerging field of immersive analytics (IA). I believe that their implementation,
evaluation, and discussion would all have been greatly improved if they had done
so prior to developing their system.

The authors should reframe their work to ground it in existing work in IA,
including:

Fonnet, Adrien, and Yannick Prié. "Survey of immersive analytics." IEEE
transactions on visualization and computer graphics (2019).

Batch, Andrea, et al. "There is no spoon: Evaluating performance, space use,
and presence with expert domain users in immersive analytics." IEEE transactions
on visualization and computer graphics 26.1 (2019): 536-546.

Cordeil, Maxime, et al. "IATK: An immersive analytics toolkit." 2019 IEEE
Conference on Virtual Reality and 3D User Interfaces (VR). IEEE, 2019.

Butcher, Peter, et al. "VRIA: A web-based framework for creating immersive
analytics experiences." IEEE transactions on visualization and computer graphics
(2020).

Whitlock, Matt, et al. "Graphical perception for immersive analytics." 2020
IEEE Conference on Virtual Reality and 3D User Interfaces (VR). IEEE, 2020.

Han, Lei, et al. "Live Semantic 3D Perception for Immersive Augmented
Reality." IEEE transactions on visualization and computer graphics 26.5 (2020):
2012-2022.

Park, Jinwoo, et al. "Instant Panoramic Texture Mapping with Semantic Object
Matching for Large-Scale Urban Scene Reproduction." IEEE Transactions on
Visualization and Computer Graphics 27.5 (2021): 2746-2756.

It may also be worth the authors' time to review the 2018 book on Immersive
Analytics by Marriott, et al.

From the broader domain of visualization research, the authors should additionally
consider familiarizing themselves with the subjects of design space and
requirements, with system and technique evaluation methods, and some of the
fundamentals of information visualization, perhaps starting with:

Plaisant, Catherine. "The challenge of information visualization evaluation."
Proceedings of the working conference on Advanced visual interfaces. 2004.

Munzner, Tamara. "A nested model for visualization design and validation."
IEEE transactions on visualization and computer graphics 15.6 (2009): 921-928.

Elmqvist, Niklas, and Ji Soo Yi. "Patterns for visualization evaluation."
Information Visualization 14.3 (2015): 250-269.

Shneiderman, Ben. "The eyes have it: A task by data type taxonomy for
information visualizations." The craft of information visualization. Morgan
Kaufmann, 2003. 364-371.

Card, Stuart K., and Jock Mackinlay. "The structure of the information
visualization design space." Proceedings of VIZ'97: Visualization Conference,
Information Visualization Symposium and Parallel Rendering Symposium. IEEE, 1997.


The absence of an explicit connection with relevant work within the discipline is
not the only problem with this paper. My second most pressing concern about this
submission is the lack of validation of the stated purpose of their VR interface,
or indeed any mention thereof.

The VR component of the system is given a central focus in this paper, and its
purpose---as well as one of the two motivations for developing the entire system
---is to "generate knowledge." Cognitive science perspectives on what exactly it
means to use a visual interface to "generate knowledge" aside, there is never any
discussion by the authors about validating the correctness, accuracy, speed, or
even the user experience aspects of data analysis within the VR environment
implemented in this paper.

The authors either need to conduct a user study or reframe the paper. If the
authors choose to reframe the paper such that subjective analysis in the VR view
is no longer a central focus, then it begs the question "what makes this paper
appropriate for VIS?"

In response to that question, I believe that the authors may be able to make it
relevant by focusing instead on the contribution of the 3D scene generation
itself, but only if they go into significantly greater detail in describing their
design space and contextualizing it with prior work, clearly defining their design
requirements and linking those requirements to the techniques implemented (not
just for the models on the backend but for the VR interface as well), and
justifying their design decisions. Ideally, this would be done in addition to an
evaluation of the VR view, but without the evaluation it really is necessary.


I don't like having to score this paper poorly because I genuinely think that this
could be a significant contribution to the VIS, VR, and intelligent systems
communities, but I once again cannot in good conscience say that the revisions
that would be required in order for it to be an appropriate publication in VIS are
possible within the review cycle.

Beyond that, please consider the minor issues and suggestions below.

------------------------------------------------
STYLE/ORGANIZATION/MISC
------------------------------------------------
• GENERAL:
- Consider beginning each section with a summary of the most important points from
the section to follow instead of beginning each section with a subsection header.
- A paragraph usually has more than 2 sentences (unless you are writing a review
like I am right now).
• ABSTRACT: Don't put a line break in your abstract.
• INTRODUCTION: You do not need to state the paper outline at the end of your
introduction; the last paragraph of this section can be removed to end it on the
much stronger note of the authors' novel contribution. This will improve your
introduction. Sometimes less is indeed more.
• RELATED WORK: If your subsections are each a paragraph or less in length, your
section does not need subsections. Alternatively, this section could be extended
with information visualization research, particularly with IA research, as noted
above.
• LIMITATIONS: This paper will need a more thorough discussion of its limitations,
which it will absolutely still have If it is to be turned around quickly for this
or another closely-related conference or journal within the next few months.
• SUPPLEMENTAL MATERIALS: Please consider submitting a video demonstration of your
work as supplemental material.

##### Overall Rating

Reject. The paper is not ready for publication in IEEE VIS. The
work may have some value but the paper requires major revisions or additional work
that are not possible within the conference review cycle to meet the quality
standard. Without them I will not be able to recommend accept.

##### Justification

The most important flaw in this submission is its lack of grounding in prior
visualization work, particularly in immersive analytics and interface evaluation.
This flaw has resulted in the second most significant issue with the paper: The
lack of both a) _validation_ and b) _design space parameterization_ of the VR view
designed by the authors. If the paper had one or the other, it could be published
with revisions that would be achievable within the review cycle.

A number of aspects of the authors' contribution are indeed novel and compelling,
and I would be delighted to see them published after appropriate steps are taken
to turn this into a visualization paper.

##### Expertise

Expert

##### Confidence

Very confident

### The Summary Review

##### The Review
  
All four reviewers gave "2". Though the reviewers find the application itself is
interesting and there are some novel parts of the paper,  the following major
flaws have been raised by the reviewers:
1. The visualization component is rather limited in this work (R2, R3, R4)
2. The evaluation/validation is insufficient (R1, R2, R4)
3. The presentation requires significant improvements (R1, R4)

The paper cannot be accepted to IEEE VIS. The authors may consider submitting to a
VR conference or significantly strengthen the visualization part and then submit
to a visualization conference.

##### Summary Rating

Reject. The paper is not ready for publication in IEEE VIS.The work
may have some value but the paper requires major revisions or additional work that
are not possible within the conference review cycle to meet the quality standard.
Without them I will not be able to recommend accept.
  
</details>


&nbsp;
&nbsp;
### Towards Sensor Data Abstraction of Autonomous Vehicle Perception Systems
05/2021 • Hannes Reichert*, Lukas Lang*, Kevin Rösch*, Daniel Bogdoll*, Konrad Doll, Bernhard Sick, Hans-Christian Reuss, Christoph Stiller, and J. Marius Zöllner

<img width="100%" src="https://user-images.githubusercontent.com/19552411/152395620-1803bb50-7310-47aa-ae0f-b08b6fee0473.png">

[PDF](https://arxiv.org/pdf/2105.06896.pdf) | [arXiv](https://arxiv.org/abs/2105.06896) | [Proceeding](https://ieeexplore.ieee.org/document/9562912)

:heavy_check_mark: Accepted at [IEEE International Smart Cities Conference (ISC2)](https://attend.ieee.org/isc2-2021/)

<details>
  <summary markdown="span">BibTeX Citation</summary>
  
  ```
  @InProceedings{Reichert_Towards_2021_ISC2,  
    author    = {Reichert, Hannes and Lang, Lukas and R\"{o}sch, Kevin and Bogdoll, Daniel and Doll, Konrad and Sick, Bernhard and Reuss, Hans-Christian and Stiller, Christoph and Z\"{o}llner, J. Marius},
    title     = {{Towards Sensor Data Abstraction of Autonomous Vehicle Perception Systems}},
    booktitle = {IEEE International Smart Cities Conference (ISC2)},
    year      = {2021}
}
  ```
</details>

<details>
  <summary>
    :red_circle: Single-Blind
    :orange_circle: Medium Quality Reviews
</summary>

### Reviewer 1
  
##### Comments to the author: Summarize the strengths and weaknesses of the paper. Provide a rationale for your rating, and suggested improvements (if appropriate).

This work fits in the context of full-stack autonomous driving perception requiring multiple sensor data-driven models. The authors state the bias problem -- resulting from the sensor setup used for data acquisition -- that can seriously alter the environment perception accuracy by impairing the perception models' transferability to new sensor setups.

The authors envision a unified and sensor-independent abstraction of sensor data, enabling a general perception pipeline for different sensor setups. They state a core research question, in which sensors combinations and the positions of new ones can be altered without effects on the abstract representation.


The pros:

1/ this work copes with the challenging problem of environment perception in autonomous driving context from various sensors data acquisition and fusion.

2/ the topic of sensor data abstraction is properly contextualized in the literature, and the body of related works is fairly complete in my opinion; the authors outline the feasibility of a joint sensor abstraction and highlight early results in this research field.

3/ their approach of data abstraction based on a joint unified and sensor-independent pipelines is clearly a relevant solution for the bias problem of sensor setups impacting the environment AI perception.

4/ the paper is well-written.

The cons:

1/ lack of examples, I know that short papers have not enough space, but providing schematic examples would be very helpful for the reviewers; mainly two: the first illustrates the bias problem and the second shows how the problem could be resolved by following the authors' sensor data abstraction method.

2/ the authors don't provide sufficient perspectives about their abstraction method implementation.

##### Familiarity: Rate your familiarity with the topic of the paper.
Familiar with this area of research (3)

##### Relevance to the track and timeliness: Rate the importance and timeliness of the topic addressed in the paper within its area of research.
Excellent (1)

##### Technical content and scientific rigour: Rate the technical content of the paper, its scientific rigour and novelty.
Solid work of notable importance. (2)

##### Quality of presentation: Rate the paper organization, the clearness of text and figures, the completeness and accuracy of references.
Well written. (2)

##### Overall evaluation: Please judge whether the paper should be accepted or rejected
Accept (1)

### Reviewer 2

##### Comments to the author: Summarize the strengths and weaknesses of the paper. Provide a rationale for your rating, and suggested improvements (if appropriate).

The authors study a review of sensor data abstraction for the field of automotive applications. The article is well-written with a clear explanation. However, the paper does not contribute to the existing literature. Additionally, the reference list is poor, and literature with high impact was not cited. The authors need to select papers to cite carefully.

##### Familiarity: Rate your familiarity with the topic of the paper.
Familiar with this area of research (3)

##### Relevance to the track and timeliness: Rate the importance and timeliness of the topic addressed in the paper within its area of research.
Acceptable (3)

##### Technical content and scientific rigour: Rate the technical content of the paper, its scientific rigour and novelty.
Valid work but limited contribution. (3)

##### Quality of presentation: Rate the paper organization, the clearness of text and figures, the completeness and accuracy of references.
Well written. (2)

##### Overall evaluation: Please judge whether the paper should be accepted or rejected
Borderline paper (3)

### Reviewer 3

##### Comments to the author: Summarize the strengths and weaknesses of the paper. Provide a rationale for your rating, and suggested improvements (if appropriate).

Paper provides a well-structured short overview of the sensor data abstraction for Highly Automated Driving setup. Work is well structured and gives a good glimpse into the problem of decoupling the various sensors' raw data generated by HAD from perception layer/application. The current approaches towards tackling this problem are also summarised in the manuscript. One suggestion would be to highlight the nature of the work (overview, summary) in the title of the manuscript.

##### Familiarity: Rate your familiarity with the topic of the paper.
Very limited expertise (4)

##### Relevance to the track and timeliness: Rate the importance and timeliness of the topic addressed in the paper within its area of research.
Good (2)

##### Technical content and scientific rigour: Rate the technical content of the paper, its scientific rigour and novelty.
Valid work but limited contribution. (3)

##### Quality of presentation: Rate the paper organization, the clearness of text and figures, the completeness and accuracy of references.
Well written. (2)

##### Overall evaluation: Please judge whether the paper should be accepted or rejected
Weak accept (2)
</details>

&nbsp;
&nbsp;
### AUREATE: An Augmented Reality Test Environment for Realistic Simulations
04/2018 • Tejaswi Koduri, Daniel Bogdoll, Shreyasha Paudel, and Gautham Sholingar

<img width="100%" src="https://user-images.githubusercontent.com/19552411/152373346-99b6d881-4543-494c-80de-f0e196f6c1c0.png">

[Proceeding](https://www.sae.org/publications/technical-papers/content/2018-01-1080/)

:heavy_check_mark: Accepted at [SAE WCX World Congress Experience](https://www.sae.org/attend/wcx)

<details>
  <summary markdown="span">BibTeX Citation</summary>
  
  ```
  @InProceedings{Koduri_Aureate_2018_WCX,
    author    = {Koduri, Tejaswi and Bogdoll, Daniel and Paudel, Shreyasha and Sholingar, Gautham},
    title     = {{AUREATE: An Augmented Reality Test Environment for Realistic Simulations}},
    booktitle = {WCX World Congress Experience},
    publisher = {SAE International},
    year      = {2018}
}
  ```
</details>

<details>
  <summary>
   :red_circle: Single-Blind Reviews
</summary>
I no longer have access to the reviews
</details>

&nbsp;
&nbsp;
### US20190065933A1: Augmenting Real Sensor Recordings With Simulated Sensor Data
08/2017 • Daniel Bogdoll, Shreyasha Paudel, and Tejaswi Koduri

<img width="100%" src="https://user-images.githubusercontent.com/19552411/152391191-8769338f-f697-4abc-835c-416319bb11e1.png">

[Patent](https://patents.google.com/patent/US20190065933A1/en) | [Status](https://patentcenter.uspto.gov/#!/applications/15693265/ifw/docs)

:grey_question: On Appeal -- Awaiting Decision by the Board of Appeals

<details>
  <summary markdown="span">BibTeX Citation</summary>
  
  ```
  @patent{Bogdoll_Augmenting_2017_US,
    author    = {Bogdoll, Daniel and Paudel, Shreyasha and Koduri, Tejaswi},
    title     = {{Augmenting Real Sensor Recordings With Simulated Sensor Data}},
    number    = {US20190065933A1},
    year      = {2017}
}
  ```
</details>
