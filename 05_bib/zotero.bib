
@article{pretschner_tests_2021,
	title = {Tests für automatisierte und autonome Fahrsysteme},
	volume = {44},
	issn = {1432-122X},
	url = {https://doi.org/10.1007/s00287-021-01364-w},
	doi = {10/gkpvs2},
	abstract = {Der Test automatisierter und autonomer Fahrsysteme ist aus verschiedenen Gründen sehr schwierig, u. a. wegen der Abwesenheit klarer Spezifikationen der zu testenden Systeme, wegen Unklarheit über die Auswahlkriterien für „gute“ Testfälle und wegen des Fehlens von Testendekriterien. Ein populärer Ansatz schlägt deswegen vor, aufgezeichnete Fahrten unabhängig vom zu testenden System als Testfälle zu verwenden. Wir zeigen, dass dies im Wesentlichen einem rein zufälligen Test des Systems gleichkommt, dass dieselbe aufgezeichnete Testfahrt dementsprechend einmal einen „guten“ und einmal einen uninteressanten Testfall darstellen kann und dass es notwendig ist, für jedes neue System neue Testfälle abzuleiten. Die simulationsbasierte heuristische Suche nach Testfällen auf Basis sogenannter Szenariotypen, die typische Verkehrssituationen beschreiben, stellt eine Lösung des Problems dar.},
	pages = {214--218},
	number = {3},
	journaltitle = {Informatik Spektrum},
	shortjournal = {Informatik Spektrum},
	author = {Pretschner, Alexander and Hauer, Florian and Schmidt, Tabea},
	urldate = {2021-06-28},
	date = {2021-06-01},
	langid = {german},
}

@online{hesse_potenziale_2021,
	title = {Potenziale für industrieübergreifendes Flottenlernen - {KI}-Mobilitätsdatenplattform zur Risikominimierung des automatisierten Fahrens},
	url = {https://www.plattform-lernende-systeme.de/files/Downloads/Publikationen/AG5_Whitepaper_Mobilitaetsplattform.pdf},
	author = {Hesse, Tobias and Peylo, Christoph},
	urldate = {2021-06-24},
	date = {2021},
	keywords = {{ToDo}},
}

@article{sanchez_morales_parallel_2020,
	title = {Parallel Multi-Hypothesis Algorithm for Criticality Estimation in Traffic and Collision Avoidance},
	url = {https://ui.adsabs.harvard.edu/abs/2020arXiv200506773S},
	abstract = {Due to the current developments towards autonomous driving and vehicle active safety, there is an increasing necessity for algorithms that are able to perform complex criticality predictions in real-time. Being able to process multi-object traffic scenarios aids the implementation of a variety of automotive applications such as driver assistance systems for collision prevention and mitigation as well as fall-back systems for autonomous vehicles. We present a fully model-based algorithm with a parallelizable architecture. The proposed algorithm can evaluate the criticality of complex, multi-modal (vehicles and pedestrians) traffic scenarios by simulating millions of trajectory combinations and detecting collisions between objects. The algorithm is able to estimate upcoming criticality at very early stages, demonstrating its potential for vehicle safety-systems and autonomous driving applications. An implementation on an embedded system in a test vehicle proves in a prototypical manner the compatibility of the algorithm with the hardware possibilities of modern cars. For a complex traffic scenario with 11 dynamic objects, more than 86 million pose combinations are evaluated in 21 ms on the {GPU} of a Drive {PX}{\textasciitilde}2.},
	pages = {arXiv:2005.06773},
	journaltitle = {{arXiv} e-prints},
	author = {Sánchez Morales, Eduardo and Membarth, Richard and Gaull, Andreas and Slusallek, Philipp and Dirndorfer, Tobias and Kammenhuber, Alexander and Lauer, Christoph and Botsch, Michael},
	urldate = {2021-06-24},
	date = {2020-05-01},
	keywords = {Computer Science - Distributed, Computer Science - Robotics, Parallel, {ToDo}, and Cluster Computing, ⛔ No {DOI} found},
}

@article{aptiv_safety_2019,
	title = {Safety First for Automated Driving},
	pages = {157},
	author = {{APTIV}},
	date = {2019},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@report{argo_ai_argo_2021,
	title = {{ARGO} {AI} Safety Report April 2021},
	url = {https://www.argo.ai/wp-content/uploads/2021/04/ArgoSafetyReport.pdf},
	author = {{ARGO} {AI}},
	urldate = {2021-06-21},
	date = {2021},
}

@article{menzel_scenarios_2018,
	title = {Scenarios for Development, Test and Validation of Automated Vehicles},
	url = {http://arxiv.org/abs/1801.08598},
	abstract = {The {ISO} 26262 standard from 2016 represents the state of the art for a safety-guided development of safety-critical electric/electronic vehicle systems. These vehicle systems include advanced driver assistance systems and vehicle guidance systems. The development process proposed in the {ISO} 26262 standard is based upon multiple V-models, and defines activities and work products for each process step. In many of these process steps, scenario based approaches can be applied to achieve the defined work products for the development of automated driving functions. To accomplish the work products of different process steps, scenarios have to focus on various aspects like a human understandable notation or a description via time-space variables. This leads to contradictory requirements regarding the level of detail and way of notation for the representation of scenarios. In this paper, the authors present requirements for the representation of scenarios in different process steps defined by the {ISO} 26262 standard, propose a consistent terminology based on prior publications for the identified levels of abstraction, and demonstrate how scenarios can be systematically evolved along the phases of the development process outlined in the {ISO} 26262 standard.},
	journaltitle = {{arXiv}:1801.08598 [cs]},
	author = {Menzel, Till and Bagschik, Gerrit and Maurer, Markus},
	urldate = {2021-06-17},
	date = {2018-04-27},
	eprinttype = {arxiv},
	eprint = {1801.08598},
	keywords = {Computer Science - Software Engineering, {ToDo}, ⛔ No {DOI} found},
}

@inproceedings{karlsson_encoding_2021,
	title = {Encoding Human Driving Styles in Motion Planning for Autonomous Vehicles},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-292195},
	abstract = {{DiVA} portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	eventtitle = {{ICRA} International Conference on Robotics and Automation},
	author = {Karlsson, Jesper and van Waveren, Sanne and Pek, Christian and Torre, Ilaria and Leite, Iolanda and Tumova, Jana},
	urldate = {2021-06-17},
	date = {2021},
	keywords = {{KIW}, {ToDo}, ⛔ No {DOI} found},
}

@article{pretschner_tests_2021-1,
	title = {Tests für automatisierte und autonome Fahrsysteme: Wiederverwendung aufgezeichneter Fahrten ist nicht zu rechtfertigen},
	volume = {44},
	issn = {0170-6012, 1432-122X},
	url = {https://link.springer.com/10.1007/s00287-021-01364-w},
	doi = {10/gkpvs2},
	shorttitle = {Tests für automatisierte und autonome Fahrsysteme},
	abstract = {Testing automated and autonomous driving systems is challenging for various reasons, e.g., because of the absence of clear and precise speciﬁcations of the system under testing, because of the unclear test selection criteria for “good” test cases, and because of missing test exit criteria. As a consequence, a popular approach suggests reusing recorded test drives as test cases regardless of the system being tested. We show that this is generally equivalent to purely random testing of the system, that a recorded test drive may be sometimes “good” and sometimes irrelevant, and that it is necessary to derive new test cases for each new system. A solution to this issue is a simulation-based heuristic search of test cases for so-called scenario types that capture recurring trafﬁc situations.},
	pages = {214--218},
	number = {3},
	journaltitle = {Informatik Spektrum},
	shortjournal = {Informatik Spektrum},
	author = {Pretschner, Alexander and Hauer, Florian and Schmidt, Tabea},
	urldate = {2021-06-17},
	date = {2021-06},
	langid = {german},
}

@article{zeng_end--end_2019,
	title = {End-to-end interpretable neural motion planner},
	volume = {2019-June},
	issn = {10636919},
	doi = {10/gjjg2w},
	abstract = {In this paper, we propose a neural motion planner for learning to drive autonomously in complex urban scenarios that include traffic-light handling, yielding, and interactions with multiple road-users. Towards this goal, we design a holistic model that takes as input raw {LIDAR} data and a {HD} map and produces interpretable intermediate representations in the form of 3D detections and their future trajectories, as well as a cost volume defining the goodness of each position that the self-driving car can take within the planning horizon. We then sample a set of diverse physically possible trajectories and choose the one with the minimum learned cost. Importantly, our cost volume is able to naturally capture multi-modality. We demonstrate the effectiveness of our approach in real-world driving data captured in several cities in North America. Our experiments show that the learned cost volume can generate safer planning than all the baselines.},
	pages = {8652--8661},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Zeng, Wenyuan and Luo, Wenjie and Suo, Simon and Sadat, Abbas and Yang, Bin and Casas, Sergio and Urtasun, Raquel},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {2101.06679},
	note = {{ISBN}: 9781728132938
80 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Read, Robotics + Driving, ★},
}

@article{joseph_towards_2021,
	title = {Towards Open World Object Detection},
	url = {http://arxiv.org/abs/2103.02603},
	abstract = {Humans have a natural instinct to identify unknown object instances in their environments. The intrinsic curiosity about these unknown instances aids in learning about them, when the corresponding knowledge is eventually available. This motivates us to propose a novel computer vision problem called: `Open World Object Detection', where a model is tasked to: 1) identify objects that have not been introduced to it as `unknown', without explicit supervision to do so, and 2) incrementally learn these identified unknown categories without forgetting previously learned classes, when the corresponding labels are progressively received. We formulate the problem, introduce a strong evaluation protocol and provide a novel solution, which we call {ORE}: Open World Object Detector, based on contrastive clustering and energy based unknown identification. Our experimental evaluation and ablation studies analyze the efficacy of {ORE} in achieving Open World objectives. As an interesting by-product, we find that identifying and characterizing unknown instances helps to reduce confusion in an incremental object detection setting, where we achieve state-of-the-art performance, with no extra methodological effort. We hope that our work will attract further research into this newly identified, yet crucial research direction.},
	author = {Joseph, K J and Khan, Salman and Khan, Fahad Shahbaz and Balasubramanian, Vineeth N},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.02603},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {{ToDo}, ★, ⛔ No {DOI} found},
}

@article{wong_identifying_2019,
	title = {Identifying unknown instances for autonomous driving},
	issn = {23318422},
	abstract = {In the past few years, we have seen great progress in perception algorithms, particular through the use of deep learning. However, most existing approaches focus on a few categories of interest, which represent only a small fraction of the potential categories that robots need to handle in the real-world. Thus, identifying objects from unknown classes remains a challenging yet crucial task. In this paper, we develop a novel open-set instance segmentation algorithm for point clouds which can segment objects from both known and unknown classes in a holistic way. Our method uses a deep convolutional neural network to project points into a category-agnostic embedding space in which they can be clustered into instances irrespective of their semantics. Experiments on two large-scale self-driving datasets validate the effectiveness of our proposed method.},
	pages = {1--10},
	issue = {{CoRL}},
	journaltitle = {{arXiv}},
	author = {Wong, Kelvin and Wang, Shenlong and Ren, Mengye and Liang, Ming and Urtasun, Raquel},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1910.11296},
	keywords = {Autonomous Driving, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Instance Segmentation, Open-Set Perception, {ToDo}, ⛔ No {DOI} found},
}

@inproceedings{tian_deeptest_2018,
	location = {New York, {NY}, {USA}},
	title = {{DeepTest}: automated testing of deep-neural-network-driven autonomous cars},
	isbn = {978-1-4503-5638-1},
	url = {https://doi.org/10.1145/3180155.3180220},
	doi = {10/ggrbjf},
	series = {{ICSE} '18},
	shorttitle = {{DeepTest}},
	abstract = {Recent advances in Deep Neural Networks ({DNNs}) have led to the development of {DNN}-driven autonomous cars that, using sensors like camera, {LiDAR}, etc., can drive without any human intervention. Most major manufacturers including Tesla, {GM}, Ford, {BMW}, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several {US} states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, {DNNs}, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for {DNN}-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement, and evaluate {DeepTest}, a systematic testing tool for automatically detecting erroneous behaviors of {DNN}-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. {DeepTest} systematically explore different parts of the {DNN} logic by generating test inputs that maximize the numbers of activated neurons. {DeepTest} found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing {DNNs} in the Udacity self-driving car challenge.},
	pages = {303--314},
	booktitle = {Proceedings of the 40th International Conference on Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi},
	urldate = {2021-06-10},
	date = {2018-05-27},
	keywords = {autonomous vehicle, deep learning, deep neural networks, neuron coverage, self-driving cars, testing},
}

@article{casas_importance_2020,
	title = {The Importance of Prior Knowledge in Precise Multimodal Prediction},
	url = {http://arxiv.org/abs/2006.02636},
	abstract = {Roads have well defined geometries, topologies, and traffic rules. While this has been widely exploited in motion planning methods to produce maneuvers that obey the law, little work has been devoted to utilize these priors in perception and motion forecasting methods. In this paper we propose to incorporate these structured priors as a loss function. In contrast to imposing hard constraints, this approach allows the model to handle non-compliant maneuvers when those happen in the real world. Safe motion planning is the end goal, and thus a probabilistic characterization of the possible future developments of the scene is key to choose the plan with the lowest expected cost. Towards this goal, we design a framework that leverages {REINFORCE} to incorporate non-differentiable priors over sample trajectories from a probabilistic model, thus optimizing the whole distribution. We demonstrate the effectiveness of our approach on real-world self-driving datasets containing complex road topologies and multi-agent interactions. Our motion forecasts not only exhibit better precision and map understanding, but most importantly result in safer motion plans taken by our self-driving vehicle. We emphasize that despite the importance of this evaluation, it has been often overlooked by previous perception and motion forecasting works.},
	journaltitle = {{arXiv}:2006.02636 [cs, stat]},
	author = {Casas, Sergio and Gulino, Cole and Suo, Simon and Urtasun, Raquel},
	urldate = {2021-06-15},
	date = {2020-06-03},
	eprinttype = {arxiv},
	eprint = {2006.02636},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, {KIW}, Statistics - Machine Learning, {ToDo}, ⛔ No {DOI} found},
}

@article{brown_risk-aware_2019,
	title = {Risk-Aware Active Inverse Reinforcement Learning},
	url = {http://arxiv.org/abs/1901.02161},
	abstract = {Active learning from demonstration allows a robot to query a human for specific types of input to achieve efficient learning. Existing work has explored a variety of active query strategies; however, to our knowledge, none of these strategies directly minimize the performance risk of the policy the robot is learning. Utilizing recent advances in performance bounds for inverse reinforcement learning, we propose a risk-aware active inverse reinforcement learning algorithm that focuses active queries on areas of the state space with the potential for large generalization error. We show that risk-aware active learning outperforms standard active {IRL} approaches on gridworld, simulated driving, and table setting tasks, while also providing a performance-based stopping criterion that allows a robot to know when it has received enough demonstrations to safely perform a task.},
	journaltitle = {{arXiv}:1901.02161 [cs, stat]},
	author = {Brown, Daniel S. and Cui, Yuchen and Niekum, Scott},
	urldate = {2021-06-13},
	date = {2019-06-03},
	eprinttype = {arxiv},
	eprint = {1901.02161},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No {DOI} found},
}

@article{sensoy_evidential_2018,
	title = {Evidential Deep Learning to Quantify Classification Uncertainty},
	url = {http://arxiv.org/abs/1806.01768},
	abstract = {Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of out-of-distribution queries and endurance against adversarial perturbations.},
	journaltitle = {{arXiv}:1806.01768 [cs, stat]},
	author = {Sensoy, Murat and Kaplan, Lance and Kandemir, Melih},
	urldate = {2021-06-13},
	date = {2018-10-31},
	eprinttype = {arxiv},
	eprint = {1806.01768},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No {DOI} found},
}

@article{kiran_deep_2021,
	title = {Deep Reinforcement Learning for Autonomous Driving: A Survey},
	url = {http://arxiv.org/abs/2002.00444},
	shorttitle = {Deep Reinforcement Learning for Autonomous Driving},
	abstract = {With the development of deep representation learning, the domain of reinforcement learning ({RL}) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning ({DRL}) algorithms and provides a taxonomy of automated driving tasks where (D){RL} methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical {RL} algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in {RL} are discussed.},
	journaltitle = {{arXiv}:2002.00444 [cs]},
	author = {Kiran, B. Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Sallab, Ahmad A. Al and Yogamani, Senthil and Pérez, Patrick},
	urldate = {2021-06-13},
	date = {2021-01-23},
	eprinttype = {arxiv},
	eprint = {2002.00444},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, ⛔ No {DOI} found},
}

@thesis{boufous_deep_2020,
	title = {Deep Reinforcement Learning for Complete Coverage Path Planning in Unknown Environments},
	type = {Master},
	author = {Boufous, Omar},
	date = {2020},
	keywords = {⛔ No {DOI} found},
}

@inproceedings{wu_deep_2019,
	title = {Deep Validation: Toward Detecting Real-World Corner Cases for Deep Neural Networks},
	doi = {10/gkms2j},
	shorttitle = {Deep Validation},
	abstract = {The exceptional performance of Deep neural networks ({DNNs}) encourages their deployment in safety-and dependability-critical systems. However, {DNNs} often demonstrate erroneous behaviors in real-world corner cases. Existing countermeasures center on improving the testing and bug-fixing practice. Unfortunately, building a bug-free {DNN}-based system is almost impossible currently due to its black-box nature, so anomaly detection is imperative in practice. Motivated by the idea of data validation in a traditional program, we propose and implement Deep Validation, a novel framework for detecting real-world error-inducing corner cases in a {DNN}-based system during runtime. We model the specifications of {DNNs} by resorting to their training data and cast checking input validity of {DNNs} as the problem of discrepancy estimation. Deep Validation achieves excellent detection results against various corner case scenarios across three popular datasets. Consequently, Deep Validation greatly complements existing efforts and is a crucial step toward building safe and dependable {DNN}-based systems.},
	eventtitle = {2019 49th Annual {IEEE}/{IFIP} International Conference on Dependable Systems and Networks ({DSN})},
	pages = {125--137},
	booktitle = {2019 49th Annual {IEEE}/{IFIP} International Conference on Dependable Systems and Networks ({DSN})},
	author = {Wu, Weibin and Xu, Hui and Zhong, Sanqiang and Lyu, Michael R. and King, Irwin},
	date = {2019-06},
	note = {{ISSN}: 1530-0889},
	keywords = {Data models, Detectors, Estimation, Kernel, Safety, Testing, Training data, anomaly detection, classification, neural networks, safety},
}

@article{silver_learning_2010,
	title = {Learning from Demonstration for Autonomous Navigation in Complex Unstructured Terrain},
	volume = {29},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364910369715},
	doi = {10/csnr26},
	abstract = {Rough terrain autonomous navigation continues to pose a challenge to the robotics community. Robust navigation by a mobile robot depends not only on the individual performance of perception and planning systems, but on how well these systems are coupled. When traversing complex unstructured terrain, this coupling (in the form of a cost function) has a large impact on robot behavior and performance, necessitating a robust design. This paper explores the application of Learning from Demonstration to this task for the Crusher autonomous navigation platform. Using expert examples of desired navigation behavior, mappings from both online and offline perceptual data to planning costs are learned. Challenges in adapting existing techniques to complex online planning systems and imperfect demonstration are addressed, along with additional practical considerations. The benefits to autonomous performance of this approach are examined, as well as the decrease in necessary designer effort. Experimental results are presented from autonomous traverses through complex natural environments.},
	pages = {1565--1592},
	number = {12},
	journaltitle = {The International Journal of Robotics Research},
	shortjournal = {The International Journal of Robotics Research},
	author = {Silver, David and Bagnell, J. Andrew and Stentz, Anthony},
	urldate = {2021-06-13},
	date = {2010-10-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd {STM}},
	keywords = {Field robotics, autonomous navigation, imitation learning, inverse reinforcement learning, learning from demonstration, mobile robotics},
}

@inproceedings{silver__2012,
	title = {A.: Active learning from demonstration for robust autonomous navigation},
	doi = {10/gkmrrv},
	shorttitle = {A.},
	abstract = {Abstract — Building robust and reliable autonomous naviga-tion systems that generalize across environments and operating scenarios remains a core challenge in robotics. Machine learning has proven a significant aid in this task; in recent years learning from demonstration has become especially popular, leading to improved systems while requiring less expert tuning and interaction. However, these approaches still place a burden on the expert, specifically to choose the best demonstrations to provide. This work proposes two approaches for active learning from demonstration, in which the learning system re-quests specific demonstrations from the expert. The approaches identify examples for which expert demonstration is predicted to provide useful information on concepts which are either novel or uncertain to the current system. Experimental results demonstrate both improved generalization performance and reduced expert interaction when using these approaches. I.},
	booktitle = {In: International Conference on Robotics and Automation},
	author = {Silver, David and Bagnell, J. Andrew and Stentz, Anthony},
	date = {2012},
}

@article{havoutis_learning_2019,
	title = {Learning from demonstration for semi-autonomous teleoperation},
	volume = {43},
	issn = {1573-7527},
	url = {https://doi.org/10.1007/s10514-018-9745-2},
	doi = {10/gf36qm},
	abstract = {Teleoperation in domains such as deep-sea or space often requires the completion of a set of recurrent tasks. We present a framework that uses a probabilistic approach to learn from demonstration models of manipulation tasks. We show how such a framework can be used in an underwater {ROV} teleoperation context to assist the operator. The learned representation can be used to resolve inconsistencies between the operator’s and the robot’s space in a structured manner, and as a fall-back system to perform previously learned tasks autonomously when teleoperation is not possible. We evaluate our framework with a realistic {ROV} task on a teleoperation mock-up with a group of volunteers, showing a significant decrease in time to complete the task when our approach is used. In addition, we illustrate how the system can execute previously learned tasks autonomously when the communication with the operator is lost.},
	pages = {713--726},
	number = {3},
	journaltitle = {Autonomous Robots},
	shortjournal = {Auton Robot},
	author = {Havoutis, Ioannis and Calinon, Sylvain},
	urldate = {2021-06-13},
	date = {2019-03-01},
	langid = {english},
}

@thesis{kuhnt_holistic_2020,
	title = {Holistic Temporal Situation Interpretation for Traffic Participant Prediction},
	type = {phdthesis},
	author = {Kuhnt, Florian},
	date = {2020},
	keywords = {advanced driver assistance syst, automated driving, ⛔ No {DOI} found},
}

@thesis{frank_reinforcement_2008,
	title = {Reinforcement learning in the presence of rare events},
	abstract = {We consider the task of reinforcement learning in an environment in which rare significant events occur independently of the actions selected by the controlling agent. If these events are sampled according to their natural probability of occurring, convergence of conventional reinforcement learning algorithms is likely to be slow, and the learning algorithms may exhibit high variance. In this work, we assume that we have access to a simulator, in which the rare event probabilities can be artificially altered. Then, importance sampling can be used to learn with this simulation data. We introduce algorithms for policy evaluation, using both tabular and function approximation representations of the value function. We prove that in both cases, the reinforcement learning algorithms converge. In the tabular case, we also analyze the bias and variance of our approach compared to {TD}-learning. We evaluate empirically the performance of the algorithm on random Markov Decision Processes, as well as on a large network planning task. Copyright 2008 by the author(s)/owner(s).},
	type = {Master},
	author = {Frank, Jordan and Mannor, Shie and Precup, Doina},
	date = {2008},
	note = {{ISBN}: 9781605582054
37 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {{KIW}, {ToDo}},
}

@inproceedings{jesenski_generation_2019,
	title = {Generation of Scenes in Intersections for the Validation of Highly Automated Driving Functions},
	doi = {10/ggjtpv},
	pages = {502--509},
	author = {Jesenski, Stefan and Stellet, Jan and Schiegg, Florian and Zöllner, J.},
	date = {2019-06-01},
	note = {9 citations (Semantic Scholar/{DOI}) [2021-05-04]},
	keywords = {Automotive engineering, Bayes methods, Nonhomogeneous media, Road transportation, Safety, Sociology, Statistics, {ToDo}},
}

@article{gayathri_ontology_2018,
	title = {Ontology based knowledge representation technique, domain modeling languages and planners for robotic path planning: A survey},
	volume = {4},
	issn = {24059595},
	url = {https://doi.org/10.1016/j.icte.2018.04.008},
	doi = {10/gft2b5},
	abstract = {Knowledge Representation and Reasoning ({KR} \& R) has become one of the promising fields of Artificial Intelligence. {KR} is dedicated towards representing information about the domain that can be utilized in path planning. Ontology based knowledge representation and reasoning techniques provide sophisticated knowledge about the environment for processing tasks or methods. Ontology helps in representing the knowledge about environment, events and actions that help in path planning and making robots more autonomous. Knowledge reasoning techniques can infer new conclusion and thus aids planning dynamically in a non-deterministic environment. In the initial sections, the representation of knowledge using ontology and the techniques for reasoning that could contribute in path planning are discussed in detail. In the following section, we also provide comparison of various planning domain modeling languages, ontology editors, planners and robot simulation tools.},
	pages = {69--74},
	number = {2},
	journaltitle = {{ICT} Express},
	author = {Gayathri, R. and Uma, V.},
	date = {2018},
	note = {Publisher: Elsevier B.V.
28 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Knowledge representation, Modeling languages, Ontology, Path planning, Planners, Reasoning, Semantic knowledge, Spatial, Temporal, {ToDo}},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2021-05-06},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, ⛔ No {DOI} found},
}

@article{liu_deep_2019,
	title = {Deep Learning for Generic Object Detection: A Survey},
	url = {http://arxiv.org/abs/1809.02165},
	shorttitle = {Deep Learning for Generic Object Detection},
	abstract = {Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research.},
	journaltitle = {{arXiv}:1809.02165 [cs]},
	author = {Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietikäinen, Matti},
	urldate = {2021-04-28},
	date = {2019-08-22},
	eprinttype = {arxiv},
	eprint = {1809.02165},
	note = {506 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No {DOI} found},
}

@article{gupta_towards_2020,
	title = {Towards Safer Self-Driving Through Great {PAIN} (Physically Adversarial Intelligent Networks)},
	url = {http://arxiv.org/abs/2003.10662},
	abstract = {Automated vehicles' neural networks suffer from overfit, poor generalizability, and untrained edge cases due to limited data availability. Researchers synthesize randomized edge-case scenarios to assist in the training process, though simulation introduces potential for overfit to latent rules and features. Automating worst-case scenario generation could yield informative data for improving self driving. To this end, we introduce a "Physically Adversarial Intelligent Network" ({PAIN}), wherein self-driving vehicles interact aggressively in the {CARLA} simulation environment. We train two agents, a protagonist and an adversary, using dueling double deep Q networks ({DDDQNs}) with prioritized experience replay. The coupled networks alternately seek-to-collide and to avoid collisions such that the "defensive" avoidance algorithm increases the mean-time-to-failure and distance traveled under non-hostile operating conditions. The trained protagonist becomes more resilient to environmental uncertainty and less prone to corner case failures resulting in collisions than the agent trained without an adversary.},
	journaltitle = {{arXiv}:2003.10662 [cs, eess, stat]},
	author = {Gupta, Piyush and Coleman, Demetris and Siegel, Joshua E.},
	urldate = {2021-06-10},
	date = {2020-03-24},
	eprinttype = {arxiv},
	eprint = {2003.10662},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning, ⛔ No {DOI} found},
}

@article{hanhirova_machine_2020,
	title = {A machine learning environment for evaluating autonomous driving software},
	url = {http://arxiv.org/abs/2003.03576},
	abstract = {Autonomous vehicles need safe development and testing environments. Many traffic scenarios are such that they cannot be tested in the real world. We see hybrid photorealistic simulation as a viable tool for developing {AI} (artificial intelligence) software for autonomous driving. We present a machine learning environment for detecting autonomous vehicle corner case behavior. Our environment is based on connecting the {CARLA} simulation software to {TensorFlow} machine learning framework and custom {AI} client software. The {AI} client software receives data from a simulated world via virtual sensors and transforms the data into information using machine learning models. The {AI} clients control vehicles in the simulated world. Our environment monitors the state assumed by the vehicle {AIs} to the ground truth state derived from the simulation model. Our system can search for corner cases where the vehicle {AI} is unable to correctly understand the situation. In our paper, we present the overall hybrid simulator architecture and compare different configurations. We present performance measurements from real setups, and outline the main parameters affecting the hybrid simulator performance.},
	journaltitle = {{arXiv}:2003.03576 [cs, stat]},
	author = {Hanhirova, Jussi and Debner, Anton and Hyyppä, Matias and Hirvisalo, Vesa},
	urldate = {2021-06-10},
	date = {2020-03-07},
	eprinttype = {arxiv},
	eprint = {2003.03576},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No {DOI} found},
}

@article{nesti_detecting_2021,
	title = {Detecting Adversarial Examples by Input Transformations, Defense Perturbations, and Voting},
	url = {http://arxiv.org/abs/2101.11466},
	abstract = {Over the last few years, convolutional neural networks ({CNNs}) have proved to reach super-human performance in visual recognition tasks. However, {CNNs} can easily be fooled by adversarial examples, i.e., maliciously-crafted images that force the networks to predict an incorrect output while being extremely similar to those for which a correct output is predicted. Regular adversarial examples are not robust to input image transformations, which can then be used to detect whether an adversarial example is presented to the network. Nevertheless, it is still possible to generate adversarial examples that are robust to such transformations. This paper extensively explores the detection of adversarial examples via image transformations and proposes a novel methodology, called {\textbackslash}textit\{defense perturbation\}, to detect robust adversarial examples with the same input transformations the adversarial examples are robust to. Such a {\textbackslash}textit\{defense perturbation\} is shown to be an effective counter-measure to robust adversarial examples. Furthermore, multi-network adversarial examples are introduced. This kind of adversarial examples can be used to simultaneously fool multiple networks, which is critical in systems that use network redundancy, such as those based on architectures with majority voting over multiple {CNNs}. An extensive set of experiments based on state-of-the-art {CNNs} trained on the Imagenet dataset is finally reported.},
	journaltitle = {{arXiv}:2101.11466 [cs]},
	author = {Nesti, Federico and Biondi, Alessandro and Buttazzo, Giorgio},
	urldate = {2021-06-10},
	date = {2021-01-27},
	eprinttype = {arxiv},
	eprint = {2101.11466},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No {DOI} found},
}

@article{rossolini_increasing_2021,
	title = {Increasing the Confidence of Deep Neural Networks by Coverage Analysis},
	url = {http://arxiv.org/abs/2101.12100},
	abstract = {The great performance of machine learning algorithms and deep neural networks in several perception and control tasks is pushing the industry to adopt such technologies in safety-critical applications, as autonomous robots and self-driving vehicles. At present, however, several issues need to be solved to make deep learning methods more trustworthy, predictable, safe, and secure against adversarial attacks. Although several methods have been proposed to improve the trustworthiness of deep neural networks, most of them are tailored for specific classes of adversarial examples, hence failing to detect other corner cases or unsafe inputs that heavily deviate from the training samples. This paper presents a lightweight monitoring architecture based on coverage paradigms to enhance the model robustness against different unsafe inputs. In particular, four coverage analysis methods are proposed and tested in the architecture for evaluating multiple detection logics. Experimental results show that the proposed approach is effective in detecting both powerful adversarial examples and out-of-distribution inputs, introducing limited extra-execution time and memory requirements.},
	journaltitle = {{arXiv}:2101.12100 [cs]},
	author = {Rossolini, Giulio and Biondi, Alessandro and Buttazzo, Giorgio Carlo},
	urldate = {2021-06-10},
	date = {2021-01-28},
	eprinttype = {arxiv},
	eprint = {2101.12100},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, ⛔ No {DOI} found},
}

@article{tuncali_requirements-driven_2020,
	title = {Requirements-Driven Test Generation for Autonomous Vehicles With Machine Learning Components},
	volume = {5},
	issn = {2379-8904},
	doi = {10/gjjg2f},
	abstract = {Autonomous vehicles are complex systems that are challenging to test and debug. A requirements-driven approach to the development process can decrease the resources required to design and test these systems, while simultaneously increasing the reliability. We present a testing framework that uses signal temporal logic ({STL}), which is a precise and unambiguous requirements language. Our framework evaluates test cases against the {STL} formulae and additionally uses the requirements to automatically identify test cases that fail to satisfy the requirements. One of the key features of our tool is the support for machine learning ({ML}) components in the system design, such as deep neural networks. The framework allows evaluation of the control algorithms, including the {ML} components, and it also includes models of {CCD} camera, lidar, and radar sensors, as well as the vehicle environment. We use multiple methods to generate test cases, including covering arrays, which is an efficient method to search discrete variable spaces. The resulting test cases can be used to debug the controller design by identifying controller behaviors that do not satisfy requirements. The test cases can also enhance the testing phase of development by identifying critical corner cases that correspond to the limits of the system's allowed behaviors. We present {STL} requirements for an autonomous vehicle system, which capture both component-level and system-level behaviors. Additionally, we present three driving scenarios and demonstrate how our requirements-driven testing framework can be used to identify critical system behaviors, which can be used to support the development process.},
	pages = {265--280},
	number = {2},
	journaltitle = {{IEEE} Transactions on Intelligent Vehicles},
	author = {Tuncali, Cumhur Erkan and Fainekos, Georgios and Prokhorov, Danil and Ito, Hisahiro and Kapinski, James},
	date = {2020-06},
	note = {Conference Name: {IEEE} Transactions on Intelligent Vehicles},
	keywords = {Autonomous vehicles, Charge coupled devices, Closed loop systems, Laser radar, Sensors, Test pattern generators, cyber-physical systems, system validation, system verification},
}

@inproceedings{ben_braiek_deepevolution_2019,
	title = {{DeepEvolution}: A Search-Based Testing Approach for Deep Neural Networks},
	doi = {10/gkhs8s},
	shorttitle = {{DeepEvolution}},
	abstract = {The increasing inclusion of Deep Learning ({DL}) models in safety-critical systems such as autonomous vehicles have led to the development of multiple model-based {DL} testing techniques. One common denominator of these testing techniques is the automated generation of test cases, e.g., new inputs transformed from the original training data with the aim to optimize some test adequacy criteria. So far, the effectiveness of these approaches has been hindered by their reliance on random fuzzing or transformations that do not always produce test cases with a good diversity. To overcome these limitations, we propose, {DeepEvolution}, a novel search-based approach for testing {DL} models that relies on metaheuristics to ensure a maximum diversity in generated test cases. We assess the effectiveness of {DeepEvolution} in testing computer-vision {DL} models and found that it significantly increases the neuronal coverage of generated test cases. Moreover, using {DeepEvolution}, we could successfully find several corner-case behaviors. Finally, {DeepEvolution} outperformed Tensorfuzz (a coverage-guided fuzzing tool developed at Google Brain) in detecting latent defects introduced during the quantization of the models. These results suggest that search-based approaches can help build effective testing tools for {DL} systems.},
	eventtitle = {2019 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	pages = {454--458},
	booktitle = {2019 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	author = {Ben Braiek, Houssem and Khomh, Foutse},
	date = {2019-09},
	note = {{ISSN}: 2576-3148},
	keywords = {Computational modeling, Computer Vision, Deep Learning, Metamorphic Testing, Neurons, Quantization (signal), Search Based Testing, Software, Software Testing, Software testing, Space exploration},
}

@inproceedings{weiss_uncertainty-wizard_2021,
	title = {Uncertainty-Wizard: Fast and User-Friendly Neural Network Uncertainty Quantification},
	doi = {10/gkhs8k},
	shorttitle = {Uncertainty-Wizard},
	abstract = {Uncertainty and confidence have been shown to be useful metrics in a wide variety of techniques proposed for deep learning testing, including test data selection and system supervision. We present Uncertainty-Wizard, a tool that allows to quantify such uncertainty and confidence in artificial neural networks. It is built on top of the industry-leading {TF}.{KERAS} deep learning {API} and it provides a near-transparent and easy to understand interface. At the same time, it includes major performance optimizations that we benchmarked on two different machines and different configurations.},
	eventtitle = {2021 14th {IEEE} Conference on Software Testing, Verification and Validation ({ICST})},
	pages = {436--441},
	booktitle = {2021 14th {IEEE} Conference on Software Testing, Verification and Validation ({ICST})},
	author = {Weiss, Michael and Tonella, Paolo},
	date = {2021-04},
	note = {{ISSN}: 2159-4848},
	keywords = {Artificial neural networks, Deep learning, Fault tolerance, Fault tolerant systems, Measurement, Software testing, Uncertainty, art neural networks, fault tolerance, software reliability, software testing, software tools},
}

@inproceedings{stocco_misbehaviour_2020,
	location = {New York, {NY}, {USA}},
	title = {Misbehaviour prediction for autonomous driving systems},
	isbn = {978-1-4503-7121-6},
	url = {https://doi.org/10.1145/3377811.3380353},
	doi = {10/gjjg2b},
	series = {{ICSE} '20},
	abstract = {Deep Neural Networks ({DNNs}) are the core component of modern autonomous driving systems. To date, it is still unrealistic that a {DNN} will generalize correctly to all driving conditions. Current testing techniques consist of offline solutions that identify adversarial or corner cases for improving the training phase. In this paper, we address the problem of estimating the confidence of {DNNs} in response to unexpected execution contexts with the purpose of predicting potential safety-critical misbehaviours and enabling online healing of {DNN}-based vehicles. Our approach {SelfOracle} is based on a novel concept of self-assessment oracle, which monitors the {DNN} confidence at runtime, to predict unsupported driving scenarios in advance. {SelfOracle} uses autoencoder-and time series-based anomaly detection to reconstruct the driving scenarios seen by the car, and to determine the confidence boundary between normal and unsupported conditions. In our empirical assessment, we evaluated the effectiveness of different variants of {SelfOracle} at predicting injected anomalous driving contexts, using {DNN} models and simulation environment from Udacity. Results show that, overall, {SelfOracle} can predict 77\% misbehaviours, up to six seconds in advance, outperforming the online input validation approach of {DeepRoad}.},
	pages = {359--371},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd International Conference on Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Stocco, Andrea and Weiss, Michael and Calzana, Marco and Tonella, Paolo},
	urldate = {2021-06-10},
	date = {2020-06-27},
	keywords = {anomaly detection, deep learning, misbehaviour prediction, testing},
}

@inproceedings{stocco_towards_2020,
	title = {Towards Anomaly Detectors that Learn Continuously},
	doi = {10/gkhs8c},
	abstract = {In this paper, we first discuss the challenges of adapting an already trained {DNN}-based anomaly detector with knowledge mined during the execution of the main system. Then, we present a framework for the continual learning of anomaly detectors, which records in-field behavioural data to determine what data are appropriate for adaptation. We evaluated our framework to improve an anomaly detector taken from the literature, in the context of misbehavior prediction for self-driving cars. Our results show that our solution can reduce the false positive rate by a large margin and adapt to nominal behaviour changes while maintaining the original anomaly detection capability.},
	eventtitle = {2020 {IEEE} International Symposium on Software Reliability Engineering Workshops ({ISSREW})},
	pages = {201--208},
	booktitle = {2020 {IEEE} International Symposium on Software Reliability Engineering Workshops ({ISSREW})},
	author = {Stocco, Andrea and Tonella, Paolo},
	date = {2020-10},
	keywords = {{AI} Testing, Adaptation models, Anomaly Detection, Anomaly detection, Autonomous Driving Systems, Continual Learning, Data models, Detectors, Monitoring, Task analysis, Training},
}

@article{chen_interpretable_2020,
	title = {Interpretable End-to-end Urban Autonomous Driving with Latent Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/2001.08726},
	abstract = {Unlike popular modularized framework, end-to-end autonomous driving seeks to solve the perception, decision and control problems in an integrated way, which can be more adapting to new scenarios and easier to generalize at scale. However, existing end-to-end approaches are often lack of interpretability, and can only deal with simple driving tasks like lane keeping. In this paper, we propose an interpretable deep reinforcement learning method for end-to-end autonomous driving, which is able to handle complex urban scenarios. A sequential latent environment model is introduced and learned jointly with the reinforcement learning process. With this latent model, a semantic birdeye mask can be generated, which is enforced to connect with a certain intermediate property in today’s modularized framework for the purpose of explaining the behaviors of learned policy. The latent space also signiﬁcantly reduces the sample complexity of reinforcement learning. Comparison tests with a simulated autonomous car in {CARLA} show that the performance of our method in urban scenarios with crowded surrounding vehicles dominates many baselines including {DQN}, {DDPG}, {TD}3 and {SAC}. Moreover, through masked outputs, the learned policy is able to provide a better explanation of how the car reasons about the driving environment. The codes and videos of this work are available at our github repo† and project website‡.},
	journaltitle = {{arXiv}:2001.08726 [cs]},
	author = {Chen, Jianyu and Li, Shengbo Eben and Tomizuka, Masayoshi},
	urldate = {2021-03-31},
	date = {2020-07-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2001.08726},
	note = {7 citations (Semantic Scholar/{arXiv}) [2021-03-31]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, {KIW}, {ToDo}, ⛔ No {DOI} found},
}

@article{wulfmeier_incorporating_2016,
	title = {Incorporating Human Domain Knowledge into Large Scale Cost Function Learning},
	url = {http://arxiv.org/abs/1612.04318},
	abstract = {Recent advances have shown the capability of Fully Convolutional Neural Networks ({FCN}) to model cost functions for motion planning in the context of learning driving preferences purely based on demonstration data from human drivers. While pure learning from demonstrations in the framework of Inverse Reinforcement Learning ({IRL}) is a promising approach, we can benefit from well informed human priors and incorporate them into the learning process. Our work achieves this by pretraining a model to regress to a manual cost function and refining it based on Maximum Entropy Deep Inverse Reinforcement Learning. When injecting prior knowledge as pretraining for the network, we achieve higher robustness, more visually distinct obstacle boundaries, and the ability to capture instances of obstacles that elude models that purely learn from demonstration data. Furthermore, by exploiting these human priors, the resulting model can more accurately handle corner cases that are scarcely seen in the demonstration data, such as stairs, slopes, and underpasses.},
	pages = {1--8},
	issue = {Nips 2016},
	author = {Wulfmeier, Markus and Rao, Dushyant and Posner, Ingmar},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1612.04318},
	note = {13 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {{KIW}, {ToDo}},
}

@inproceedings{ranjbar_scene_2020,
	title = {Scene Novelty Prediction from Unsupervised Discriminative Feature Learning},
	doi = {10/gkhs3t},
	abstract = {Deep learning approaches are widely explored in safety-critical autonomous driving systems on various tasks. Network models, trained on big data, map input to probable prediction results. However, it is unclear how to get a measure of confidence on this prediction at the test time.Our approach to gain this additional information is to estimate how similar test data is to the training data that the model was trained on. We map training instances onto a feature space that is the most discriminative among them. We then model the entire training set as a Gaussian distribution in that feature space. The novelty of the test data is characterized by its low probability of being in that distribution, or equivalently a large Mahalanobis distance in the feature space.Our distance metric in the discriminative feature space achieves a better novelty prediction performance than the state-of-the-art methods on most classes in {CIFAR}-10 and {ImageNet}. Using semantic segmentation as a proxy task often needed for autonomous driving, we show that our unsupervised novelty prediction correlates with the performance of a segmentation network trained on full pixel-wise annotations. These experimental results demonstrate potential applications of our method upon identifying scene familiarity and quantifying the confidence in autonomous driving actions.},
	eventtitle = {2020 {IEEE} 23rd International Conference on Intelligent Transportation Systems ({ITSC})},
	pages = {1--7},
	booktitle = {2020 {IEEE} 23rd International Conference on Intelligent Transportation Systems ({ITSC})},
	author = {Ranjbar, Arian and Yeh, Chun-Hsiao and Hornauer, Sascha and Yu, Stella X. and Chan, Ching-Yao},
	date = {2020-09},
	keywords = {Anomaly detection, Autonomous vehicles, Data models, Measurement, Task analysis, Training, Training data},
}

@article{sadat_diverse_2021,
	title = {Diverse Complexity Measures for Dataset Curation in Self-driving},
	url = {http://arxiv.org/abs/2101.06554},
	abstract = {Modern self-driving autonomy systems heavily rely on deep learning. As a consequence, their performance is inﬂuenced signiﬁcantly by the quality and richness of the training data. Data collecting platforms can generate many hours of raw data in a daily basis, however, it is not feasible to label everything. It is thus of key importance to have a mechanism to identify ”what to label”. Active learning approaches identify examples to label, but their interestingness is tied to a ﬁxed model performing a particular task. These assumptions are not valid in self-driving, where we have to solve a diverse set of tasks (i.e., perception, and motion forecasting) and our models evolve over time frequently. In this paper we introduce a novel approach and propose a new data selection method that exploits a diverse set of criteria that quantize interestingness of trafﬁc scenes. Our experiments on a wide range of tasks and models show that the proposed curation pipeline is able to select datasets that lead to better generalization and higher performance.},
	journaltitle = {{arXiv}:2101.06554 [cs]},
	author = {Sadat, Abbas and Segal, Sean and Casas, Sergio and Tu, James and Yang, Bin and Urtasun, Raquel and Yumer, Ersin},
	urldate = {2021-05-10},
	date = {2021-01-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2101.06554},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, ⛔ No {DOI} found},
}

@inproceedings{bock_data_2018,
	title = {Data Basis for Scenario-Based Validation of {HAD} on Highways},
	author = {Bock, Julian},
	date = {2018},
	keywords = {⛔ No {DOI} found},
}

@article{kamath_mdetr_2021,
	title = {{MDETR} -- Modulated Detection for End-to-End Multi-Modal Understanding},
	url = {http://arxiv.org/abs/2104.12763},
	abstract = {Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose {MDETR}, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on {GQA} and {CLEVR}. The code and models are available at https://github.com/ashkamath/mdetr.},
	journaltitle = {{arXiv}:2104.12763 [cs]},
	author = {Kamath, Aishwarya and Singh, Mannat and {LeCun}, Yann and Misra, Ishan and Synnaeve, Gabriel and Carion, Nicolas},
	urldate = {2021-05-10},
	date = {2021-04-26},
	eprinttype = {arxiv},
	eprint = {2104.12763},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, {ToDo}, ⛔ No {DOI} found},
}

@article{provine_ontology-based_2004,
	title = {Ontology-Based Methods for Enhancing Autonomous Vehicle Path Planning},
	url = {https://www.nist.gov/publications/ontology-based-methods-enhancing-autonomous-vehicle-path-planning},
	abstract = {We report the results of a first implementation demonstrating the use of an ontology to support reasoning about obstacles to improve the capabilities and perfor},
	author = {Provine, R. and Schlenoff, Craig I. and Balakirsky, Stephen B. and Smith, S. and Uschold, M.},
	urldate = {2021-05-17},
	date = {2004-11-01},
	langid = {english},
	note = {Last Modified: 2017-02-17T12:55-05:00},
	keywords = {{ToDo}, ⛔ No {DOI} found},
}

@article{muhayyuddin_ontological_2017,
	title = {Ontological Physics-based Motion Planning for Manipulation},
	url = {http://arxiv.org/abs/1709.09271},
	abstract = {Robotic manipulation involves actions where contacts occur between the robot and the objects. In this scope, the availability of physics-based engines allows motion planners to comprise dynamics between rigid bodies, which is necessary for planning this type of actions. However, physics-based motion planning is computationally intensive due to the high dimensionality of the state space and the need to work with a low integration step to find accurate solutions. On the other hand, manipulation actions change the environment and conditions further actions and motions. To cope with this issue, the representation of manipulation actions using ontologies enables a semantic-based inference process that alleviates the computational cost of motion planning. This paper proposes a manipulation planning framework where physics-based motion planning is enhanced with ontological knowledge representation and reasoning. The proposal has been implemented and is illustrated and validated with a simple example. Its use in grasping tasks in cluttered environments is currently under development.},
	journaltitle = {{arXiv}:1709.09271 [cs]},
	author = {Muhayyuddin, M. and Akbari, Aliakbar and Rosell, Jan},
	urldate = {2021-05-17},
	date = {2017-10-29},
	eprinttype = {arxiv},
	eprint = {1709.09271},
	keywords = {Computer Science - Robotics, ⛔ No {DOI} found},
}

@article{zhao_core_nodate,
	title = {Core Ontologies for Safe Autonomous Driving},
	abstract = {Representing the knowledge of driving environments in a structured machine-readable format is necessary for safe autonomous driving. We use ontologies to represent the knowledge of maps, driving paths, and driving environments to improve safety for smart vehicles. In this paper, we introduce core ontologies that are used for developing Advanced Driver Assistance Systems. The ontologies can be reused and extended for constructing Knowledge Base for smart vehicles as well as for implementing diﬀerent types of Advanced Driver Assistance Systems.},
	pages = {4},
	author = {Zhao, Lihua and Ichise, Ryutaro and Mita, Seiichi and Sasaki, Yutaka},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@online{hecht_self-driving_2020,
	title = {Self-driving vehicles: Many challenges remain for autonomous navigation},
	url = {https://www.laserfocusworld.com/test-measurement/article/14169619/selfdriving-vehicles-many-challenges-remain-for-autonomous-navigation},
	shorttitle = {Self-driving vehicles},
	abstract = {Driver assistance is easy, but it’s much harder for robots to take over the whole job of driving anywhere at any time. What’s coming first is autonomy on well-maintained roads in good weather.},
	titleaddon = {Laser Focus World},
	author = {Hecht, Jeff},
	urldate = {2021-05-20},
	date = {2020-04},
	langid = {english},
}

@inproceedings{cho_learning-based_2018,
	title = {Learning-Based Model Predictive Control Under Signal Temporal Logic Specifications},
	doi = {10/gj3vtr},
	abstract = {This paper presents a control strategy synthesis method for dynamical systems with differential constraints while satisfying a set of given rules in consideration of their importances. A special attention is given to situations where all rules cannot be met in order to fulfill a given task. Such dilemmas compel us to make a decision on the degree of satisfaction of each rule including which rule should be maintained or not. In this work, we propose a learning-based model predictive control method in order to solve this problem, where a key insight is to combine a learning method and traditional control scheme so that the designed controller behaves close to human experts. A rule is represented as a signal temporal logic ({STL}) formula. A robustness slackness, a margin to the satisfaction of the rule, is learned from expert's demonstrations using Gaussian process regression. The learned margin is used in a model predictive control procedure, which helps to decide how much to obey each rule, even ignoring specific rules. In track driving simulation, we show that the proposed method generates human-like behavior and efficiently handles dilemmas as human teachers do.},
	eventtitle = {2018 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {7322--7329},
	booktitle = {2018 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	author = {Cho, Kyunghoon and Oh, Songhwai},
	date = {2018-05},
	note = {{ISSN}: 2577-087X},
	keywords = {Collision avoidance, Gaussian processes, Predictive control, Robustness, Service robots, Task analysis, {ToDo}},
}

@article{cho_deep_2019,
	title = {Deep Predictive Autonomous Driving Using Multi-Agent Joint Trajectory Prediction and Traffic Rules},
	issn = {21530866},
	doi = {10/ghk254},
	abstract = {Autonomous driving is a challenging problem because the autonomous vehicle must understand complex and dynamic environment. This understanding consists of predicting future behavior of nearby vehicles and recognizing predefined rules. It is observed that not all rules have equivalent values, and the priority of the rules may change depending on the situation or the driver's driving style. In this work, we jointly reason both a future trajectories of vehicles and degree of satisfaction of each rule in the deep learning framework. Joint reasoning allows modeling interactions between vehicles, and leads to better prediction results. A rule is represented as a signal temporal logic ({STL}) formula, and a robustness slackness, a margin to the satisfaction of the rule, is predicted for the both autonomous and other vehicle, in addition to future trajectories. Learned robustness slackness decides which rule should be prioritized for the given situation for the autonomous vehicle, and filter out non-valid predicted trajectories for surrounding vehicles. The predicted information from the deep learning framework is used in model predictive control ({MPC}), which allows the autonomous vehicle navigate efficiently and safely. We test the feasibility of our approach in publicly available {NGSIM} datasets. Proposed method shows a driving style similar to the human one and considers the safety related to the rules through the future prediction of the surrounding vehicles.},
	pages = {2076--2081},
	journaltitle = {{IEEE} International Conference on Intelligent Robots and Systems},
	author = {Cho, Kyunghoon and Ha, Timothy and Lee, Gunmin and Oh, Songhwai},
	date = {2019},
	note = {{ISBN}: 9781728140049
5 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {{ToDo}},
}

@article{hohenecker_ontology_2020,
	title = {Ontology Reasoning with Deep Neural Networks},
	volume = {68},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1808.07980},
	doi = {10/gj3fbq},
	abstract = {The ability to conduct logical reasoning is a fundamental aspect of intelligent human behavior, and thus an important problem along the way to human-level artificial intelligence. Traditionally, logic-based symbolic methods from the field of knowledge representation and reasoning have been used to equip agents with capabilities that resemble human logical reasoning qualities. More recently, however, there has been an increasing interest in using machine learning rather than logic-based symbolic formalisms to tackle these tasks. In this paper, we employ state-of-the-art methods for training deep neural networks to devise a novel model that is able to learn how to effectively perform logical reasoning in the form of basic ontology reasoning. This is an important and at the same time very natural logical reasoning task, which is why the presented approach is applicable to a plethora of important real-world problems. We present the outcomes of several experiments, which show that our model is able to learn to perform highly accurate ontology reasoning on very large, diverse, and challenging benchmarks. Furthermore, it turned out that the suggested approach suffers much less from different obstacles that prohibit logic-based symbolic reasoning, and, at the same time, is surprisingly plausible from a biological point of view.},
	journaltitle = {Journal of Artificial Intelligence Research},
	shortjournal = {jair},
	author = {Hohenecker, Patrick and Lukasiewicz, Thomas},
	urldate = {2021-05-17},
	date = {2020-07-07},
	eprinttype = {arxiv},
	eprint = {1808.07980},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{paull_towards_2012,
	location = {Vilamoura-Algarve, Portugal},
	title = {Towards an Ontology for Autonomous Robots},
	isbn = {978-1-4673-1736-8 978-1-4673-1737-5 978-1-4673-1735-1},
	url = {http://ieeexplore.ieee.org/document/6386119/},
	doi = {10/gft2b3},
	abstract = {The {IEEE} {RAS} Ontologies for Robotics and Automation Working Group is dedicated to developing a methodology for knowledge representation and reasoning in robotics and automation. As part of this working group, the Autonomous Robots sub-group is tasked with developing ontology modules for autonomous robots. This paper describes the work in progress on the development of ontologies for autonomous systems. For autonomous systems, the focus is on the cooperation, coordination, and communication of multiple unmanned aerial vehicles ({UAVs}), unmanned ground vehicles ({UGVs}), and autonomous underwater vehicles ({AUVs}). The ontologies serve as a framework for working out concepts of employment with multiple vehicles for a variety of operational scenarios with emphasis on collaborative and cooperative missions.},
	eventtitle = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS} 2012)},
	pages = {1359--1364},
	booktitle = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
	publisher = {{IEEE}},
	author = {Paull, Liam and Severac, Gaetan and Raffo, Guilherme V. and Angel, Julian Mauricio and Boley, Harold and Durst, Phillip J and Gray, Wendell and Habib, Maki and Nguyen, Bao and Ragavan, S. Veera and Saeedi G., Sajad and Sanz, Ricardo and Seto, Mae and Stefanovski, Aleksandar and Trentini, Michael and Li, Howard},
	urldate = {2021-05-17},
	date = {2012-10},
	langid = {english},
}

@inproceedings{zhao_ontology-based_2018,
	title = {An ontology-based approach towards coupling task and path planning for the simulation of manipulation tasks},
	rights = {info:eu-repo/semantics/{openAccess}},
	url = {https://doi.org/10.1109/AICCSA.2018.8612805},
	doi = {10/gj3d9d},
	abstract = {Simulating complex industrial manipulation tasks (e.g., assembly, disassembly and maintenance tasks) under strong geometric constraints in a virtual environment, requires the joint usage of task and path planning, not only to compute a sequence of primitive actions (i.e., a task plan) at task planning level to identify the order to manipulate different objects (e.g., assembly order), but also to generate and validate motions for each of these primitive actions in a virtual environment by computing valid collision-free paths for these actions at path planning level. Although task and path planning have been respectively welly discussed by artificial intelligence and robotic domain, the link between them still remains an open issue, in particular because path planning for a primitive action often uses purely geometric data. This purely geometric path planning suffers from the classical failures (i.e., high-possibility of failure, high processing time and low path relevance) of automated path planning techniques when dealing with complex geometric models. Thus, it can possibly lead to high computational time of the joint task and path planning process and can probably produce a poor implementation of a task plan. Instead of geometric data, involving higher abstraction level information related to a task to be performed in the path planning of a primitive action could lead to a better relevance of simulations. In this work, we propose an ontology-based approach to generate a specific path planning query for a primitive action, using a well-structured task-oriented knowledge model. This specific path planning query aims at obtaining an increased control on the path planning process of the targeted primitive action.},
	eventtitle = {2018 {IEEE}/{ACS} 15th International Conference on Computer Systems and Applications ({AICCSA})},
	pages = {1--8},
	booktitle = {2018 {IEEE}/{ACS} 15th International Conference on Computer Systems and Applications ({AICCSA})},
	author = {Zhao, Yingshen and Fillatreau, Philippe and Karray, Mohamed Hedi and Archimède, Bernard},
	urldate = {2021-05-17},
	date = {2018},
	langid = {english},
}

@online{sae_j3016c_2021,
	title = {J3016C: Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles - {SAE} International},
	url = {https://www.sae.org/standards/content/j3016_202104/},
	shorttitle = {J3016C},
	abstract = {This document describes [motor] vehicle driving automation systems that perform part or all of the dynamic driving task ({DDT}) on a sustained basis. It provides a taxonomy with detailed definitions for six levels of driving automation, ranging from no driving automation (Level 0) to full driving auto},
	author = {{SAE}},
	urldate = {2021-05-10},
	date = {2021},
}

@online{noauthor_out--distribution_nodate,
	title = {Out-of-distribution Detection and Generation using Soft Brownian Offset Sampling and Autoencoders},
	url = {https://arxiv.org/abs/2105.02965},
	urldate = {2021-05-10},
}

@thesis{hosseini_conception_nodate,
	title = {Conception of Advanced Driver Assistance Systems for Precise and Safe Control of Teleoperated Road Vehicles in Urban Environments},
	url = {https://mediatum.ub.tum.de/doc/1415643/1415643.pdf},
	type = {phdthesis},
	author = {Hosseini, Seyed},
	urldate = {2021-05-08},
}

@online{sharma_ai_2020,
	title = {{AI} Can See Clearly Now: {GANs} Take the Jitters Out of Video Calls},
	url = {https://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/},
	shorttitle = {{AI} Can See Clearly Now},
	abstract = {{NVIDIA} Maxine, a cloud-{AI} video-streaming platform, will use {GANs} to boost bandwidth performance.},
	titleaddon = {The Official {NVIDIA} Blog},
	author = {Sharma, Sid},
	urldate = {2021-05-10},
	date = {2020-10-05},
	langid = {american},
}

@article{radek_kuchta_smart_2014,
	title = {Smart City Concept, Applications and Services},
	volume = {03},
	issn = {21670919},
	url = {https://www.omicsgroup.org/journals/smart-city-concept-applications-and-services-2167-0919-117.php?aid=33684},
	doi = {10/gjw6dh},
	abstract = {The purpose of this article is to summarize the current state of understanding the smart city concept and to present a proposed communication platform for the development of city services. The first part of the article is an introduction and definition of a smart city concept. This introduction gives an overview of various aspects - city services, smart infrastructure and facilities, using information and communication technologies, interconnection, feedback, and electronic and digital applications. The next part addresses individual challenges for the planning, development, and operation of cities. New solutions allow for use of different data on cities and meet the request for better city services. An overview of smart cities applications and services is given in the next section of the article. The deployment, implementation and approval of innovative internet based services and applications have to be made in order to permit facing the challenges of advanced cities. An overview of some aspects related to the technological solution of services is discussed in the summary section.},
	number = {2},
	journaltitle = {Journal of Telecommunications System \& Management},
	shortjournal = {J Telecommun Syst Manage},
	author = {Radek Kuchta, Radovan Novotny},
	urldate = {2021-05-08},
	date = {2014},
	langid = {english},
}

@inproceedings{camara_propagation_2010,
	title = {Propagation of Public Safety Warning Messages: A Delay Tolerant Network Approach},
	doi = {10/fh8366},
	shorttitle = {Propagation of Public Safety Warning Messages},
	abstract = {This work presents an auxiliary mechanism to aid in the distribution of warning messages for Emergency Alert Systems ({EAS}). The main objectives of the proposed mechanism are to speed up and broaden the warning messages distribution process to provide to the public faster access to crucial information. {EAS} are public safety message systems designed to enable authorities to address the population in case of an emergency. This kind of system has been in use for a long time, however traditionally they are composed of radio/{TV} broadcast messages or sirens spread through endangered regions. This system on the other hand addresses the next generation of {EAS} systems that will be based on wireless computer networks and satellite technologies. The method proposed here is a complementary way to spread warning messages that not only successfully broadens the {EAS} reachability but also significantly speeds up the messages distribution process.},
	eventtitle = {2010 {IEEE} Wireless Communication and Networking Conference},
	pages = {1--6},
	booktitle = {2010 {IEEE} Wireless Communication and Networking Conference},
	author = {Câmara, Daniel and Bonnet, Christian and Filali, Fethi},
	date = {2010-04},
	note = {{ISSN}: 1558-2612},
	keywords = {Communications Society, Disruption tolerant networking, Intelligent sensors, Message systems, Mobile communication, Propagation delay, Radio broadcasting, Road safety, {TV} broadcasting, Vehicle safety},
}

@inproceedings{alshammari_intelligent_2019,
	title = {Intelligent Multi-Camera Video Surveillance System for Smart City Applications},
	doi = {10/gjw482},
	abstract = {One of the challenges that many security systems face is the inability to allow the system to perform certain functions without an operator monitoring the progress of the system, automatically. People cannot operate twenty-four hours a day. Human operators have a limit to operate during the time they are awake. People will always require sleep and even when they are awake they cannot sit in one place for a long time without any distraction. That is why there are unique features in the multi-camera video system that can make significant impacts in the security industry. The technical aspect of the technology is exceptional and it can assist many people to counter security challenges they face in their daily lives. It is important to consider these aspects when selecting which type of security systems are needed to adopt. Our approach could detect and recognize a human target from videos taken from cameras mounted on the wall to cover a target area. The proposed approach consists of detection and tracking of any targets that might be a threat. The decision-making process then decides whether or not the person is a threat or not.},
	eventtitle = {2019 {IEEE} 9th Annual Computing and Communication Workshop and Conference ({CCWC})},
	pages = {0317--0323},
	booktitle = {2019 {IEEE} 9th Annual Computing and Communication Workshop and Conference ({CCWC})},
	author = {Alshammari, Abdullah and Rawat, Danda B.},
	date = {2019-01},
	keywords = {Cameras, Cloud computing, Delays, Edge computing, Image color analysis, {IoT}, Security, Video surveillance, communication, intelligent, security, smart city, surveillance},
}

@inproceedings{neumeier_measuring_2019,
	location = {Paris, France},
	title = {Measuring the Feasibility of Teleoperated Driving in Mobile Networks},
	isbn = {978-3-903176-17-1},
	url = {https://ieeexplore.ieee.org/document/8784466/},
	doi = {10/gjw47h},
	abstract = {Teleoperated Driving is the remote control driving of a vehicle by a human driver. The concept of Teleoperated Driving requires the use of mobile networks, which typically experience variable throughput, variable latency and uneven network coverage. To investigate whether Teleoperated Driving can be possible with contemporary mobile networks, we have conducted measurements while driving with vehicles in the real world. We used complementary measurement setups to obtain results that can be compared. The dataset consists of about 5200 km (4660 minutes) driving measurements. Results show that Teleoperated Driving could be possible, but the high variance of network parameters makes it difﬁcult to use the system at all times. It appears that the speed of the vehicle and the distance to the base station may not inﬂuence Teleoperated Driving, while handover with changed radio technology, signal strength and distance to the teleoperation station may have an impact. Possible mitigations to overcome these problems along with a basic whitelisting approach is discussed.},
	eventtitle = {2019 Network Traffic Measurement and Analysis Conference ({TMA})},
	pages = {113--120},
	booktitle = {2019 Network Traffic Measurement and Analysis Conference ({TMA})},
	publisher = {{IEEE}},
	author = {Neumeier, Stefan and Walelgne, Ermias Andargie and Bajpai, Vaibhav and Ott, Jorg and Facchi, Christian},
	urldate = {2021-05-08},
	date = {2019-06},
	langid = {english},
}

@article{roy_does_2021,
	title = {Does Your Dermatology Classifier Know What It Doesn't Know? Detecting the Long-Tail of Unseen Conditions},
	url = {http://arxiv.org/abs/2104.03829},
	shorttitle = {Does Your Dermatology Classifier Know What It Doesn't Know?},
	abstract = {We develop and rigorously evaluate a deep learning based system that can accurately classify skin conditions while detecting rare conditions for which there is not enough data available for training a confident classifier. We frame this task as an out-of-distribution ({OOD}) detection problem. Our novel approach, hierarchical outlier detection ({HOD}) assigns multiple abstention classes for each training outlier class and jointly performs a coarse classification of inliers vs. outliers, along with fine-grained classification of the individual classes. We demonstrate the effectiveness of the {HOD} loss in conjunction with modern representation learning approaches ({BiT}, {SimCLR}, {MICLe}) and explore different ensembling strategies for further improving the results. We perform an extensive subgroup analysis over conditions of varying risk levels and different skin types to investigate how the {OOD} detection performance changes over each subgroup and demonstrate the gains of our framework in comparison to baselines. Finally, we introduce a cost metric to approximate downstream clinical impact. We use this cost metric to compare the proposed method against a baseline system, thereby making a stronger case for the overall system effectiveness in a real-world deployment scenario.},
	journaltitle = {{arXiv}:2104.03829 [cs]},
	author = {Roy, Abhijit Guha and Ren, Jie and Azizi, Shekoofeh and Loh, Aaron and Natarajan, Vivek and Mustafa, Basil and Pawlowski, Nick and Freyberg, Jan and Liu, Yuan and Beaver, Zach and Vo, Nam and Bui, Peggy and Winter, Samantha and {MacWilliams}, Patricia and Corrado, Greg S. and Telang, Umesh and Liu, Yun and Cemgil, Taylan and Karthikesalingam, Alan and Lakshminarayanan, Balaji and Winkens, Jim},
	urldate = {2021-05-06},
	date = {2021-04-08},
	eprinttype = {arxiv},
	eprint = {2104.03829},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ⛔ No {DOI} found},
}

@article{gu_zero-shot_2021,
	title = {Zero-Shot Detection via Vision and Language Knowledge Distillation},
	url = {http://arxiv.org/abs/2104.13921},
	abstract = {Zero-shot image classification has made promising progress by training the aligned image and text encoders. The goal of this work is to advance zero-shot object detection, which aims to detect novel objects without bounding box nor mask annotations. We propose {ViLD}, a training method via Vision and Language knowledge Distillation. We distill the knowledge from a pre-trained zero-shot image classification model (e.g., {CLIP}) into a two-stage detector (e.g., Mask R-{CNN}). Our method aligns the region embeddings in the detector to the text and image embeddings inferred by the pre-trained model. We use the text embeddings as the detection classifier, obtained by feeding category names into the pre-trained text encoder. We then minimize the distance between the region embeddings and image embeddings, obtained by feeding region proposals into the pre-trained image encoder. During inference, we include text embeddings of novel categories into the detection classifier for zero-shot detection. We benchmark the performance on {LVIS} dataset by holding out all rare categories as novel categories. {ViLD} obtains 16.1 mask {AP}\$\_r\$ with a Mask R-{CNN} ({ResNet}-50 {FPN}) for zero-shot detection, outperforming the supervised counterpart by 3.8. The model can directly transfer to other datasets, achieving 72.2 {AP}\$\_\{50\}\$, 36.6 {AP} and 11.8 {AP} on {PASCAL} {VOC}, {COCO} and Objects365, respectively.},
	journaltitle = {{arXiv}:2104.13921 [cs]},
	author = {Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin},
	urldate = {2021-05-06},
	date = {2021-04-28},
	eprinttype = {arxiv},
	eprint = {2104.13921},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ⛔ No {DOI} found},
}

@article{li_semantic_2021,
	title = {Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization},
	url = {http://arxiv.org/abs/2104.05833},
	shorttitle = {Semantic Segmentation with Generative Models},
	abstract = {Training deep networks with limited labeled data while achieving a strong generalization ability is key in the quest to reduce human annotation efforts. This is the goal of semi-supervised learning, which exploits more widely available unlabeled data to complement small labeled data sets. In this paper, we propose a novel framework for discriminative pixel-level tasks using a generative model of both images and labels. Concretely, we learn a generative adversarial network that captures the joint image-label distribution and is trained efficiently using a large set of unlabeled images supplemented with only few labeled ones. We build our architecture on top of {StyleGAN}2, augmented with a label synthesis branch. Image labeling at test time is achieved by first embedding the target image into the joint latent space via an encoder network and test-time optimization, and then generating the label from the inferred embedding. We evaluate our approach in two important domains: medical image segmentation and part-based face segmentation. We demonstrate strong in-domain performance compared to several baselines, and are the first to showcase extreme out-of-domain generalization, such as transferring from {CT} to {MRI} in medical imaging, and photographs of real faces to paintings, sculptures, and even cartoons and animal faces. Project Page: {\textbackslash}url\{https://nv-tlabs.github.io/{semanticGAN}/\}},
	journaltitle = {{arXiv}:2104.05833 [cs]},
	author = {Li, Daiqing and Yang, Junlin and Kreis, Karsten and Torralba, Antonio and Fidler, Sanja},
	urldate = {2021-05-06},
	date = {2021-04-12},
	eprinttype = {arxiv},
	eprint = {2104.05833},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ⛔ No {DOI} found},
}

@article{pastore_closer_2021,
	title = {A Closer Look at Self-training for Zero-Label Semantic Segmentation},
	url = {http://arxiv.org/abs/2104.11692},
	abstract = {Being able to segment unseen classes not observed during training is an important technical challenge in deep learning, because of its potential to reduce the expensive annotation required for semantic segmentation. Prior zero-label semantic segmentation works approach this task by learning visual-semantic embeddings or generative models. However, they are prone to overfitting on the seen classes because there is no training signal for them. In this paper, we study the challenging generalized zero-label semantic segmentation task where the model has to segment both seen and unseen classes at test time. We assume that pixels of unseen classes could be present in the training images but without being annotated. Our idea is to capture the latent information on unseen classes by supervising the model with self-produced pseudo-labels for unlabeled pixels. We propose a consistency regularizer to filter out noisy pseudo-labels by taking the intersections of the pseudo-labels generated from different augmentations of the same image. Our framework generates pseudo-labels and then retrain the model with human-annotated and pseudo-labelled data. This procedure is repeated for several iterations. As a result, our approach achieves the new state-of-the-art on {PascalVOC}12 and {COCO}-stuff datasets in the challenging generalized zero-label semantic segmentation setting, surpassing other existing methods addressing this task with more complex strategies.},
	journaltitle = {{arXiv}:2104.11692 [cs]},
	author = {Pastore, Giuseppe and Cermelli, Fabio and Xian, Yongqin and Mancini, Massimiliano and Akata, Zeynep and Caputo, Barbara},
	urldate = {2021-05-06},
	date = {2021-04-21},
	eprinttype = {arxiv},
	eprint = {2104.11692},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No {DOI} found},
}

@article{vilar_extracting_2021,
	title = {Extracting Structured Supervision From Captions for Weakly Supervised Semantic Segmentation},
	volume = {9},
	issn = {2169-3536},
	doi = {10/gjv7rv},
	abstract = {Weakly supervised semantic segmentation ({WSSS}) methods have received significant attention in recent years, since they can dramatically reduce the annotation costs of fully supervised alternatives. While most previous studies focused on leveraging classification labels, we explore instead the use of image captions, which can be obtained easily from the web and contain richer visual information. Existing methods for this task assigned text snippets to relevant semantic labels by simply matching class names, and then employed a model trained to localize arbitrary text in images to generate pseudo-ground truth segmentation masks. Instead, we propose a dedicated caption processing module to extract structured supervision from captions, consisting of improved relevant object labels, their visual attributes, and additional background categories, all of which are useful for improving segmentation quality. This module uses syntactic structures learned from text data, and semantic relations retrieved from a knowledge database, without requiring additional annotations on the specific image domain, and consequently can be extended immediately to new object categories. We then present a novel localization network, which is trained to localize only these structured labels. This strategy simplifies model design, while focusing training signals on relevant visual information. Finally, we describe a method for leveraging all types of localization maps to obtain high-quality segmentation masks, which are used to train a supervised model. On the challenging {MS}-{COCO} dataset, our method moves the state-of-the-art forward significantly for {WSSS} with image-level supervision by a margin of 7.6\% absolute (26.7\% relative) mean Intersection-over-Union, achieving 54.5\% precision and 50.9\% recall.},
	pages = {65702--65720},
	journaltitle = {{IEEE} Access},
	author = {Vilar, Daniel R. and Perez, Claudio A.},
	date = {2021},
	note = {Conference Name: {IEEE} Access},
	keywords = {Cams, Image captions, Image segmentation, Location awareness, Semantics, Task analysis, Training, Visualization, semantic segmentation, weakly supervised},
}

@inproceedings{saypadith_video_2021,
	title = {Video Anomaly Detection based on Deep Generative Network},
	doi = {10/gjv7rt},
	abstract = {In this paper, we present a framework for the detection of anomalies in video scenes. Both spatial and temporal features extract and learn through the framework. We employ inception modules and residual skip connections inside the framework to make the network learning higher-level features, which we call "multi-scale U-Net". A multi-scale U-Net kept useful features of the image that lost during training caused by the convolution operator. The numbers of training and testing parameters in our framework are reduced while the detection accuracy is still improved. We evaluated the proposed framework on three benchmark datasets: {UCSD}, {CHUK} Avenue and {ShanghaiTech} dataset. Our proposed framework achieved 95.7\%, 86.8\% and 73.0\% in terms of {AUC}, which surpasses the state-of-art learning-based methods.},
	eventtitle = {2021 {IEEE} International Symposium on Circuits and Systems ({ISCAS})},
	pages = {1--5},
	booktitle = {2021 {IEEE} International Symposium on Circuits and Systems ({ISCAS})},
	author = {Saypadith, Savath and Onoye, Takao},
	date = {2021-05},
	note = {{ISSN}: 2158-1525},
	keywords = {Anomaly detection, Feature extraction, {GAN}, {IEEE} Regions, Inception module, Learning systems, Legged locomotion, Stacking, Training, U-Net, Video anomaly detection, multi-scale U-Net},
}

@article{mercat_multi-head_2019,
	title = {Multi-Head Attention for Multi-Modal Joint Vehicle Motion Forecasting},
	url = {http://arxiv.org/abs/1910.03650},
	abstract = {This paper presents a novel vehicle motion forecasting method based on multi-head attention. It produces joint forecasts for all vehicles on a road scene as sequences of multi-modal probability density functions of their positions. Its architecture uses multi-head attention to account for complete interactions between all vehicles, and long short-term memory layers for encoding and forecasting. It relies solely on vehicle position tracks, does not need maneuver definitions, and does not represent the scene with a spatial grid. This allows it to be more versatile than similar model while combining any forecasting capabilities, namely joint forecast with interactions, uncertainty estimation, and multi-modality. The resulting prediction likelihood outperforms state-of-the-art models on the same dataset.},
	journaltitle = {{arXiv}:1910.03650 [cs]},
	author = {Mercat, Jean and Gilles, Thomas and Zoghby, Nicole El and Sandou, Guillaume and Beauvois, Dominique and Gil, Guillermo Pita},
	urldate = {2021-05-06},
	date = {2019-12-20},
	eprinttype = {arxiv},
	eprint = {1910.03650},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, ⛔ No {DOI} found},
}

@article{khandelwal_what-if_2020,
	title = {What-If Motion Prediction for Autonomous Driving},
	url = {http://arxiv.org/abs/2008.10587},
	abstract = {Forecasting the long-term future motion of road actors is a core challenge to the deployment of safe autonomous vehicles ({AVs}). Viable solutions must account for both the static geometric context, such as road lanes, and dynamic social interactions arising from multiple actors. While recent deep architectures have achieved state-of-the-art performance on distance-based forecasting metrics, these approaches produce forecasts that are predicted without regard to the {AV}'s intended motion plan. In contrast, we propose a recurrent graph-based attentional approach with interpretable geometric (actor-lane) and social (actor-actor) relationships that supports the injection of counterfactual geometric goals and social contexts. Our model can produce diverse predictions conditioned on hypothetical or "what-if" road lanes and multi-actor interactions. We show that such an approach could be used in the planning loop to reason about unobserved causes or unlikely futures that are directly relevant to the {AV}'s intended route.},
	journaltitle = {{arXiv}:2008.10587 [cs, stat]},
	author = {Khandelwal, Siddhesh and Qi, William and Singh, Jagjeet and Hartnett, Andrew and Ramanan, Deva},
	urldate = {2021-05-06},
	date = {2020-08-24},
	eprinttype = {arxiv},
	eprint = {2008.10587},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No {DOI} found},
}

@article{noy_ontology_nodate,
	title = {Ontology Development 101: A Guide to Creating Your First Ontology},
	pages = {25},
	author = {Noy, Natalya F and {McGuinness}, Deborah L},
	langid = {english},
	keywords = {{ToDo}, ⛔ No {DOI} found},
}

@article{liu_multimodal_2021,
	title = {Multimodal Motion Prediction with Stacked Transformers},
	url = {http://arxiv.org/abs/2103.11624},
	abstract = {Predicting multiple plausible future trajectories of the nearby vehicles is crucial for the safety of autonomous driving. Recent motion prediction approaches attempt to achieve such multimodal motion prediction by implicitly regularizing the feature or explicitly generating multiple candidate proposals. However, it remains challenging since the latent features may concentrate on the most frequent mode of the data while the proposal-based methods depend largely on the prior knowledge to generate and select the proposals. In this work, we propose a novel transformer framework for multimodal motion prediction, termed as {mmTransformer}. A novel network architecture based on stacked transformers is designed to model the multimodality at feature level with a set of fixed independent proposals. A region-based training strategy is then developed to induce the multimodality of the generated proposals. Experiments on Argoverse dataset show that the proposed model achieves the state-of-the-art performance on motion prediction, substantially improving the diversity and the accuracy of the predicted trajectories. Demo video and code are available at https://decisionforce.github.io/{mmTransformer}.},
	journaltitle = {{arXiv}:2103.11624 [cs]},
	author = {Liu, Yicheng and Zhang, Jinghuai and Fang, Liangji and Jiang, Qinhong and Zhou, Bolei},
	urldate = {2021-05-06},
	date = {2021-03-24},
	eprinttype = {arxiv},
	eprint = {2103.11624},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, ⛔ No {DOI} found},
}

@article{liang_learning_2020,
	title = {Learning Lane Graph Representations for Motion Forecasting},
	url = {http://arxiv.org/abs/2007.13732},
	abstract = {We propose a motion forecasting model that exploits a novel structured map representation as well as actor-map interactions. Instead of encoding vectorized maps as raster images, we construct a lane graph from raw map data to explicitly preserve the map structure. To capture the complex topology and long range dependencies of the lane graph, we propose {LaneGCN} which extends graph convolutions with multiple adjacency matrices and along-lane dilation. To capture the complex interactions between actors and maps, we exploit a fusion network consisting of four types of interactions, actor-to-lane, lane-to-lane, lane-to-actor and actor-to-actor. Powered by {LaneGCN} and actor-map interactions, our model is able to predict accurate and realistic multi-modal trajectories. Our approach significantly outperforms the state-of-the-art on the large scale Argoverse motion forecasting benchmark.},
	journaltitle = {{arXiv}:2007.13732 [cs]},
	author = {Liang, Ming and Yang, Bin and Hu, Rui and Chen, Yun and Liao, Renjie and Feng, Song and Urtasun, Raquel},
	urldate = {2021-05-06},
	date = {2020-07-27},
	eprinttype = {arxiv},
	eprint = {2007.13732},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No {DOI} found},
}

@article{li_end--end_2020,
	title = {End-to-end Contextual Perception and Prediction with Interaction Transformer},
	url = {http://arxiv.org/abs/2008.05927},
	abstract = {In this paper, we tackle the problem of detecting objects in 3D and forecasting their future motion in the context of self-driving. Towards this goal, we design a novel approach that explicitly takes into account the interactions between actors. To capture their spatial-temporal dependencies, we propose a recurrent neural network with a novel Transformer architecture, which we call the Interaction Transformer. Importantly, our model can be trained end-to-end, and runs in real-time. We validate our approach on two challenging real-world datasets: {ATG}4D and {nuScenes}. We show that our approach can outperform the state-of-the-art on both datasets. In particular, we significantly improve the social compliance between the estimated future trajectories, resulting in far fewer collisions between the predicted actors.},
	journaltitle = {{arXiv}:2008.05927 [cs]},
	author = {Li, Lingyun Luke and Yang, Bin and Liang, Ming and Zeng, Wenyuan and Ren, Mengye and Segal, Sean and Urtasun, Raquel},
	urldate = {2021-05-06},
	date = {2020-08-13},
	eprinttype = {arxiv},
	eprint = {2008.05927},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, ⛔ No {DOI} found},
}

@article{song_learning_2021,
	title = {Learning to Predict Vehicle Trajectories with Model-based Planning},
	url = {http://arxiv.org/abs/2103.04027},
	abstract = {Predicting the future trajectories of on-road vehicles is critical for autonomous driving. In this paper, we introduce a novel prediction framework called {PRIME}, which stands for Prediction with Model-based Planning. Unlike recent prediction works that utilize neural networks to model scene context and produce unconstrained trajectories, {PRIME} is designed to generate accurate and feasibility-guaranteed future trajectory predictions, which guarantees the trajectory feasibility by exploiting a model-based generator to produce future trajectories under explicit constraints and enables accurate multimodal prediction by using a learning-based evaluator to select future trajectories. We conduct experiments on the large-scale Argoverse Motion Forecasting Benchmark. Our {PRIME} outperforms state-of-the-art methods in prediction accuracy, feasibility, and robustness under imperfect tracking. Furthermore, we achieve the 1st place on the Argoervese Leaderboard.},
	journaltitle = {{arXiv}:2103.04027 [cs]},
	author = {Song, Haoran and Luan, Di and Ding, Wenchao and Wang, Michael Yu and Chen, Qifeng},
	urldate = {2021-05-06},
	date = {2021-03-05},
	eprinttype = {arxiv},
	eprint = {2103.04027},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, ⛔ No {DOI} found},
}

@article{asmar_aware_2020,
	title = {{AWARE}: An Ontology for Situational Awareness of Autonomous Vehicles in Manufacturing},
	abstract = {With the development of autonomous vehicles, recent research focuses on semantically representing robotic proprioceptive and exteroceptive perceptions (i.e., perception of the own body and of an external world). Such semantic representation is queried by reasoning systems to achieve what we would refer to as machine awareness. This aligns with the general purpose of artiﬁcial intelligence to utilize commonsense knowledge, but rather relates to the knowledge representation and knowledge elicitation aspects. In this work, we present our ontology for representing background knowledge grounding cognition capabilities of autonomous transport robots. Also, existing ontologies in the domain of robotics and Internet of Things are integrated. We then demonstrate the applicability and extensibility of our ontology to autonomous and automated vehicles implemented in an automobile manufacturing plant. Our robotic situational awareness ontology can provide a basis for organizing and controlling robots in a smart factory in the near future and showcases how situational awareness facilitates the coexistence of smart autonomous agents.},
	pages = {8},
	author = {Asmar, Boulos El and Chelly, Syrine and Farber, Michael},
	date = {2020},
	langid = {english},
	keywords = {Read, ⛔ No {DOI} found},
}

@article{noauthor_ieee_2015,
	title = {{IEEE} Standard Ontologies for Robotics and Automation},
	doi = {10/gf9rr3},
	abstract = {A core ontology that specifies the main, most general concepts, relations, and axioms of robotics and automation (R\&A) is defined in this standard, which is intended as a reference for knowledge representation and reasoning in robots, as well as a formal reference vocabulary for communicating knowledge about R\&A between robots and humans. This standard is composed of a core ontology about R\&A, called {CORA}, together with other ontologies that give support to {CORA}.},
	pages = {1--60},
	journaltitle = {{IEEE} Std 1872-2015},
	date = {2015-04},
	note = {Conference Name: {IEEE} Std 1872-2015},
	keywords = {Accuracy, Automation, {IEEE} 1872({TM}), {IEEE} Robotics and Automation Society, {IEEE} standards, Ontologies, Robots, automation, core ontology, methodology, ontology, robotics},
}

@incollection{pan_aware_2020,
	location = {Cham},
	title = {{AWARE}: A Situational Awareness Framework for Facilitating Adaptive Behavior of Autonomous Vehicles in Manufacturing},
	volume = {12507},
	isbn = {978-3-030-62465-1 978-3-030-62466-8},
	url = {https://link.springer.com/10.1007/978-3-030-62466-8_40},
	shorttitle = {{AWARE}},
	abstract = {In this paper, we introduce Aware, a knowledge-enabled framework for robots’ situational awareness. It is designed to support autonomous logistics vehicles operating in automobile manufacturing plants. Aware comprises an ontology grounding robots’ observations, a knowledge reasoner, and a set of behavioral rules: The Aware ontology models data streams of proprioceptive and exteroceptive sensors into high-level semantic representations. The knowledge reasoner infers adequate policy by reasoning over a sliding window of observations, presumably depicting the robot’s perceptions and actual state of knowledge. The behavioral rules, in analogy to road traﬃc rules and common sense, regulate the operation of autonomous robots in a manufacturing environment despite their obvious peculiarity. Our rules are the ﬁrst ones facilitating the orderly and timely ﬂow of vehicles. We show the applicability of Aware in an industrial set up. Overall, we posit that situational awareness is a fundamental element towards functional autonomy and argue that it can provide a reliable basis for organizing and controlling robots in a smart factory in the near future.},
	pages = {651--666},
	booktitle = {The Semantic Web – {ISWC} 2020},
	publisher = {Springer International Publishing},
	author = {El Asmar, Boulos and Chelly, Syrine and Azzi, Nour and Nassif, Lynn and Asmar, Jana El and Färber, Michael},
	editor = {Pan, Jeff Z. and Tamma, Valentina and d’Amato, Claudia and Janowicz, Krzysztof and Fu, Bo and Polleres, Axel and Seneviratne, Oshani and Kagal, Lalana},
	urldate = {2021-05-06},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-62466-8_40},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{talebpour_influence_2015,
	title = {Influence of Autonomous and Connected Vehicles on Stability of Traffic Flow},
	url = {https://trid.trb.org/view/1339509},
	eventtitle = {Transportation Research Board 94th Annual {MeetingTransportation} Research Board},
	author = {Talebpour, Alireza and Mahmassani, Hani S.},
	urldate = {2021-05-06},
	date = {2015},
	note = {Number: 15-5971},
	keywords = {⛔ No {DOI} found},
}

@article{curto_effects_2021,
	title = {The effects of autonomous vehicles on safety},
	volume = {2343},
	issn = {0094-243X},
	url = {https://aip.scitation.org/doi/abs/10.1063/5.0047883},
	doi = {10/gjv4qd},
	pages = {110013},
	number = {1},
	journaltitle = {{AIP} Conference Proceedings},
	shortjournal = {{AIP} Conference Proceedings},
	author = {Curto, Salvatore and Severino, Alessandro and Trubia, Salvatore and Arena, Fabio and Puleo, Lucio},
	urldate = {2021-05-06},
	date = {2021-03-30},
	note = {Publisher: American Institute of Physics},
}

@article{franke_cooperative_2015,
	title = {A Cooperative Driver Assistance System : Decentralization Process and Test Framework},
	abstract = {Cooperative Driving has attracted significant attention in recent years. With the help of vehicle-to-vehicle (V2V) communication, perception systems may increase their quality of signals, their availability, and their perception range as well as decrease their latency and probability of failure. Moreover, V2V communication enables advancements from individual to cooperative decision making. With the help of a decentralized decision making among road users, the compatibility of varying planning algorithms distributed on several vehicles can be guaranteed. Additionally, the presented approach offers the opportunity to preserve the autonomy of decision making for each vehicle with an integrated validation. The integrated validation declines contradicting maneuvers by applying internal functional safety rules. The increasing progress in the research of cooperative technologies imposes new requirements for testing and validation. Therefore, a modular test framework regarding both the simultaneous integration of several autonomous vehicles and characteristics of a V2V based peer-to-peer communication is described. A combination of {ADTF} (the application prototyping framework within the Volkswagen group), {VTD} (a simulation tool-chain of {VIRES}) and {OMNet}++ (an open-source component-based network simulator) allows a host of experiments to test and validate cooperative driver assistance systems. In order to prove the decentralization process and the test framework, the paper shows simulation results for a cooperative safety system and a cooperative comfort system.},
	journaltitle = {7. Tagung Fahrerassistenzsysteme},
	author = {Franke, Kai and Düring, Michael and Günther, Hendrik-jörn},
	date = {2015},
	keywords = {⛔ No {DOI} found},
}

@article{nister_introduction_2019,
	title = {An Introduction to the Safety Force Field},
	abstract = {This document is an introduction to the Safety Force Field. The precise math is detailed in [1]. We all want a vehicle that monitors the surroundings and shields us from collisions. The Safety Force Field is the practical realization of that. It is not possible to guarantee safety regardless of what other actors do, but it is possible to guarantee that we do not contribute to an unsafe situation. If all actors in traffic had such a guarantee, no unsafe situations or collisions would occur. The Safety Force Field achieves this guarantee while allowing normal everyday driving, even the kind of driving that is necessary in practice to, for example, make a lane change in congested traffic. The Safety Force Field takes the vehicle’s understanding of the surroundings and determines a set of acceptable actions. It relies on a constructive computational mechanism that determines which actions contribute to maintaining obstacle avoidance safety. The acceptable actions never create or escalate an unsafe situation. This allows us to monitor and protect against unacceptable actions. The computation can be combined with any driving software as a layer in the motion planning that monitors and prevents unacceptable actions. The Safety Force Field follows from one core principle as opposed to a large set of rules and exceptions. The guarantees it provides have been mathematically proven. It seamlessly handles highway driving, cluttered urban situations, and unstructured zone driving. It cleanly separates obstacle avoidance from a long tail of complicated rules of the road. It considers longitudinal and lateral constraints together, and the constraints can also be visualized in a very direct way.},
	journaltitle = {An Introduction to the Safety Force Field},
	author = {Nister, David and Lee, Hon-Leung and Ng, Julia and Wang, Yizhou},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{leidinger_5g_2019,
	title = {5G – Evolution oder Revolution?},
	author = {Leidinger, Christine},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{les_5g_2017,
	title = {5G Connected Vehicles: the Missing Link to Highly Autonomous Vehicles},
	author = {Les, É O U},
	date = {2017},
	keywords = {⛔ No {DOI} found},
}

@article{heinzmann_framework_2019,
	title = {A framework for safety violation identification and assessment in autonomous driving},
	volume = {2419},
	issn = {16130073},
	abstract = {Safety in self-driving cars is essential and an interdisciplinary matter. Nevertheless, there exists a massive gap between system developers knowledge about safety concepts and the knowledge of safety engineers on autonomous driving. Thus, an approach to close this gap and integrating new ideas and concepts of the critical safety domain to self-driving cars is needed. This work presents a framework for mapping safety-critical situations based on safety measures in {CARLA} simulator. Through this framework, safety engineers can define basic safety measures such as respecting speed limits, keeping an appropriate distance to the vehicle ahead and keeping the suitable lane. Developers can quickly integrate their agent(s), and the framework generates a mapping of the safetycritical states by running an agent over several episodes in a simulated environment while maintaining the considerations of developers and safety engineers. In the simulation environment, our evaluations showed promising and intuitive results on identification of safety violations of two machine learning agents. Respectively, several safetycritical situations could be identified and analysed according to the outcome of the mappings.},
	journaltitle = {{CEUR} Workshop Proceedings},
	author = {Heinzmann, Lukas and Shafaei, Sina and Osman, Mohd Hafeez and Segler, Christoph and Knoll, Alois},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{lee_simple_2018,
	title = {A simple unified framework for detecting out-of-distribution samples and adversarial attacks},
	issn = {23318422},
	abstract = {Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.},
	pages = {1--11},
	issue = {{LID}},
	journaltitle = {{arXiv}},
	author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
	date = {2018},
	keywords = {⛔ No {DOI} found},
}

@article{bulusu_anomalous_2020,
	title = {Anomalous instance detection in deep learning: A survey},
	issn = {23318422},
	abstract = {Deep Learning ({DL}) is vulnerable to out-of-distribution and adversarial examples resulting in incorrect outputs. To make {DL} more robust, several posthoc anomaly detection techniques to detect (and discard) these anomalous samples have been proposed in the recent past. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection for {DL} based applications. We provide a taxonomy for existing techniques based on their underlying assumptions and adopted approaches. We discuss various techniques in each of the categories and provide the relative strengths and weaknesses of the approaches. Our goal in this survey is to provide an easier yet better understanding of the techniques belonging to different categories in which research has been done on this topic. Finally, we highlight the unsolved research challenges while applying anomaly detection techniques in {DL} systems and present some high-impact future research directions.},
	issue = {March},
	journaltitle = {{arXiv}},
	author = {Bulusu, Saikiran and Kailkhura, Bhavya and Li, Bo and Varshney, Pramod K. and Song, Dawn},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2003.06979},
	keywords = {⛔ No {DOI} found},
}

@article{shafaei_less_2018,
	title = {A less biased evaluation of out-of-distribution sample detectors},
	issn = {23318422},
	abstract = {In the real world, a learning system could receive an input that is unlike anything it has seen during training. Unfortunately, out-of-distribution samples can lead to unpredictable behaviour. We need to know whether any given input belongs to the population distribution of the training/evaluation data to prevent unpredictable behaviour in deployed systems. A recent surge of interest in this problem has led to the development of sophisticated techniques in the deep learning literature. However, due to the absence of a standard problem definition or an exhaustive evaluation, it is not evident if we can rely on these methods. What makes this problem different from a typical supervised learning setting is that the distribution of outliers used in training may not be the same as the distribution of outliers encountered in the application. Classical approaches that learn inliers vs. outliers with only two datasets can yield optimistic results. We introduce {OD}-test, a three-dataset evaluation scheme as a more reliable strategy to assess progress on this problem. We present an exhaustive evaluation of a broad set of methods from related areas on image classification tasks. Contrary to the existing results, we show that for realistic applications of highdimensional images the previous techniques have low accuracy and are not reliable in practice.},
	pages = {1--13},
	journaltitle = {{arXiv}},
	author = {Shafaei, Alireza and Schmidt, Mark and Little, James J.},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1809.04729},
	keywords = {⛔ No {DOI} found},
}

@article{chen_architecture_2017,
	title = {Architecture of Vehicle Trajectories Extraction with Roadside {LiDAR} Serving Connected Vehicles},
	abstract = {This research developed a data processing procedure for detection and tracking of multi-lane multi-vehicle trajectories with a roadside Light Detection and Ranging ({LiDAR}) sensor. Different from existing perception methods for the autonomous vehicle system, this procedure was explicitly developed to extract trajectories from a roadside {LiDAR} sensor. The proposed procedure includes five main steps: region of interest ({ROI}) selection, ground surface filtering, point clustering, vehicle/non-vehicle classification, and geometrical vehicle tracking. The case study showed that the trajectories of vehicles can be generated with the proposed method. This research is the start of the new-generation connected infrastructures serving connected/autonomous vehicles with the roadside {LiDAR} sensors. It will accelerate the deployment of connected-vehicle technologies to improve traffic safety, mobility and fuel efficiency.},
	pages = {9},
	journaltitle = {{IEEE} Access},
	author = {Chen, Jingrong and Tian, Sheng and Xu, Hao and Yue, Rui and Sun, Yuan and Cui, Yuepeng},
	date = {2017},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{koopman_autonomous_2019,
	title = {Autonomous vehicles meet the physical world: Rss, variability, uncertainty, and proving safety},
	volume = {2019},
	issn = {23318422},
	abstract = {The Responsibility-Sensitive Safety ({RSS}) model offers provable safety for vehicle behaviors such as minimum safe following distance. However, handling worst-case variability and uncertainty may significantly lower vehicle permissiveness, and in some situations safety cannot be guaranteed. Digging deeper into Newtonian mechanics, we identify complications that result from considering vehicle status, road geometry and environmental parameters. An es-pecially challenging situation occurs if these parameters change during the course of a collision avoidance maneuver such as hard braking. As part of our analysis, we expand the original {RSS} following distance equation to account for edge cases involving potential collisions mid-way through a braking process. We additionally propose a Micro-Operational Design Domain (μ{ODD}) ap-proach to subdividing the operational space as a way of improving permissive-ness. Confining probabilistic aspects of safety to μ{ODD} transitions permits prov-ing safety (when possible) under the assumption that the system has transitioned to the correct μ{ODD} for the situation. Each μ{ODD} can additionally be used to encode system fault responses, take credit for advisory information (e.g., from vehicle-to-vehicle communication), and anticipate likely emergent situations.},
	pages = {1--14},
	journaltitle = {{arXiv}},
	author = {Koopman, Philip and Osyk, Beth and Weast, Jack},
	date = {2019},
	keywords = {Autonomous vehicle safety, Operational design domain, {RSS}, ⛔ No {DOI} found},
}

@article{fagnani_anomaly_2019,
	title = {Anomaly Detection with Online Learning - Applications in Autonomous Driving},
	author = {Fagnani, Alessandro and Yunus, Raza},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{huang_autonomous_2020,
	title = {Autonomous Driving with Deep Learning: A Survey of State-of-Art Technologies},
	abstract = {— Since {DARPA}’s Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007, autonomous driving has been the most active field of {AI} applications. Almost at the same time, deep learning has made breakthrough by several pioneers, three of them (also called fathers of deep learning), Hinton, Bengio and {LeCun}, won {ACM} Turin Award in 2019. This is a survey of autonomous driving technologies with deep learning methods. We investigate the major fields of self-driving systems, such as perception, mapping and localization, prediction, planning and control, simulation, V2X and safety etc. Due to the limited space, we focus the analysis on several key areas, i.e. 2D/3D object detection in perception, depth estimation from cameras, multiple sensor fusion on the data, feature and task level respectively, behaviour modelling and prediction of vehicle driving and pedestrian trajectories.},
	journaltitle = {{arXiv}},
	author = {Huang, Yu and Chen, Yue},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2006.06091},
	keywords = {{CNN}, Control, Deep learning, {GAN}, {GRU}, {LSTM}, Localization, Mapping, Perception, Planning, Prediction, {RNN}, Reinforcement learning, Safety, Simulation, Simulation learning, Uncertainty, V2X, —Autonomous driving, ⛔ No {DOI} found},
}

@article{heidecker_application-driven_2021,
	title = {An Application-Driven Conceptualization of Corner Cases for Perception in Highly Automated Driving},
	author = {Heidecker, Florian and Breitenstein, Jasmin and Kevin, R and Jonas, L and Stiller, Christoph and Fingscheidt, Tim and Sick, Bernhard},
	date = {2021},
	keywords = {★, ⛔ No {DOI} found},
}

@article{moran_bachelor_2016,
	title = {Bachelor thesis - Improving Big Data Visual Analytics With Interactive Virtual Reality},
	url = {https://dspace.mit.edu/handle/1721.1/105972%0Ahttp://ieeexplore.ieee.org/abstract/document/7322473/},
	number = {2014},
	journaltitle = {Ieeexplore.Ieee.Org},
	author = {Moran, A and Gadepally, V and ({HPEC}), M Hubbell - … and {IEEE}, 2015 and 2015, Undefined},
	date = {2016},
	keywords = {⛔ No {DOI} found},
}

@article{asai_classical_2017,
	title = {Classical Planning in Deep Latent Space: Bridging the Subsymbolic-Symbolic Boundary},
	abstract = {Current domain-independent, classical planners require symbolic models of the problem domain and instance as input, resulting in a knowledge acquisition bottleneck. Meanwhile, although deep learning has achieved signiﬁcant success in many ﬁelds, the knowledge is encoded in a subsymbolic representation which is incompatible with symbolic systems such as planners. We propose {LatPlan}, an unsupervised architecture combining deep learning and classical planning. Given only an unlabeled set of image pairs showing a subset of transitions allowed in the environment (training inputs), and a pair of images representing the initial and the goal states (planning inputs), {LatPlan} ﬁnds a plan to the goal state in a symbolic latent space and returns a visualized plan execution. The contribution of this paper is twofold: (1) State Autoencoder, which ﬁnds a propositional state representation of the environment using a Variational Autoencoder. It generates a discrete latent vector from the images, based on which a {PDDL} model can be constructed and then solved by an offthe-shelf planner. (2) Action Autoencoder / Discriminator, a neural architecture which jointly ﬁnds the action symbols and the implicit action models (preconditions/effects), and provides a successor function for the implicit graph search. We evaluate {LatPlan} using image-based versions of 3 planning domains: 8-puzzle, Towers of Hanoi and {LightsOut}.},
	pages = {8},
	author = {Asai, Masataro and Fukunaga, Alex},
	date = {2017},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{edge_automotive_2019,
	title = {Automotive Edge Computing Consortium - Use-case and Requirement Document ({URD})},
	journaltitle = {Aecc},
	author = {Edge, Automotive and Consortium, Computing},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{khader_introduction_2020,
	title = {An Introduction to Automotive {LIDAR}},
	pages = {7},
	author = {Khader, Motaz and Cherian, Samir},
	date = {2020},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{filos_can_2020,
	title = {Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?},
	issn = {23318422},
	abstract = {Out-of-training-distribution ({OOD}) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to {OOD} scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called robust imitative planning ({RIP}). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in {OOD} scenes. If the model’s uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term adaptive robust imitative planning ({AdaRIP}). Our methods outperform current state-of-the-art approaches in the {nuScenes} prediction challenge, but since no benchmark evaluating {OOD} detection and adaption currently exists to assess control, we introduce an autonomous car novel-scene benchmark, {CARNOVEL}, to evaluate the robustness of driving agents to a suite of tasks with distribution shifts.},
	journaltitle = {{arXiv}},
	author = {Filos, Angelos and Tigas, Panagiotis and {McAllister}, Rowan and Rhinehart, Nicholas and Levine, Sergey and Gal, Yarin},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2006.14911},
	keywords = {⛔ No {DOI} found},
}

@article{koopman_credible_2019,
	title = {Credible Autonomy Safety Argumentation},
	volume = {27},
	abstract = {A significant challenge to deploying mission-and safety-critical autonomous systems is the difficulty of creating a credible assurance argument. This paper collects lessons learned from having observed both credible and faulty assurance argumentation attempts, with a primary emphasis on autonomous ground vehicle safety cases. Various common argumentation approaches are described, including conformance to a non-autonomy safety standard, proven in use, field testing, simulation, and formal verification. Of particular note are argumentation faults and anti-patterns that have shown up in numerous safety cases that we have encountered. These observations can help both designers and auditors detect common mistakes in safety argumentation for autonomous systems.},
	pages = {1--27},
	journaltitle = {Proceedings of 27th Safety-Critical Systems Symposium},
	author = {Koopman, Philip and Kane, Aaron and Black, Jen and Philip Koopman, Aaron Kane, Jen Black},
	date = {2019},
	keywords = {Public, ⛔ No {DOI} found},
}

@article{jain_effects_2019,
	title = {Effects of communication on the performance of cooperative adaptive cruise control},
	volume = {8},
	issn = {22783075},
	abstract = {Advancement 5G communication technology paves the way for inter-vehicle communication like V2V or V2I. These technologies gave direction for future transportation system such as Cooperative Adaptive Cruise Control ({CACC}). In this article, we will analyse and show the effects of network issues on the performance of individual vehicle and {CACC}. We will do sensitivity analysis for delay variation. We will study time delay approximation technique and validate with the original system. Numerical simulation is done in {MATLAB} and Python environment to validate the theoretical analysis.},
	pages = {355--359},
	number = {4},
	journaltitle = {International Journal of Innovative Technology and Exploring Engineering},
	author = {Jain, Ankur and Roy, B. K.},
	date = {2019},
	keywords = {Cooperative adaptive cruise control, Networked control system, Performance evaluation, Time delay, ⛔ No {DOI} found},
}

@article{achituve_gp-tree_2021,
	title = {{GP}-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning},
	url = {http://arxiv.org/abs/2102.07868},
	abstract = {Gaussian processes ({GPs}) are non-parametric, flexible, models that work well in many tasks. Combining {GPs} with deep learning methods via deep kernel learning is especially compelling due to the strong expressive power induced by the network. However, inference in {GPs}, whether with or without deep kernel learning, can be computationally challenging on large datasets. Here, we propose {GP}-Tree, a novel method for multi-class classification with Gaussian processes and deep kernel learning. We develop a tree-based hierarchical model in which each internal node of the tree fits a {GP} to the data using the Polya-Gamma augmentation scheme. As a result, our method scales well with both the number of classes and data size. We demonstrate our method effectiveness against other Gaussian process training baselines, and we show how our general {GP} approach is easily applied to incremental few-shot learning and reaches state-of-the-art performance.},
	author = {Achituve, Idan and Navon, Aviv and Yemini, Yochai and Chechik, Gal and Fetaya, Ethan},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.07868},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{james_classification_2018,
	title = {Classification of {LIDAR} Sensor Contaminations with Deep Neural Networks},
	abstract = {Light detecting and ranging ({LIDAR}) sensors are extensively studied in autonomous driving research. Monitoring the performance of {LIDAR} sensors has become significantly important to ensure their reliability and hence guarantee the safety of the vehicle. Underestimation of sensor performance can give away reliable object data, overestimation may result in safety issues. Besides light and weather conditions, the performance is strongly affected by contaminations on the sensor front plate. In this paper, we focus on classifying different types of contaminations using a deep learning approach. We train a deep neural network ({DNN}) following a multi-view concept. For the generation of training and test data, experiments have been conducted, in which the front plate of a {LIDAR} sensor has been contaminated artificially with various substances. The recorded data is transformed to contain the essential information in a compact format. The results are compared to classical machine learning techniques to demonstrates the potential of {DNN} approaches for the problem under consideration.},
	pages = {8},
	author = {James, Jyothish K and Puhlfürst, Georg and Golyanik, Vladislav and Stricker, Didier},
	date = {2018},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{tan_equalization_2020,
	title = {Equalization loss for long-tailed object recognition},
	issn = {23318422},
	abstract = {Object recognition techniques using convolutional neural networks ({CNN}) have achieved great success. However, state-of-the-art object detection methods still perform poorly on large vocabulary and long-tailed datasets, e.g. {LVIS}. In this work, we analyze this problem from a novel perspective: each positive sample of one category can be seen as a negative sample for other categories, making the tail categories receive more discouraging gradients. Based on it, we propose a simple but effective loss, named equalization loss, to tackle the problem of long-tailed rare categories by simply ignoring those gradients for rare categories. The equalization loss protects the learning of rare categories from being at a disadvantage during the network parameter updating. Thus the model is capable of learning better discriminative features for objects of rare classes. Without any bells and whistles, our method achieves {AP} gains of 4.1\% and 4.8\% for the rare and common categories on the challenging {LVIS} benchmark, compared to the Mask R-{CNN} baseline. With the utilization of the effective equalization loss, we finally won the 1st place in the {LVIS} Challenge 2019. Code has been made available at: https://github.com/tztztztztz/eql.detectron2},
	pages = {11662--11671},
	journaltitle = {{arXiv}},
	author = {Tan, Jingru and Wang, Changbao and Li, Buyu and Li, Quanquan and Ouyang, Wanli and Yin, Changqing and Yan, Junjie},
	date = {2020},
	keywords = {⛔ No {DOI} found},
}
@article{naturwissenschaften_driver_2017,
	title = {Driver Modeling and Simulation of Lane Change Situations},
	author = {Naturwissenschaften, Doktors Der},
	date = {2017},
	keywords = {⛔ No {DOI} found},
}

@article{zou_castle_2020,
	title = {Castle in the Sky : Dynamic Sky Replacement and Harmonization in Videos},
	author = {Zou, Zhengxia},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2010.11800v1},
	keywords = {⛔ No {DOI} found},
}

@article{lohdefink_gan-_2019,
	title = {{GAN}- vs. {JPEG}2000 image compression for distributed automotive perception: Higher Peak {SNR} does not mean better semantic segmentation},
	issn = {23318422},
	abstract = {The high amount of sensors required for autonomous driving poses enormous challenges on the capacity of automotive bus systems. There is a need to understand tradeoffs between bitrate and perception performance. In this paper, we compare the image compression standards {JPEG}, {JPEG}2000, and {WebP} to a modern encoder/decoder image compression approach based on generative adversarial networks ({GANs}). We evaluate both the pure compression performance using typical metrics such as peak signal-to-noise ratio ({PSNR}), structural similarity ({SSIM}) and others, but also the performance of a subsequent perception function, namely a semantic segmentation (characterized by the mean intersection over union ({mIoU}) measure). Not surprisingly, for all investigated compression methods, a higher bitrate means better results in all investigated quality metrics. Interestingly, however, we show that the semantic segmentation {mIoU} of the {GAN} autoencoder in the highly relevant low-bitrate regime (at 0.0625 bit/pixel) is better by 3.9 \% absolute than {JPEG}2000, although the latter still is considerably better in terms of {PSNR} (5.91 {dB} difference). This effect can greatly be enlarged by training the semantic segmentation model with images originating from the decoder, so that the {mIoU} using the segmentation model trained by {GAN} reconstructions exceeds the use of the model trained with original images by almost 20 \% absolute. We conclude that distributed perception in future autonomous driving will most probably not provide a solution to the automotive bus capacity bottleneck by using standard compression schemes such as {JPEG}2000, but requires modern coding approaches, with the {GAN} encoder/decoder method being a promising candidate.},
	issue = {Iv},
	journaltitle = {{arXiv}},
	author = {Löhdefink, Jonas and Bär, Andreas and Schmidt, Nico M. and Hüger, Fabian and Schlicht, Peter and Fingscheidt, Tim},
	date = {2019},
	note = {{ISBN}: 9781728105604},
	keywords = {⛔ No {DOI} found},
}

@article{liu_harnessing_2012,
	title = {Harnessing the power of virtual reality},
	volume = {108},
	issn = {03607275},
	pages = {28--33},
	number = {7},
	journaltitle = {Chemical Engineering Progress},
	author = {Liu, Xinhua and Guo, Li and Xia, Zhaojie and Lu, Bona and Zhao, Mingkun and Meng, Fanxiao and Li, Zhouzhou and Li, Jinghai},
	date = {2012},
	keywords = {⛔ No {DOI} found},
}

@article{yurtsever_blending_2020,
	title = {Blending Generative Adversarial Image Synthesis with Rendering for Computer Graphics},
	abstract = {Conventional computer graphics pipelines require detailed 3D models, meshes, textures, and rendering engines to generate 2D images from 3D scenes. These processes are labor-intensive. We introduce Hybrid Neural Computer Graphics ({HNCG}) as an alternative. The contribution is a novel image formation strategy to reduce the 3D model and texture complexity of computer graphics pipelines. Our main idea is straightforward: Given a 3D scene, render only important objects of interest and use generative adversarial processes for synthesizing the rest of the image. To this end, we propose a novel image formation strategy to form 2D semantic images from 3D scenery consisting of simple object models without textures. These semantic images are then converted into photo-realistic {RGB} images with a state-of-the-art conditional Generative Adversarial Network ({cGAN}) based image synthesizer trained on real-world data. Meanwhile, objects of interest are rendered using a physics-based graphics engine. This is necessary as we want to have full control over the appearance of objects of interest. Finally, the partially-rendered and {cGAN} synthesized images are blended with a blending {GAN}. We show that the proposed framework outperforms conventional rendering with ablation and comparison studies. Semantic retention and Fréchet Inception Distance ({FID}) measurements were used as the main performance metrics.},
	issue = {August},
	journaltitle = {{arXiv}},
	author = {Yurtsever, Ekim and Yang, Dongfang and Mert Koc, Ibrahim and Redmill, Keith A.},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2007.15820},
	keywords = {⛔ No {DOI} found},
}

@article{zhang_holistic_2021,
	title = {Holistic 3D Scene Understanding from a Single Image with Implicit Representation},
	url = {http://arxiv.org/abs/2103.06422},
	abstract = {We present a new pipeline for holistic 3D scene understanding from a single image, which could predict object shape, object pose, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit representation to solve this challenge. We not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine 3D object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features. A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estimation, and 3D object detection.},
	author = {Zhang, Cheng and Cui, Zhaopeng and Zhang, Yinda and Zeng, Bing and Pollefeys, Marc and Liu, Shuaicheng},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.06422},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{alejandro_high-speed_2007,
	title = {High-Speed Autonomous Navigation with Motion Prediction for Unknown Moving Obstacles},
	author = {Alejandro, Dizan and Govea, Vasquez and Large, Frédéric and Fraichard, Thierry and Alejandro, Dizan and Govea, Vasquez and Large, Frédéric and Fraichard, Thierry and High-speed, Christian Laugier},
	date = {2007},
	keywords = {⛔ No {DOI} found},
}

@article{laflamme_driving_2019,
	title = {Driving datasets literature review},
	journaltitle = {{arXiv}},
	author = {Laflamme, Charles Éric Noël and Giguère, Philippe and Pomerleau, François},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1910.11968},
	keywords = {⛔ No {DOI} found},
}

@article{janai_computer_2019,
	title = {Computer Vision for Autonomous Vehicles : Problems , Datasets and State of the Art},
	author = {Janai, Joel},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1704.05519v2},
	keywords = {★, ⛔ No {DOI} found},
}

@article{hoyhtya_integrated_2017,
	title = {Integrated 5G satellite-terrestrial systems: Use cases for road safety and autonomous ships},
	volume = {2017-Octob},
	issn = {25736124},
	abstract = {In this article, we describe an integrated satellite-terrestrial system for 5G and beyond. The focus of this paper is to describe the system architecture, define the most important technological advances, and specifically look at challenges and solutions for road safety and maritime communications. Selected use cases represent areas where both satellite and terrestrial systems have important roles in fulfilling the reliability, capacity, and latency needs, enabling location awareness and transmission of critical information between different entities. The most potential frequency bands are described, including the 5G pioneer band 24.25-27.5 {GHz} where spectrum sharing techniques will most probably be used to enable both satellite and terrestrial transmissions. We will discuss how database-assisted sharing could be used for efficient use of the radio spectrum.},
	journaltitle = {Ka and Broadband Communications Conference},
	author = {Höyhtyä, Marko and Ojanperä, Tiia and Mäkelä, Jukka and Ruponen, Sami and Järvensivu, Pertti},
	date = {2017},
	keywords = {⛔ No {DOI} found},
}

@article{sohn_fixmatch_2020,
	title = {{FixMatch}: Simplifying semi-supervised learning with consistency and confidence},
	issn = {23318422},
	abstract = {Semi-supervised learning ({SSL}) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common {SSL} methods: consistency regularization and pseudo-labeling. Our algorithm, {FixMatch}, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that {FixMatch} achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on {CIFAR}-10 with 250 labels and 88.61\% accuracy with 40 - just 4 labels per class. Since {FixMatch} bears many similarities to existing {SSL} methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to {FixMatch}'s success. We make our code available at https://github.com/google-research/fixmatch.},
	issue = {{NeurIPS}},
	journaltitle = {{arXiv}},
	author = {Sohn, Kihyuk and Berthelot, David and Li, Chun Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D. and Kurakin, Alex and Zhang, Han and Raffel, Colin},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2001.07685},
	keywords = {⛔ No {DOI} found},
}

@article{li_aads_2019,
	title = {{AADS}: Augmented autonomous driving simulation using data-driven algorithms augmented autonomous driving simulation},
	abstract = {Simulation systems have become an essential component in the development and validation of autonomous driving technologies. The prevailing state-of-the-art approach for simulation is to use game engines or high-fidelity computer graphics ({CG}) models to create driving scenarios. However, creating {CG} models and vehicle movements (a.k.a. the assets for simulation) remains a manual task that can be costly and time-consuming. In addition, the fidelity of {CG} images still lacks the richness and authenticity of real-world images and using these {CG} images for training leads to degraded performance. In this paper we present a novel approach to address these issues: Augmented Autonomous Driving Simulation ({AADS}). Our formulation augments real-world pictures with a simulated traffic flow to create photo-realistic simulation images and renderings. More specifically, we use {LiDAR} and cameras to scan street scenes. From the acquired trajectory data, we generate highly plausible traffic flows for cars and pedestrians and compose them into the background. The composite images can be re-synthesized with different viewpoints and sensor models (camera or {LiDAR}). The resulting images are photo-realistic, fully annotated, and ready for end-to-end training and testing of autonomous driving systems from perception to planning. We explain our system design and validate our algorithms with a number of autonomous driving tasks from detection to segmentation and predictions. Compared to traditional approaches, our method offers unmatched scalability and realism. Scalability is particularly important for {AD} simulation and we believe the complexity and diversity of the real world cannot be realistically captured in a virtual environment. Our augmented approach combines the flexibility of a virtual environment (e.g., vehicle movements) with the richness of the real world to allow effective simulation of any location on earth.},
	pages = {1--13},
	journaltitle = {{arXiv}},
	author = {Li, W. and Pan, C. W. and Zhang, R. and Ren, J. P. and Ma, Y. X. and Fang, J. and Yan, F. L. and Geng, Q. C. and Huang, X. Y. and Gong, H. J. and Xu, W. W. and Wang, G. P. and Manocha, D. and Yang, R. G.},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{atta_adaptive_2020,
	title = {An adaptive approach: Smart traffic congestion control system},
	abstract = {The paper explores the utilization of {RFID} innovation for trafﬁc congestion and ﬁnd out the blockage at any intersection of the street by utilizing {RFID} reader and labels as sensors. The idea behind this paper is to make the ﬁxed and preset activity of trafﬁc signal dynamic. The paper affords a unique method for making the signal timing proportional to the congestion on the roads at any time directly. Proposed intelligent system can maintain the dynamic timings of trafﬁc signals by sensing the density of trafﬁc to minimize the congestion with the help of {IoT} enabled sensors which provides the advanced and powerful communication technologies for the citizens.},
	pages = {8},
	journaltitle = {Computer and Information Sciences},
	author = {Atta, Ayesha},
	date = {2020},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{von_rueden_informed_2019,
	title = {Informed machine learning - a taxonomy and survey of integrating knowledge into learning systems},
	abstract = {Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process, which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. First, we provide a definition and propose a concept for informed machine learning, which illustrates its building blocks and distinguishes it from conventional machine learning. Second, we introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Third, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.},
	pages = {1--20},
	journaltitle = {{arXiv}},
	author = {Von Rueden, Laura and Mayer, Sebastian and Beckh, Katharina and Georgiev, Bogdan and Giesselbach, Sven and Heese, Raoul and Kirsch, Birgit and Pfrommer, Julius and Pick, Annika and Ramamurthy, Rajkumar and Walczak, Michał and Garcke, Jochen and Bauckhage, Christian and Schuecker, Jannis},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1903.12394},
	keywords = {Expert Knowledge, Hybrid, Informed, Machine Learning, Prior Knowledge, Survey, ★, ⛔ No {DOI} found},
}

@article{technologies_how_2018,
	title = {How 5G Will Influence Autonomous Driving Systems},
	pages = {1--9},
	journaltitle = {Keysight Technologies},
	author = {Technologies, Keysight},
	date = {2018},
	keywords = {autonomous driving vehicles, wireless communication, ⛔ No {DOI} found},
}

@article{hanappe_impacts_2018,
	title = {Impacts and potential benefits of autonomous vehicles: From an international context to Grand Paris},
	url = {https://www.apur.org/fr/nos-travaux/impacts-benefices-possibles-vehicules-autonomes-contexte-international-cas-grand-paris},
	abstract = {Autonomous vehicles ({AVs}) have the potential to transform future of urban living. By offering the opportunity for safe, efficient, accessible and affordable transportation they promise not only a novel system of mobility, but also a novel approach to the urban lifestyle and urban design. Yet these benefits are far from guaranteed. Scholars show that Avs have the potential for numerous negative impacts in contraposition to their positive potential, depending, of course, on the form of their implementation. they could combine with other growing trends in the mobility space (such as shared use or mass electrification) to introduce a positive rupture in today’s mobility or, conversely, they could exacerbate existing trends towards congestion and climate change, further entrenching the negative aspects of today’s status quo. whereas the number of experiments has greatly increased in recent years in France and around the world, French Government will authorize from early 2019 autonomous vehicles tests without human driver on natio- nal roads. Impacts of autonomous mobility could affect a wide field of domains, from urban transportation to freight organization systems. the foreseeable arrival of drone, delivery robot, robotaxi, shuttle or autonomous garbage truck…, in our streets imposes to better identify the fallout for major cities. The challenge - and the opportunity - of today is to redefine the mobility system before the technology solidi- fies its own path. In pursuit of that goal, this study, registered in the program of work of Apur in 2018 and car- ried out in connection with the Mit, strives to offer a primer on international autonomous vehicle development and regulation, applying its lessons to the context of the Paris metropolitan area. it aims to provide public actors with the understanding, insights and tools it needs to enact pertinent policy measures today to shape the arrival of autonomous vehicles tomorrow.},
	pages = {1--52},
	journaltitle = {Apur},
	author = {Hanappe, Florence and Hudson, Annie and Pelloux, Patricia and Alba, Dominique and Musseau, Pierre},
	date = {2018},
	keywords = {⛔ No {DOI} found},
}

@inproceedings{wang_l2r_2020,
	title = {L2R {GAN}: {LiDAR}-to-Radar Translation},
	url = {https://openaccess.thecvf.com/content/ACCV2020/html/Wang_L2R_GAN_LiDAR-to-Radar_Translation_ACCV_2020_paper.html},
	shorttitle = {L2R {GAN}},
	eventtitle = {Proceedings of the Asian Conference on Computer Vision},
	author = {Wang, Leichen and Goldluecke, Bastian and Anklam, Carsten},
	urldate = {2021-04-19},
	date = {2020},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{lunz_inverse_2020,
	title = {Inverse graphics {GAN}: Learning to generate 3D shapes from unstructured 2D data},
	abstract = {Recent work has shown the ability to learn generative models for 3D shapes from only unstructured 2D images. However, training such models requires differentiating through the rasterization step of the rendering process, therefore past work has focused on developing bespoke rendering models which smooth over this non-differentiable process in various ways. Such models are thus unable to take advantage of the photo-realistic, fully featured, industrial renderers built by the gaming and graphics industry. In this paper we introduce the first scalable training technique for 3D generative models from 2D data which utilizes an off-the-shelf non-differentiable renderer. To account for the non-differentiability, we introduce a proxy neural renderer to match the output of the non-differentiable renderer. We further propose discriminator output matching to ensure that the neural renderer learns to smooth over the rasterization appropriately. We evaluate our model on images rendered from our generated 3D shapes, and show that our model can consistently learn to generate better shapes than existing models when trained with exclusively unstructured 2D images.},
	journaltitle = {{arXiv}},
	author = {Lunz, Sebastian and Li, Yingzhen and Fitzgibbon, Andrew and Kushman, Nate},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2002.12674},
	keywords = {⛔ No {DOI} found},
}

@article{chang_analyzing_2007,
	title = {Analyzing feature trajectories for event detection},
	author = {Chang, Kuiyu},
	date = {2007},
	note = {{ISBN}: 9781595935977},
	keywords = {dft, event detection, feature categorization, ⛔ No {DOI} found},
}

@article{taha_knowledge_2021,
	title = {Knowledge Evolution in Neural Networks},
	url = {http://arxiv.org/abs/2103.05152},
	abstract = {Deep learning relies on the availability of a large corpus of data (labeled or unlabeled). Thus, one challenging unsettled question is: how to train a deep network on a relatively small dataset? To tackle this question, we propose an evolution-inspired training approach to boost performance on relatively small datasets. The knowledge evolution ({KE}) approach splits a deep network into two hypotheses: the fit-hypothesis and the reset-hypothesis. We iteratively evolve the knowledge inside the fit-hypothesis by perturbing the reset-hypothesis for multiple generations. This approach not only boosts performance, but also learns a slim network with a smaller inference cost. {KE} integrates seamlessly with both vanilla and residual convolutional networks. {KE} reduces both overfitting and the burden for data collection. We evaluate {KE} on various network architectures and loss functions. We evaluate {KE} using relatively small datasets (e.g., {CUB}-200) and randomly initialized deep networks. {KE} achieves an absolute 21\% improvement margin on a state-of-the-art baseline. This performance improvement is accompanied by a relative 73\% reduction in inference cost. {KE} achieves state-of-the-art results on classification and metric learning benchmarks. Code available at http://bit.ly/3uLgwYb},
	author = {Taha, Ahmed and Shrivastava, Abhinav and Davis, Larry},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.05152},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{zhan_interaction_2019,
	title = {Interaction dataset: An international, adversarial and cooperative motion dataset in interactive driving scenarios with semantic maps},
	issn = {23318422},
	abstract = {Interactive motion datasets of road participants are vital to the development of autonomous vehicles in both industry and academia. Research areas such as motion prediction, motion planning, representation learning, imitation learning, behavior modeling, behavior generation, and algorithm testing, require support from high-quality motion datasets containing interactive driving scenarios with different driving cultures. In this paper, we present an {INTERnational}, Adversarial and Cooperative {moTION} dataset ({INTERACTION} dataset) in interactive driving scenarios with semantic maps. Five features of the dataset are highlighted. 1) The interactive driving scenarios are diverse, including urban/highway/ramp merging and lane changes, roundabouts with yield/stop signs, signalized intersections, intersections with one/two/all-way stops, etc. 2) Motion data from different countries and different continents are collected so that driving preferences and styles in different cultures are naturally included. 3) The driving behavior is highly interactive and complex with adversarial and cooperative motions of various traffic participants. Highly complex behavior such as negotiations, aggressive/irrational decisions and traffic rule violations are densely contained in the dataset, while regular behavior can also be found from cautious car-following, stop, left/right/U-turn to rational lane-change and cycling and pedestrian crossing, etc. 4) The levels of criticality span wide, from regular safe operations to dangerous, near-collision maneuvers. Real collision, although relatively slight, is also included. 5) Maps with complete semantic information are provided with physical layers, reference lines, lanelet connections and traffic rules. The data is recorded from drones and traffic cameras, and the processing pipelines for both are briefly described. Statistics of the dataset in terms of number of entities and interaction density are also provided, along with some utilization examples in the areas of motion prediction, imitation learning, decision-making and planing, representation learning, interaction extraction and social behavior generation. The dataset can be downloaded via https://interaction-dataset.com.},
	journaltitle = {{arXiv}},
	author = {Zhan, Wei and Sun, Liting and Wang, Di and Shi, Haojie and Clausse, Aubrey and Naumann, Maximilian and Kümmerle, Julius and Königshof, Hendrik and Stiller, Christoph and de la Fortelle, Arnaud and Tomizuka, Masayoshi},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1910.03088},
	keywords = {⛔ No {DOI} found},
}

@article{domingues_distributed_2009,
	title = {A Distributed Software Architecture for Collaborative Teleoperation based on a {VR} Platform and Web Application Interoperability},
	abstract = {Augmented Reality and Virtual Reality can provide to a Human Operator ({HO}) a real help to complete complex tasks, such as robot teleoperation and cooperative teleassistance. Using appropriate augmentations, the {HO} can interact faster, safer and easier with the remote real world. In this paper, we present an extension of an existing distributed software and network architecture for collaborative teleoperation based on networked human-scale mixed reality and mobile platform. The first teleoperation system was composed by {VR} application used together and it is impossible to control a distant robot simultaneously. Our goal is to update the teleoperation system to permit a heterogeneous collaborative teleoperation between 2 platforms. An important feature of this interface is based on different Mobile platforms to control one or many robots.},
	journaltitle = {Work},
	author = {Domingues, Christophe and Otmane, Samir and Davesne, Fréderic and Mallem, Malik},
	date = {2009},
	keywords = {⛔ No {DOI} found},
}

@article{wang_review_2018,
	title = {A review on Cooperative Adaptive Cruise Control ({CACC}) systems: Architectures, controls, and applications},
	abstract = {Connected and automated vehicles ({CAVs}) have the potential to address the safety, mobility and sustainability issues of our current transportation systems. Cooperative adaptive cruise control ({CACC}), for example, is one promising technology to aow {CAVs} to be driven in a cooperative manner and introduces system-wide benefits. In this paper, we review the progress achieved by researchers worldwide regarding different aspects of {CACC} systems. Literature of {CACC} system architectures are reviewed, which explain how this system works from a higher level. Different control methodologies and their related issues are reviewed to introduce {CACC} systems from a lower level. Applications of {CACC} technology are demonstrated with detailed literature, which draw an overa landscape of {CACC}, point out current opportunities and chaenges, and anticipate its development in the near future.},
	journaltitle = {{arXiv}},
	author = {Wang, Ziran and Wu, Guoyuan and Barth, Matthew J.},
	date = {2018},
	keywords = {⛔ No {DOI} found},
}

@article{wevolver_2020_2020,
	title = {2020 Autonomous Vehicle Technology Report},
	volume = {3},
	abstract = {Autonomous Vehicle (A/V) technology has been advancing at a rapid pace but the prospect of mass deployment has not been achieved. Inherent expected benefits of this technology such as reduced loss of life, productivity improvements, and greater vehicle efficiency utilization have yet to materialize. The American public’s resistance, and governmental regulation have prevented widespread adoption of this technology that while developed and continuously evolving, is still in its testing phase. Public concerns regarding safety are justified as highly publicized (Lohrmann, 2018) failures of the technology have been reported, heightening public concerns.},
	pages = {193--205},
	journaltitle = {Muma Business Review},
	author = {{Wevolver}},
	date = {2020},
	keywords = {⛔ No {DOI} found},
}

@article{otte_survey_2008,
	title = {A Survey of Machine Learning Approaches to Robotic Path-Planning},
	volume = {5},
	abstract = {The vision system of an autonomous robot can obtain and analyze the necessary environmental information used for autonomous decisions. The processing and recognition of the images captured through the autonomous robot visual system is discussed in this paper. The discussion mainly includes three key issues, namely the extraction of image region of interest, the canceling of image inverse projection error, and pattern recognition and matching. For these three key issues, the paper proposes respective solution and verifies the validity of the above solutions through simulation experiments. Finally, the paper analyzes the existing problems in the scheme.},
	pages = {90--98},
	number = {1},
	journaltitle = {International Journal of Robotics Research},
	author = {Otte, Michael W.},
	date = {2008},
	note = {{ISBN}: 9781509006533},
	keywords = {Autonomous navigation, Efficiency energy, {HSV}, {ROI}, Target recognition, ⛔ No {DOI} found},
}

@article{son_anomaly_2019,
	title = {Anomaly Detection with Adversarial Dual Autoencoders},
	abstract = {Semi-supervised and unsupervised Generative Adversarial Networks ({GAN})-based methods have been gaining popularity in anomaly detection task recently. However, {GAN} training is somewhat challenging and unstable. Inspired from previous work in {GAN}-based image generation, we introduce a {GAN}-based anomaly detection framework – Adversarial Dual Autoencoders ({ADAE}) - consists of two autoencoders as generator and discriminator to increase training stability. We also employ discriminator reconstruction error as anomaly score for better detection performance. Experiments across different datasets of varying complexity show strong evidence of a robust model that can be used in different scenarios, one of which is brain tumor detection.},
	pages = {12},
	author = {Son, Vu Ha and Daisuke, Ueta and Kiyoshi, Hashimoto and Kazuki, Maeno and Pranata, Sugiri and Shen, Sheng Mei},
	date = {2019},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{shim_learning_2021,
	title = {Learning a Domain-Agnostic Visual Representation for Autonomous Driving via Contrastive Loss},
	url = {http://arxiv.org/abs/2103.05902},
	abstract = {Deep neural networks have been widely studied in autonomous driving applications such as semantic segmentation or depth estimation. However, training a neural network in a supervised manner requires a large amount of annotated labels which are expensive and time-consuming to collect. Recent studies leverage synthetic data collected from a virtual environment which are much easier to acquire and more accurate compared to data from the real world, but they usually suffer from poor generalization due to the inherent domain shift problem. In this paper, we propose a Domain-Agnostic Contrastive Learning ({DACL}) which is a two-stage unsupervised domain adaptation framework with cyclic adversarial training and contrastive loss. {DACL} leads the neural network to learn domain-agnostic representation to overcome performance degradation when there exists a difference between training and test data distribution. Our proposed approach achieves better performance in the monocular depth estimation task compared to previous state-of-the-art methods and also shows effectiveness in the semantic segmentation task.},
	author = {Shim, Dongseok and Kim, H. Jin},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.05902},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{xian_latent_2016,
	title = {Latent Embeddings for Zero-shot Classification},
	abstract = {We present a novel latent embedding model for learning a compatibility function between image and class embeddings, in the context of zero-shot classification. The proposed method augments the state-of-the-art bilinear compatibility model by incorporating latent variables. Instead of learning a single bilinear map, it learns a collection of maps with the selection, of which map to use, being a latent variable for the current image-class pair. We train the model with a ranking based objective function which penalizes incorrect rankings of the true class for a given image. We empirically demonstrate that our model improves the state-of-the-art for various class embeddings consistently on three challenging publicly available datasets for the zero-shot setting. Moreover, our method leads to visually highly interpretable results with clear clusters of different fine-grained object properties that correspond to different latent variable maps.},
	pages = {1--27},
	number = {4},
	journaltitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Xian, Yongqin and Akata, Zeynep and Sharma, Gaurav and Nguyen, Quynh and Hein, Matthias and Schiele, Bernt},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1603.08895},
	keywords = {★, ⛔ No {DOI} found},
}

@article{alhaija_augmented_2017,
	title = {Augmented reality meets deep learning for car instance segmentation in urban scenes},
	abstract = {The success of deep learning in computer vision is based on the availability of large annotated datasets. To lower the need for hand labeled images, virtually rendered 3D worlds have recently gained popularity. Unfortunately, creating realistic 3D content is challenging on its own and requires significant human effort. In this work, we propose an alternative paradigm which combines real and synthetic data for learning semantic instance segmentation models. Exploiting the fact that not all aspects of the scene are equally important for this task, we propose to augment real-world imagery with virtual objects of the target category. Capturing real-world images at large scale is easy and cheap, and directly provides real background appearances without the need for creating complex 3D models of the environment. We present an efficient procedure to augment these images with virtual objects. This allows us to create realistic composite images which exhibit both realistic background appearance as well as a large number of complex object arrangements. In contrast to modeling complete 3D environments, our data augmentation approach requires only a few user interactions in combination with 3D shapes of the target object category. We demonstrate the utility of the proposed approach for training a state-of-the-art high-capacity deep model for semantic instance segmentation. In particular, we consider the task of segmenting car instances on the {KITTI} dataset which we have annotated with pixel-accurate ground truth. Our experiments demonstrate that models trained on augmented imagery generalize better than those trained on synthetic data or models trained on limited amounts of annotated real data.},
	pages = {1--12},
	journaltitle = {British Machine Vision Conference 2017, {BMVC} 2017},
	author = {Alhaija, Hassan Abu and Mustikovela, Siva Karthik and Mescheder, Lars and Geiger, Andreas and Rother, Carsten},
	date = {2017},
	note = {{ISBN}: 190172560X},
	keywords = {⛔ No {DOI} found},
}

@article{breitenstein_corner_2020,
	title = {Corner Cases for Visual Perception in Automated Driving: Some Guidance on Detection Approaches},
	abstract = {Automated driving has become a major topic of interest not only in the active research community but also in mainstream media reports. Visual perception of such intelligent vehicles has experienced large progress in the last decade thanks to advances in deep learning techniques but some challenges still remain. One such challenge is the detection of corner cases. They are unexpected and unknown situations that occur while driving. Conventional visual perception methods are often not able to detect them because corner cases have not been witnessed during training. Hence, their detection is highly safety-critical, and detection methods can be applied to vast amounts of collected data to select suitable training data. A reliable detection of corner cases will not only further automate the data selection procedure and increase safety in autonomous driving but can thereby also affect the public acceptance of the new technology in a positive manner. In this work, we continue a previous systematization of corner cases on different levels by an extended set of examples for each level. Moreover, we group detection approaches into different categories and link them with the corner case levels. Hence, we give directions to showcase specific corner cases and basic guidelines on how to technically detect them.},
	pages = {1257--1264},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Breitenstein, Jasmin and Termohlen, Jan Aike and Lipinski, Daniel and Fingscheidt, Tim},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2102.05897},
	note = {4 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{protzmann_large-scale_2020,
	title = {Large-scale modeling of future automotive data traffic towards the edge cloud},
	abstract = {We elaborate traffic types and volumes generated by mobile automotive users and examine their impact on metropolitan transport networks. Utilizing a large-scale joint simulation environment, including vehicle traffic, future automotive applications, and novel network architectures, we generate data traffic demand estimations for the rather large area of Berlin-Brandenburg, Germany. Additionally, we include a comprehensive study about traffic estimations of fixed access traffic. In future works, this model can then be used for developing and testing future network and reconfiguration algorithms.},
	pages = {21--23},
	journaltitle = {Photonische Netze - 20. {ITG}-Fachtagung},
	author = {Protzmann, Robert and Hübner, Karl and Ascheuer, Norbert and Bauknecht, Uwe and Enderle, Tobias and Gebhard, Ulrich and Raack, Christian and Witt, Arthur},
	date = {2020},
	note = {{ISBN}: 9783800749591},
	keywords = {5G, Automotive communication, Distributed data centers, Edge cloud, Multi-domain simulation, {VSimRTI}, ★, ⛔ No {DOI} found},
}

@article{zakir_hossain_comprehensive_2018,
	title = {A comprehensive survey of deep learning for image captioning},
	volume = {0},
	issn = {23318422},
	abstract = {Generating a description of an image is called image captioning. Image captioning requires to recognize the important objects, their attributes and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey paper, we aim to present a comprehensive review of existing deep learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep learning based automatic image captioning.},
	number = {0},
	journaltitle = {{arXiv}},
	author = {Zakir Hossain, M. D. and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1810.04020},
	keywords = {{CNN}, Computer Vision, Deep Learning, Image Captioning, {LSTM}, Natural Language Processing, ⛔ No {DOI} found},
}

@article{chen_learning_2017,
	title = {Learning efficient object detection models with knowledge distillation},
	volume = {2017-Decem},
	issn = {10495258},
	abstract = {Despite significant accuracy improvement in convolutional neural networks ({CNN}) based object detectors, they often require prohibitive runtimes to process an image for real-time applications. State-of-the-art models often use very deep networks with a large number of floating point operations. Efforts such as model compression learn compact models with fewer number of parameters, but with much reduced accuracy. In this work, we propose a new framework to learn compact and fast object detection networks with improved accuracy using knowledge distillation [20] and hint learning [34]. Although knowledge distillation has demonstrated excellent improvements for simpler classification setups, the complexity of detection poses new challenges in the form of regression, region proposals and less voluminous labels. We address this through several innovations such as a weighted cross-entropy loss to address class imbalance, a teacher bounded loss to handle the regression component and adaptation layers to better learn from intermediate teacher distributions. We conduct comprehensive empirical evaluation with different distillation configurations over multiple datasets including {PASCAL}, {KITTI}, {ILSVRC} and {MS}-{COCO}. Our results show consistent improvement in accuracy-speed trade-offs for modern multi-class detection models.},
	pages = {743--752},
	issue = {Nips},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
	date = {2017},
	keywords = {⛔ No {DOI} found},
}

@article{carballo_libre_2020,
	title = {{LIBRE}: The Multiple 3D {LiDAR} Dataset},
	url = {http://arxiv.org/abs/2003.06129},
	shorttitle = {{LIBRE}},
	abstract = {In this work, we present {LIBRE}: {LiDAR} Benchmarking and Reference, a ﬁrst-of-its-kind dataset featuring 10 different {LiDAR} sensors, covering a range of manufacturers, models, and laser conﬁgurations. Data captured independently from each sensor includes three different environments and conﬁgurations: static targets, where objects were placed at known distances and measured from a ﬁxed position within a controlled environment; adverse weather, where static obstacles were measured from a moving vehicle, captured in a weather chamber where {LiDARs} were exposed to different conditions (fog, rain, strong light); and ﬁnally, dynamic trafﬁc, where dynamic objects were captured from a vehicle driven on public urban roads, multiple times at different times of the day, and including supporting sensors such as cameras, infrared imaging, and odometry devices. {LIBRE} will contribute to the research community to (1) provide a means for a fair comparison of currently available {LiDARs}, and (2) facilitate the improvement of existing self-driving vehicles and robotics-related software, in terms of development and tuning of {LiDAR}-based perception algorithms.},
	journaltitle = {{arXiv}:2003.06129 [cs]},
	author = {Carballo, Alexander and Lambert, Jacob and Monrroy-Cano, Abraham and Wong, David Robert and Narksri, Patiphon and Kitsukawa, Yuki and Takeuchi, Eijiro and Kato, Shinpei and Takeda, Kazuya},
	urldate = {2021-04-12},
	date = {2020-06-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.06129},
	note = {10 citations (Semantic Scholar/{arXiv}) [2021-04-12]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, ⛔ No {DOI} found},
}

@article{vs_mega-cda_2021,
	title = {{MeGA}-{CDA}: Memory Guided Attention for Category-Aware Unsupervised Domain Adaptive Object Detection},
	url = {http://arxiv.org/abs/2103.04224},
	abstract = {Existing approaches for unsupervised domain adaptive object detection perform feature alignment via adversarial training. While these methods achieve reasonable improvements in performance, they typically perform category-agnostic domain alignment, thereby resulting in negative transfer of features. To overcome this issue, in this work, we attempt to incorporate category information into the domain adaptation process by proposing Memory Guided Attention for Category-Aware Domain Adaptation ({MeGA}-{CDA}). The proposed method consists of employing category-wise discriminators to ensure category-aware feature alignment for learning domain-invariant discriminative features. However, since the category information is not available for the target samples, we propose to generate memory-guided category-specific attention maps which are then used to route the features appropriately to the corresponding category discriminator. The proposed method is evaluated on several benchmark datasets and is shown to outperform existing approaches.},
	author = {{VS}, Vibashan and Oza, Poojan and Sindagi, Vishwanath A. and Gupta, Vikram and Patel, Vishal M.},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.04224},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{qureshi_motion_2019,
	title = {Motion Planning Networks},
	url = {http://arxiv.org/abs/1806.05767},
	abstract = {Fast and efﬁcient motion planning algorithms are crucial for many state-of-the-art robotics applications such as self-driving cars. Existing motion planning methods become ineffective as their computational complexity increases exponentially with the dimensionality of the motion planning problem. To address this issue, we present Motion Planning Networks ({MPNet}), a neural network-based novel planning algorithm. The proposed method encodes the given workspaces directly from a point cloud measurement and generates the end-to-end collision-free paths for the given start and goal conﬁgurations. We evaluate {MPNet} on various 2D and 3D environments including the planning of a 7 {DOF} Baxter robot manipulator. The results show that {MPNet} is not only consistently computationally efﬁcient in all environments but also generalizes to completely unseen environments. The results also show that the computation time of {MPNet} consistently remains less than 1 second in all presented experiments, which is signiﬁcantly lower than existing state-of-the-art motion planning algorithms.},
	journaltitle = {{arXiv}:1806.05767 [cs, stat]},
	author = {Qureshi, Ahmed H. and Simeonov, Anthony and Bency, Mayur J. and Yip, Michael C.},
	urldate = {2021-04-21},
	date = {2019-02-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1806.05767},
	note = {68 citations (Semantic Scholar/{arXiv}) [2021-04-21]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning, ⛔ No {DOI} found},
}

@article{blum_deep_2019,
	title = {Deep learned path planning via randomized reward-linked-goals and potential space applications},
	issn = {23318422},
	abstract = {Space exploration missions have seen use of increasingly sophisticated robotic systems with ever more autonomy. Deep learning promises to take this even a step further, and has applications for high-level tasks, like path planning, as well as low-level tasks, like motion control, which are critical components for mission efficiency and success. Using deep reinforcement end-to-end learning with randomized reward function parameters during training, we teach a simulated 8 de-gree-of-freedom quadruped ant-like robot to travel anywhere within a perimeter, conducting path plan and motion control on a single neural network, without any system model or prior knowledge of the terrain or environment. Our approach also allows for user specified waypoints, which could translate well to either fully autonomous or semi-autonomous/tele-operated space applications that encounter delay times. We trained the agent using randomly generated waypoints linked to the reward function and passed waypoint coordinates as inputs to the neural network. Such applications show promise on a variety of space exploration robots, including high speed rovers for fast locomotion and legged cave robots for rough terrain.},
	journaltitle = {{arXiv}},
	author = {Blum, Tamir and Jones, William and Yoshida, Kazuya},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1909.06034},
	keywords = {Reinforcement Learning, Locomotion, Path planning,, ⛔ No {DOI} found},
}

@article{wang_long-tailed_2020,
	title = {{LONG}-tailed recognition by Routing Diverse distribution-aware experts},
	issn = {23318422},
	abstract = {Natural data are often long-tail distributed over semantic classes. Existing recognition methods tend to focus on tail performance gain, often at the expense of head performance loss from increased classifier variance. The low tail performance manifests itself in large inter-class confusion and high classifier variance. We aim to reduce both the bias and the variance of a long-tailed classifier by {RoutIng} Diverse Experts ({RIDE}). It has three components: 1) a shared architecture for multiple classifiers (experts); 2) a distribution-aware diversity loss that encourages more diverse decisions for classes with fewer training instances; and 3) an expert routing module that dynamically assigns more ambiguous instances to additional experts. With on-par computational complexity, {RIDE} significantly outperforms the state-of-the-art methods by 5\% to 7\% on all the benchmarks including {CIFAR}100-{LT}, {ImageNet}-{LT} and {iNaturalist}. {RIDE} is also a universal framework that can be applied to different backbone networks and integrated into various long-tailed algorithms and training mechanisms for consistent performance gains.},
	pages = {1--14},
	journaltitle = {{arXiv}},
	author = {Wang, Xudong and Lian, Long and Miao, Zhongqi and Liu, Ziwei and Yu, Stella X.},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2010.01809},
	keywords = {⛔ No {DOI} found},
}

@article{kang_decoupling_2019,
	title = {Decoupling representation and classifier for long-tailed recognition},
	issn = {23318422},
	abstract = {The long-tail distribution of the visual world poses great challenges for deep learning based classification models on how to handle the class imbalance problem. Existing solutions usually involve class-balancing strategies, e.g. by loss re-weighting, data re-sampling, or transfer learning from head- to tail-classes, but most of them adhere to the scheme of jointly learning representations and classifiers. In this work, we decouple the learning procedure into representation learning and classification, and systematically explore how different balancing strategies affect them for long-tailed recognition. The findings are surprising: (1) data imbalance might not be an issue in learning high-quality representations; (2) with representations learned with the simplest instance-balanced (natural) sampling, it is also possible to achieve strong long-tailed recognition ability at little cost by adjusting only the classifier. We conduct extensive experiments and set new state-of-the-art performance on common long-tailed benchmarks like {ImageNet}-{LT}, Places-{LT} and {iNaturalist}, showing that it is possible to outperform carefully designed losses, sampling strategies, even complex modules with memory, by using a straightforward approach that decouples representation and classification.},
	pages = {1--16},
	journaltitle = {{arXiv}},
	author = {Kang, Bingyi and Xie, Saining and Rohrbach, Marcus and Yan, Zhicheng and Gordo, Albert and Feng, Jiashi and Kalantidis, Yannis},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1910.09217},
	keywords = {⛔ No {DOI} found},
}

@article{univ_simon_kriegel_autonomous_2014,
	title = {Autonomous 3D Modeling of Unknown Objects for Active Scene Exploration},
	issn = {1521-3765},
	abstract = {i {ABSTRACT} The objective of this research work was to discrimina te specific crops (here sugarcane) that are of interest to specific industries or the government agencies for better decision making process. Sugarcane in India is a high priority crop for the government given the fact that India is the second largest producer of sugar in the world and the largest consumer of the sugar produced in the world. Any shortage in the sugar produce would have large effects on the sugarcane and its by-product industry and also the country in the end. Hence there is a need to prepare specific crop maps in order to be well equipped for any shortage in agricultural produce. The need of temporal data for continuous monitoring of crops and the unavailability of continuous temporal data is a well-known problem. So this problem was tried to be solved by using data from different optical sensors like {LISS}-{III} and {AWiFS} (from {IRS}-P6) and {TM} from Landsat-5. For an accurate estimation of area, {PCM} (Possiblistic c Means), a possibilistic fuzzy based classifier capable of extracting single class in an image was used. A spectral separability analysis (using single sensor data from {LISS}-{III} and {AWiFS} separately) was conducted between the class of interest (sugarcane plant and ratoon) and the non-interest classes to select the best 2, 3, 4 ... dates combination to discriminate the class of interest. Combinations of these best dates were then classified using {PCM} classifier to extract specific class to find the best overall dates combination to discriminate the same class. In the absence of any reference data the soft classified outputs from {LISS}-{III} sensor were assessed using an entropy measure criterion. The date combinations providing the least entropy was selected as the optimum date combination for discriminating the specific class. This date combination from {LISS}-{III} was used as a reference for assessing the soft classified outputs from {AWiFS} sensor using an image to image accura cy assessment technique. Various operators like {MIN}, {LEAST} and {PROD} were also evaluated for their assessing their behaviour and effectiveness in image to image accuracy assessment. In the second case the effect of data from another sensor i.e. Landsat-5 {TM} when added to the optimum date combination from {LISS}-{III} was also evaluated. It was found that the entropy of the classified outputs from the selected best dates combination and multi sensor approach was lower than the entropy measured from the single sensor ({LISS}-{III}) approach. Lower entropy meant the uncertainty associated with classification was lower and accuracy was higher, and vice-versa. This study explored the applicability of temporal single and multi-sensor data for discrimination of specific crop, sugarcane plant and ratoon. A multi sensor approach helped in increasing the temporal data sampling for the continuous monitoring of crops when data available from any single sensor approach was insufficient. The end result of this study was the detection of best temporal dates for discriminating a specific crop, sugarcane-plant and ratoon. Such information can be used by agricultural scientists in selecting an optimum number of strategically placed temporal images in the crop growing season for discriminating the specific crop accurately. Index Terms- Temporal, Multi sensor, {PCM}, {FERM}, Image to image based accuracy assessment, Ent},
	author = {Univ Simon Kriegel, Dipl-Ing P and Kemper, Univ-Prof A and der Dissertation, Prüfer and Beetz, Univ-Prof M},
	date = {2014},
	keywords = {⛔ No {DOI} found},
}

@article{kaiser_learning_2017,
	title = {Learning to remember rare events},
	issn = {23318422},
	abstract = {Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training. Our memory module can be easily added to any part of a supervised neural network. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can successfully generalize from them. We set new state-of-the-art for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task.},
	pages = {1--10},
	journaltitle = {{arXiv}},
	author = {Kaiser, Łukasz and Nachum, Ofir and Roy, Aurko and Bengio, Samy},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1703.03129},
	keywords = {⛔ No {DOI} found},
}

@article{esterle_modeling_2020,
	title = {Modeling and Testing Multi-Agent Traffic Rules within Interactive Behavior Planning},
	url = {http://arxiv.org/abs/2009.14186},
	abstract = {Autonomous vehicles need to abide by the same rules that humans follow. Some of these traffic rules may depend on multiple agents or time. Especially in situations with traffic participants that interact densely, the interactions with other agents need to be accounted for during planning. To study how multi-agent and time-dependent traffic rules shall be modeled, a framework is needed that restricts the behavior to rule-conformant actions during planning, and that can eventually evaluate the satisfaction of these rules. This work presents a method to model the conformance to traffic rules for interactive behavior planning and to test the ramifications of the traffic rule formulations on metrics such as collision, progress, or rule violations. The interactive behavior planning problem is formulated as a dynamic game and solved using Monte Carlo Tree Search, for which we contribute a new method to integrate history-dependent traffic rules into a decision tree. To study the effect of the rules, we treat it as a multi-objective problem and apply a relaxed lexicographical ordering to the vectorized rewards. We demonstrate our approach in a merging scenario. We evaluate the effect of modeling and combining traffic rules to the eventual compliance in simulation. We show that with our approach, interactive behavior planning while satisfying even complex traffic rules can be achieved. Moving forward, this gives us a generic framework to formalize traffic rules for autonomous vehicles.},
	journaltitle = {{arXiv}:2009.14186 [cs]},
	author = {Esterle, Klemens and Gressenbuch, Luis and Knoll, Alois},
	urldate = {2021-04-14},
	date = {2020-09-29},
	eprinttype = {arxiv},
	eprint = {2009.14186},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-04-14]},
	keywords = {Computer Science - Robotics, ⛔ No {DOI} found},
}

@article{bieshaar_cyclist_2021,
	title = {Cyclist Motion State Forecasting – Going beyond Detection},
	abstract = {In this article, we present two novel methods to forecast the motion states of cyclists. The states we aim to anticipate are waiting, starting, moving, and stopping. This information can be utilized to increase road safety when used in an automated vehicle. We classify the cyclist motion state for every step in a discrete-time horizon using a single neural network in the ﬁrst method. In our second approach, we consider a two-stage model, i.e., a neural network predicts the current and the next motion state, and a second quantile regression neural network ({QRNN}) forecasts the time to transition between these two motion states. Our results show that both methods have advantages and disadvantages. The ﬁrst method can forecast multiple changes in motion state while the second is restricted to a single transition. However, the ﬁrst method is limited to a ﬁxed forecast horizon. The two-stage approach, which forecasts motion and time separately, is more ﬂexible regarding the forecast horizon, i.e., it can forecast very long as well as short time spans. Regarding the transition detection performance, both methods perform equally well. Our experiments show that the time to transition to the next motion state can be forecasted accurately, especially for short-time horizons.},
	pages = {8},
	author = {Bieshaar, Maarten and Zernetsch, Stefan and Riepe, Katharina and Doll, Konrad and Sick, Bernhard},
	date = {2021},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{dong_mcity_2019,
	title = {Mcity Data Collection for Automated Vehicles Study},
	url = {http://arxiv.org/abs/1912.06258},
	abstract = {The main goal of this paper is to introduce the data collection effort at Mcity targeting automated vehicle development. We captured a comprehensive set of data from a set of perception sensors (Lidars, Radars, Cameras) as well as vehicle steering/brake/throttle inputs and an {RTK} unit. Two incabin cameras record the human driver’s behaviors for possible future use. The naturalistic driving on selected open roads is recorded at different time of day and weather conditions. We also perform designed choreography data collection inside the Mcity test facility focusing on vehicle to vehicle, and vehicle to vulnerable road user interactions–which is quite unique among existing open-source datasets. The vehicle platform, data content, tags/labels, and selected analysis results are shown in this paper1.},
	journaltitle = {{arXiv}:1912.06258 [cs, eess]},
	author = {Dong, Yiqun and Zhong, Yuanxin and Yu, Wenbo and Zhu, Minghan and Lu, Pingping and Fang, Yeyang and Hong, Jiajun and Peng, Huei},
	urldate = {2021-04-12},
	date = {2019-12-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1912.06258},
	note = {5 citations (Semantic Scholar/{arXiv}) [2021-04-12]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, ⛔ No {DOI} found},
}

@article{sinha_neural_2020,
	title = {Neural bridge sampling for evaluating safety-critical autonomous systems},
	volume = {0},
	abstract = {Learning-based methodologies increasingly find applications in safety-critical domains like autonomous driving and medical robotics. Due to the rare nature of dangerous events, real-world testing is prohibitively expensive and unscalable. In this work, we employ a probabilistic approach to safety evaluation in simulation, where we are concerned with computing the probability of dangerous events. We develop a novel rare-event simulation method that combines exploration, exploitation, and optimization techniques to find failure modes and estimate their rate of occurrence. We provide rigorous guarantees for the performance of our method in terms of both statistical and computational efficiency. Finally, we demonstrate the efficacy of our approach on a variety of scenarios, illustrating its usefulness as a tool for rapid sensitivity analysis and model comparison that are essential to developing and testing safety-critical autonomous systems.},
	pages = {1--27},
	number = {1},
	journaltitle = {{arXiv}},
	author = {Sinha, Aman and O’Kelly, Matthew and Duchi, John and Tedrake, Russ},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2008.10581},
	keywords = {⛔ No {DOI} found},
}

@article{chen_mvlidarnet_2020,
	title = {{MVLidarNet}: Real-time multi-class scene understanding for autonomous driving using multiple views},
	issn = {23318422},
	abstract = {— Autonomous driving requires the inference of actionable information such as detecting and classifying objects, and determining the drivable space. To this end, we present a two-stage deep neural network ({MVLidarNet}) for multi-class object detection and drivable segmentation using multiple views of a single {LiDAR} point cloud. The first stage processes the point cloud projected onto a perspective view in order to semantically segment the scene. The second stage then processes the point cloud (along with semantic labels from the first stage) projected onto a bird’s eye view, to detect and classify objects. Both stages are simple encoder-decoders. We show that our multi-view, multi-stage, multi-class approach is able to detect and classify objects while simultaneously determining the drivable space using a single {LiDAR} scan as input, in challenging scenes with more than one hundred vehicles and pedestrians at a time. The system operates efficiently at 150 fps on an embedded {GPU} designed for a self-driving car, including a postprocessing step to maintain identities over time. We show results on both {KITTI} and a much larger internal dataset, thus demonstrating the method’s ability to scale by an order of magnitude.},
	journaltitle = {{arXiv}},
	author = {Chen, Ke and Oldja, Ryan and Smolyanskiy, Nikolai and Birchfield, Stan and Popov, Alexander and Wehr, David and Eden, Ibrahim and Pehserl, Joachim},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2006.05518},
	keywords = {⛔ No {DOI} found},
}

@article{mandal_object_2020,
	title = {Object Detection and Tracking Algorithms for Vehicle Counting: A Comparative Analysis},
	url = {http://arxiv.org/abs/2007.16198},
	shorttitle = {Object Detection and Tracking Algorithms for Vehicle Counting},
	abstract = {The rapid advancement in the field of deep learning and high performance computing has highly augmented the scope of video based vehicle counting system. In this paper, the authors deploy several state of the art object detection and tracking algorithms to detect and track different classes of vehicles in their regions of interest ({ROI}). The goal of correctly detecting and tracking vehicles' in their {ROI} is to obtain an accurate vehicle count. Multiple combinations of object detection models coupled with different tracking systems are applied to access the best vehicle counting framework. The models' addresses challenges associated to different weather conditions, occlusion and low-light settings and efficiently extracts vehicle information and trajectories through its computationally rich training and feedback cycles. The automatic vehicle counts resulting from all the model combinations are validated and compared against the manually counted ground truths of over 9 hours' traffic video data obtained from the Louisiana Department of Transportation and Development. Experimental results demonstrate that the combination of {CenterNet} and Deep {SORT}, Detectron2 and Deep {SORT}, and {YOLOv}4 and Deep {SORT} produced the best overall counting percentage for all vehicles.},
	journaltitle = {{arXiv}:2007.16198 [cs]},
	author = {Mandal, Vishal and Adu-Gyamfi, Yaw},
	urldate = {2021-04-13},
	date = {2020-07-31},
	eprinttype = {arxiv},
	eprint = {2007.16198},
	note = {2 citations (Semantic Scholar/{arXiv}) [2021-04-14]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No {DOI} found},
}

@article{caesar_nuscenes_2020,
	title = {{nuScenes}: A multimodal dataset for autonomous driving},
	url = {http://arxiv.org/abs/1903.11027},
	shorttitle = {{nuScenes}},
	abstract = {Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present {nuTonomy} scenes ({nuScenes}), the ﬁrst dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree ﬁeld of view. {nuScenes} comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering {KITTI} dataset. We deﬁne novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online1.},
	journaltitle = {{arXiv}:1903.11027 [cs, stat]},
	author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
	urldate = {2021-04-12},
	date = {2020-05-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1903.11027},
	note = {527 citations (Semantic Scholar/{arXiv}) [2021-04-12]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning, ⛔ No {DOI} found},
}

@article{salehi_multiresolution_2020,
	title = {Multiresolution knowledge distillation for anomaly detection},
	issn = {23318422},
	abstract = {Unsupervised representation learning has proved to be a critical component of anomaly detection/localization in images. The challenges to learn such a representation are two-fold. Firstly, the sample size is not often large enough to learn a rich generalizable representation through conventional techniques. Secondly, while only normal samples are available at training, the learned features should be discriminative of normal and anomalous samples. Here, we propose to use the “distillation” of features at various layers of an expert network, pre-trained on {ImageNet}, into a simpler cloner network to tackle both issues. We detect and localize anomalies using the discrepancy between the expert and cloner networks’ intermediate activation values given the input data. We show that considering multiple intermediate hints in distillation leads to better exploiting the expert’s knowledge and more distinctive discrepancy compared to solely utilizing the last layer activation values. Notably, previous methods either fail in precise anomaly localization or need expensive region-based training. In contrast, with no need for any special or intensive training procedure, we incorporate interpretability algorithms in our novel framework for localization of anomalous regions. Despite the striking contrast between some test datasets and {ImageNet}, we achieve competitive or significantly superior results compared to the {SOTA} methods on {MNIST}, F-{MNIST}, {CIFAR}-10, {MVTecAD}, Retinal-{OCT}, and two Medical datasets on both anomaly detection and localization.},
	journaltitle = {{arXiv}},
	author = {Salehi, Mohammadreza and Sadjadi, Niousha and Baselizadeh, Soroosh and Rohban, Mohammad Hossein and Rabiee, Hamid R.},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2011.11108},
	keywords = {⛔ No {DOI} found},
}

@article{di_biase_pixel-wise_2021,
	title = {Pixel-wise Anomaly Detection in Complex Driving Scenes},
	url = {http://arxiv.org/abs/2103.05445},
	abstract = {The inability of state-of-the-art semantic segmentation methods to detect anomaly instances hinders them from being deployed in safety-critical and complex applications, such as autonomous driving. Recent approaches have focused on either leveraging segmentation uncertainty to identify anomalous areas or re-synthesizing the image from the semantic label map to find dissimilarities with the input image. In this work, we demonstrate that these two methodologies contain complementary information and can be combined to produce robust predictions for anomaly segmentation. We present a pixel-wise anomaly detection framework that uses uncertainty maps to improve over existing re-synthesis methods in finding dissimilarities between the input and generated images. Our approach works as a general framework around already trained segmentation networks, which ensures anomaly detection without compromising segmentation accuracy, while significantly outperforming all similar methods. Top-2 performance across a range of different anomaly datasets shows the robustness of our approach to handling different anomaly instances.},
	author = {Di Biase, Giancarlo and Blum, Hermann and Siegwart, Roland and Cadena, Cesar},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.05445},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{stentz_optimal_1995,
	title = {Optimal and efficient path planning for unknown and dynamic environments},
	volume = {10},
	issn = {08268185},
	abstract = {The task of planning trajectories for a mobile robot has received considerable attention in the research literature. Most work assumes the robot has a complete and accurate model of its environment before it begins to move; less attention has been paid to the problem of partially known environments. Existing approaches plan an initial path based on known information and then modify the plan locally or re-plan the entire path as the robot discovers obstacles with its sensors, sacrificing optimality or computational efficiency respectively. This paper introduces a new algorithm, D*, capable of planning paths in unknown, partially known, and changing environments in an efficient, optimal, and complete manner.},
	pages = {89--100},
	number = {3},
	journaltitle = {International Journal of Robotics and Automation},
	author = {Stentz, Anthony},
	date = {1995},
	keywords = {⛔ No {DOI} found},
}

@article{masana_metric_2018,
	title = {Metric learning for novelty and anomaly detection},
	issn = {23318422},
	abstract = {When neural networks process images which do not resemble the distribution seen during training, so called out-of-distribution images, they often make wrong predictions, and do so too confidently. The capability to detect out-of-distribution images is therefore crucial for many real-world applications. We divide out-of-distribution detection between novelty detection-images of classes which are not in the training set but are related to those-, and anomaly detection-images with classes which are unrelated to the training set. By related we mean they contain the same type of objects, like digits in {MNIST} and {SVHN}. Most existing work has focused on anomaly detection, and has addressed this problem considering networks trained with the cross-entropy loss. Differently from them, we propose to use metric learning which does not have the drawback of the softmax layer (inherent to cross-entropy methods), which forces the network to divide its prediction power over the learned classes. We perform extensive experiments and evaluate both novelty and anomaly detection, even in a relevant application such as traffic sign recognition, obtaining comparable or better results than previous works. c 2018. The copyright of this document resides with its authors.},
	pages = {1--17},
	journaltitle = {{arXiv}},
	author = {Masana, Marc and Ruiz, Idoia and Serrat, Joan and Van De Weijer, Joost and Lopez, Antonio M.},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1808.05492},
	keywords = {⛔ No {DOI} found},
}

@article{shah_cadp_2018,
	title = {{CADP}: A novel dataset for {CCTV} traffic camera based accident analysis},
	volume = {1},
	issn = {23318422},
	abstract = {This paper presents a novel dataset for traffic accidents analysis. Our goal is to resolve the lack of public data for research about automatic spatio-temporal annotations for traffic safety in the roads. Through the analysis of the proposed dataset, we observed a significant degradation of object detection in pedestrian category in our dataset, due to the object sizes and complexity of the scenes. To this end, we propose to integrate contextual information into conventional Faster R-{CNN} using Context Mining ({CM}) and Augmented Context Mining ({ACM}) to complement the accuracy for small pedestrian detection. Our experiments indicate a considerable improvement in object detection accuracy: +8.51\% for {CM} and +6.20\% for {ACM}. Finally, we demonstrate the performance of accident forecasting in our dataset using Faster R-{CNN} and an Accident {LSTM} architecture. We achieved an average of 1.684 seconds in terms of Time-To-Accident measure with an Average Precision of 47.25\%. Our Webpage for the paper is https://goo.gl/{cqK}2wE.},
	issue = {i},
	journaltitle = {{arXiv}},
	author = {Shah, Ankit Parag and Lamare, Jean Bapstite and Nguyen-Anh, Tuan and Hauptmann, Alexander},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1809.05782},
	note = {{ISBN}: 9781538692943},
	keywords = {⛔ No {DOI} found},
}

@article{bieshaar_out--distribution_2021,
	title = {Out-of-distribution Detection and Generation using Soft Brownian Offset Sampling and Autoencoders},
	author = {Bieshaar, Maarten},
	date = {2021},
	keywords = {⛔ No {DOI} found},
}

@article{houston_one_2020,
	title = {One Thousand and One Hours: Self-driving Motion Prediction Dataset},
	url = {http://arxiv.org/abs/2006.14480},
	shorttitle = {One Thousand and One Hours},
	abstract = {Motivated by the impact of large-scale datasets on {ML} systems we present the largest self-driving dataset for motion prediction to date, containing over 1,000 hours of data. This was collected by a ﬂeet of 20 autonomous vehicles along a ﬁxed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. On top of this, the dataset contains a high-deﬁnition semantic map with 15,242 labelled elements and a high-deﬁnition aerial view over the area. We show that using a dataset of this size dramatically improves performance for key self-driving problems. Combined with the provided software kit, this collection forms the largest and most detailed dataset to date for the development of self-driving machine learning tasks, such as motion forecasting, motion planning and simulation.},
	journaltitle = {{arXiv}:2006.14480 [cs]},
	author = {Houston, John and Zuidhof, Guido and Bergamini, Luca and Ye, Yawei and Chen, Long and Jain, Ashesh and Omari, Sammy and Iglovikov, Vladimir and Ondruska, Peter},
	urldate = {2021-04-12},
	date = {2020-11-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.14480},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, ⛔ No {DOI} found},
}

@article{wang_recursive_2021,
	title = {Recursive Training for Zero-Shot Semantic Segmentation},
	url = {http://arxiv.org/abs/2103.00086},
	abstract = {General purpose semantic segmentation relies on a backbone {CNN} network to extract discriminative features that help classify each image pixel into a 'seen' object class (ie., the object classes available during training) or a background class. Zero-shot semantic segmentation is a challenging task that requires a computer vision model to identify image pixels belonging to an object class which it has never seen before. Equipping a general purpose semantic segmentation model to separate image pixels of 'unseen' classes from the background remains an open challenge. Some recent models have approached this problem by fine-tuning the final pixel classification layer of a semantic segmentation model for a Zero-Shot setting, but struggle to learn discriminative features due to the lack of supervision. We propose a recursive training scheme to supervise the retraining of a semantic segmentation model for a zero-shot setting using a pseudo-feature representation. To this end, we propose a Zero-Shot Maximum Mean Discrepancy ({ZS}-{MMD}) loss that weighs high confidence outputs of the pixel classification layer as a pseudo-feature representation, and feeds it back to the generator. By closing-the-loop on the generator end, we provide supervision during retraining that in turn helps the model learn a more discriminative feature representation for 'unseen' classes. We show that using our recursive training and {ZS}-{MMD} loss, our proposed model achieves state-of-the-art performance on the Pascal-{VOC} 2012 dataset and Pascal-Context dataset.},
	author = {Wang, Ce and Farazi, Moshiur and Barnes, Nick},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.00086},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{shalev-shwartz_safe_2016,
	title = {Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving},
	url = {http://arxiv.org/abs/1610.03295},
	abstract = {Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained. In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an "Option Graph" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further.},
	author = {Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1610.03295},
	note = {316 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{guerrero-ib_sensor_2018,
	title = {Sensor Technologies for Intelligent Transportation Systems},
	abstract = {Modern society faces serious problems with transportation systems, including but not limited to trafﬁc congestion, safety, and pollution. Information communication technologies have gained increasing attention and importance in modern transportation systems. Automotive manufacturers are developing in-vehicle sensors and their applications in different areas including safety, trafﬁc management, and infotainment. Government institutions are implementing roadside infrastructures such as cameras and sensors to collect data about environmental and trafﬁc conditions. By seamlessly integrating vehicles and sensing devices, their sensing and communication capabilities can be leveraged to achieve smart and intelligent transportation systems. We discuss how sensor technology can be integrated with the transportation infrastructure to achieve a sustainable Intelligent Transportation System ({ITS}) and how safety, trafﬁc control and infotainment applications can beneﬁt from multiple sensors deployed in different elements of an {ITS}. Finally, we discuss some of the challenges that need to be addressed to enable a fully operational and cooperative {ITS} environment.},
	pages = {24},
	author = {Guerrero-Ib, Juan},
	date = {2018},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{hero_iii_sensor_2011,
	title = {Sensor Management: Past, Present, and Future},
	url = {http://arxiv.org/abs/1109.2363},
	shorttitle = {Sensor Management},
	abstract = {Sensor systems typically operate under resource constraints that prevent the simultaneous use of all resources all of the time. Sensor management becomes relevant when the sensing system has the capability of actively managing these resources; i.e., changing its operating conﬁguration during deployment in reaction to previous measurements. Examples of systems in which sensor management is currently used or is likely to be used in the near future include autonomous robots, surveillance and reconnaissance networks, and waveform-agile radars. This paper provides an overview of the theory, algorithms, and applications of sensor management as it has developed over the past decades and as it stands today.},
	journaltitle = {{arXiv}:1109.2363 [cs, math, stat]},
	author = {Hero {III}, Alfred O. and Cochran, Douglas},
	urldate = {2021-03-29},
	date = {2011-09-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1109.2363},
	note = {195 citations (Semantic Scholar/{arXiv}) [2021-03-29]},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control, Statistics - Applications, ⛔ No {DOI} found},
}

@article{qi_pointnet_2017,
	title = {{PointNet}++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
	url = {http://arxiv.org/abs/1706.02413},
	shorttitle = {{PointNet}++},
	abstract = {Few prior works study deep learning on point sets. {PointNet} by Qi et al. is a pioneer in this direction. However, by design {PointNet} does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies {PointNet} recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called {PointNet}++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	journaltitle = {{arXiv}:1706.02413 [cs]},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	urldate = {2021-03-29},
	date = {2017-06-07},
	eprinttype = {arxiv},
	eprint = {1706.02413},
	note = {2315 citations (Semantic Scholar/{arXiv}) [2021-03-29]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No {DOI} found},
}

@article{aptiv_safety_2019,
	title = {Safety first for automated driving},
	abstract = {This publication summarizes widely known safety by design and verification and validation (V\&V) methods of {SAE} L3 and L4 automated driving. This summary is required for maximizing the evidence of a positive risk balance of automated driving solutions compared to the average human driving performance. There is already a vast array of publications focusing on only specific subtopics of automated driving. In contrast, this publication promotes a comprehensive approach to safety relevant topics of automated driving and is based on the input of {OEMs}, tiered suppliers and key technology providers. The objective of this publication is to systematically break down safety principles into safety by design capabilities, elements and architectures and then to summarize the V\&V methods in order to demonstrate the positive risk balance. With Level 3 and 4 automated driving systems still under development, this publication represents guidance for potential methods and considerations in the development and V\&V. This publication is not intended to serve as a final statement or minimum or maximum guideline or standard for automated driving systems. Instead, the intent of this publication is to contribute to current activities working towards the industry- wide standardization of automated driving.},
	pages = {1--157},
	journaltitle = {White Paper},
	author = {{APTIV} and {Audi} and {Baidu} and {BMW} and {Continental} and {Daimler} and {FCA} and {HERE} and {Infineon} and {Intel} and {Volkswagen}},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{isele_radar_2020,
	title = {Radar Artifact Labeling Framework ({RALF}): Method for Plausible Radar Detections in Datasets},
	url = {http://arxiv.org/abs/2012.01993},
	shorttitle = {Radar Artifact Labeling Framework ({RALF})},
	abstract = {Research on localization and perception for Autonomous Driving is mainly focused on camera and {LiDAR} datasets, rarely on radar data. Manually labeling sparse radar point clouds is challenging. For a dataset generation, we propose the cross sensor Radar Artifact Labeling Framework ({RALF}). Automatically generated labels for automotive radar data help to cure radar shortcomings like artifacts for the application of artificial intelligence. {RALF} provides plausibility labels for radar raw detections, distinguishing between artifacts and targets. The optical evaluation backbone consists of a generalized monocular depth image estimation of surround view cameras plus {LiDAR} scans. Modern car sensor sets of cameras and {LiDAR} allow to calibrate image-based relative depth information in overlapping sensing areas. K-Nearest Neighbors matching relates the optical perception point cloud with raw radar detections. In parallel, a temporal tracking evaluation part considers the radar detections' transient behavior. Based on the distance between matches, respecting both sensor and model uncertainties, we propose a plausibility rating of every radar detection. We validate the results by evaluating error metrics on semi-manually labeled ground truth dataset of \$3.28{\textbackslash}cdot10{\textasciicircum}6\$ points. Besides generating plausible radar detections, the framework enables further labeled low-level radar signal datasets for applications of perception and Autonomous Driving learning tasks.},
	journaltitle = {{arXiv}:2012.01993 [cs]},
	author = {Isele, Simon T. and Schilling, Marcel P. and Klein, Fabian E. and Saralajew, Sascha and Zoellner, J. Marius},
	urldate = {2021-04-19},
	date = {2020-12-03},
	eprinttype = {arxiv},
	eprint = {2012.01993},
	keywords = {68T40, 68T45, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, I.2.10, I.2.9, ⛔ No {DOI} found},
}

@article{korthals_deep_2018,
	title = {Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data},
	url = {https://openreview.net/forum?id=rJl8FoRcY7},
	abstract = {Deriving a general formulation of a multi-modal {VAE} from the joint marginal log-likelihood.},
	author = {Korthals, Timo and Hesse, Marc and Leitner, Jürgen},
	urldate = {2021-04-18},
	date = {2018-09-27},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{casas_importance_2020,
	title = {The Importance of Prior Knowledge in Precise Multimodal Prediction},
	abstract = {Roads have well defined geometries, topologies, and traffic rules. While this has been widely exploited in motion planning methods to produce maneuvers that obey the law, little work has been devoted to utilize these priors in perception and motion forecasting methods. In this paper we propose to incorporate these structured priors as a loss function. In contrast to imposing hard constraints, this approach allows the model to handle non-compliant maneuvers when those happen in the real world. Safe motion planning is the end goal, and thus a probabilistic characterization of the possible future developments of the scene is key to choose the plan with the lowest expected cost. Towards this goal, we design a framework that leverages {REINFORCE} to incorporate non-differentiable priors over sample trajectories from a probabilistic model, thus optimizing the whole distribution. We demonstrate the effectiveness of our approach on real-world self-driving datasets containing complex road topologies and multi-agent interactions. Our motion forecasts not only exhibit better precision and map understanding, but most importantly result in safer motion plans taken by our self-driving vehicle. We emphasize that despite the importance of this evaluation, it has been often overlooked by previous perception and motion forecasting works.},
	journaltitle = {{arXiv}},
	author = {Casas, Sergio and Gulino, Cole and Suo, Simon and Urtasun, Raquel},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2006.02636},
	keywords = {⛔ No {DOI} found},
}

@article{fu_similarity_2005,
	title = {{SIMILARITY} {BASED} {VEHICLE} {TRAJECTORY} {CLUSTERING} {AND} {ANOMALY} {DETECTION}},
	abstract = {In this paper, we proposed a hierarchical clustering frame-work to classify vehicle motion trajectories in real traffic video based on their pairwise similarities. First raw trajec-tories are pre-processed and resampled at equal space inter-vals. Then spectral clustering is used to group trajectories with similar spatial patterns. Dominant paths and lanes can be distinguished as a result of two-layer hierarchical cluster-ing. Detection of novel trajectories is also possible based on the clustering results. Experimental results demonstrate the superior performance of spectral clustering compared with conventional fuzzy K-means clustering and some results of anomaly detection are presented.},
	pages = {602--605},
	journaltitle = {Compute},
	author = {Fu, Zhouyu and Hu, Weiming and Tan, Tieniu},
	date = {2005},
	note = {{ISBN}: 0780391349},
	keywords = {⛔ No {DOI} found},
}

@article{koopman_heavy_2018,
	title = {The Heavy Tail Safety Ceiling},
	url = {http://users.ece.cmu.edu/~koopman},
	abstract = {Creating safe autonomous vehicles will require not only extensive training and testing against realistic operational scenarios, but also dealing with uncertainty.},
	pages = {1--2},
	journaltitle = {Automated and Connected Vehicle Systems Testing Symposium},
	author = {Koopman, Philip},
	date = {2018},
	keywords = {preprint, ⛔ No {DOI} found},
}

@article{daniels_reducing_2020,
	title = {Reducing the Representation Error of {GAN} Image Priors Using the Deep Decoder},
	url = {http://arxiv.org/abs/2001.08747},
	abstract = {Generative models, such as {GANs}, learn an explicit low-dimensional representation of a particular class of images, and so they may be used as natural image priors for solving inverse problems such as image restoration and compressive sensing. {GAN} priors have demonstrated impressive performance on these tasks, but they can exhibit substantial representation error for both in-distribution and out-of-distribution images, because of the mismatch between the learned, approximate image distribution and the data generating distribution. In this paper, we demonstrate a method for reducing the representation error of {GAN} priors by modeling images as the linear combination of a {GAN} prior with a Deep Decoder. The deep decoder is an underparameterized and most importantly unlearned natural signal model similar to the Deep Image Prior. No knowledge of the speciﬁc inverse problem is needed in the training of the {GAN} underlying our method. For compressive sensing and image superresolution, our hybrid model exhibits consistently higher {PSNRs} than both the {GAN} priors and Deep Decoder separately, both on in-distribution and out-of-distribution images. This model provides a method for extensibly and cheaply leveraging both the beneﬁts of learned and unlearned image recovery priors in inverse problems.},
	journaltitle = {{arXiv}:2001.08747 [cs, eess, stat]},
	author = {Daniels, Max and Hand, Paul and Heckel, Reinhard},
	urldate = {2021-04-16},
	date = {2020-01-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2001.08747},
	note = {1 citations (Semantic Scholar/{arXiv}) [2021-04-16]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning, ⛔ No {DOI} found},
}

@article{alsamhi_survey_2016,
	title = {Survey on Collaborative Smart Drones and Internet of Things for Improving Smartness of Smart Cities},
	volume = {4},
	abstract = {Smart cities contain intelligent things which can intelligently automatically and collaboratively enhance life quality, save people’s lives, and act a sustainable resource ecosystem. To achieve these advanced collaborative technologies such as drones, robotics, artiﬁcial intelligence, and Internet of Things ({IoT}) are required to increase the smartness of smart cities by improving the connectivity, energy efﬁciency, and quality of services ({QoS}). Therefore, collaborative drones and {IoT} play a vital role in supporting a lot of smart-city applications such as those involved in communication, transportation, agriculture,safety and security, disaster mitigation, environmental protection, service delivery, energy saving, e-waste reduction, weather monitoring, healthcare, etc. This paper presents a survey of the potential techniques and applications of collaborative drones and {IoT} which have recently been proposed in order to increase the smartness of smart cities. It provides a comprehensive overview highlighting the recent and ongoing research on collaborative drone and {IoT} in improving the real-time application of smart cities. This survey is different from previous ones in term of breadth, scope, and focus. In particular, we focus on the new concept of collaborative drones and {IoT} for improving smart-city applications. This survey attempts to show how collaborative drones and {IoT} improve the smartness of smart cities based on data collection, privacy and security, public safety, disaster management, energy consumption and quality of life in smart cities. It mainly focuses on the measurement of the smartness of smart cities, i.e., environmental aspects, life quality, public safety, and disaster management.},
	pages = {30},
	journaltitle = {{IEEE} Access},
	author = {Alsamhi, S H and Ma, Ou and Ansari, Mohammad Samar and Almalki, F},
	date = {2016},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{johansson_knowledge_2017,
	title = {The knowledge base of machine learning , across data analytics teams in a matrix organization An exploratory case study on machine learning},
	author = {Johansson, Josefin},
	date = {2017},
	keywords = {⛔ No {DOI} found},
}

@article{joshi_tie_2021,
	title = {{TIE}: Time-Informed Exploration For Robot Motion Planning},
	url = {http://arxiv.org/abs/2004.05241},
	shorttitle = {{TIE}},
	abstract = {Anytime sampling-based methods are an attractive technique for solving kino-dynamic motion planning problems. These algorithms scale well to higher dimensions and can efﬁciently handle state and control constraints. However, an intelligent exploration strategy is required to accelerate their convergence and avoid redundant computations. Using ideas from reachability analysis, this work deﬁnes a “{TimeInformed} Set”, that focuses the search for time-optimal kinodynamic planning after an initial solution is found. Such a Time-Informed Set includes all trajectories that can potentially improve the current best solution and hence exploration outside this set is redundant. Benchmarking experiments show that an exploration strategy based on the {TIS} can accelerate the convergence of sampling-based kino-dynamic motion planners.},
	journaltitle = {{arXiv}:2004.05241 [cs]},
	author = {Joshi, Sagar Suhas and Hutchinson, Seth and Tsiotras, Panagiotis},
	urldate = {2021-04-21},
	date = {2021-03-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.05241},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-04-21]},
	keywords = {Computer Science - Robotics, ⛔ No {DOI} found},
}

@article{kottke_tutorial_2011,
	title = {Tutorial on Active Learning},
	url = {https://www.youtube.com/watch?v=oCuxjkRdCMc&t=1s},
	author = {Kottke, Daniel},
	date = {2011},
	keywords = {(), ⛔ No {DOI} found},
}

@article{lecue_thales_2019,
	title = {Thales {XAI} platform: Adaptable explanation of machine learning systems - A knowledge graphs perspective},
	volume = {2456},
	issn = {16130073},
	abstract = {Explanation in Machine Learning systems has been identified to be the main asset to have for large scale deployment of Artificial Intelligence ({AI}) in critical systems. Explanations could be example-, features-, semantics-based or even counterfactual to potentially action on an {AI} system; they could be represented in many different ways e.g., textual, graphical, or visual. All representations serve different means, purpose and operators. We built the first-of-its-kind {XAI} ({eXplainable} {AI}) platform for critical systems i.e., Thales {XAI} Platform which aims at serving explanations through various forms. This paper emphasizes on the semantics-based explanations for Machine Learning systems.},
	pages = {315--316},
	journaltitle = {{CEUR} Workshop Proceedings},
	author = {Lécué, Freddy and Abeloos, Baptiste and Anctil, Jonathan and Bergeron, Manuel and Dalla-Rosa, Damien and Corbeil-Letourneau, Simon and Martet, Florian and Pommellet, Tanguy and Salvan, Laura and Veilleux, Simon and Ziaeefard, Maryam},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{yu_unsupervised_2019,
	title = {Unsupervised Out-of-Distribution Detection by Maximum Classifier Discrepancy},
	issn = {23318422},
	abstract = {Since deep learning models have been implemented in many commercial applications, it is important to detect out-of-distribution ({OOD}) inputs correctly to maintain the performance of the models, ensure the quality of the collected data, and prevent the applications from being used for other-than-intended purposes. In this work, we propose a two-head deep convolutional neural network ({CNN}) and maximize the discrepancy between the two classifiers to detect {OOD} inputs. We train a two-head {CNN} consisting of one common feature extractor and two classifiers which have different decision boundaries but can classify in-distribution ({ID}) samples correctly. Unlike previous methods, we also utilize unlabeled data for unsupervised training and we use these unlabeled data to maximize the discrepancy between the decision boundaries of two classifiers to push {OOD} samples outside the manifold of the in-distribution ({ID}) samples, which enables us to detect {OOD} samples that are far from the support of the {ID} samples. Overall, our approach significantly outperforms other state-of-the-art methods on several {OOD} detection benchmarks and two cases of real-world simulation.},
	pages = {9518--9526},
	journaltitle = {{arXiv}},
	author = {Yu, Qing and Aizawa, Kiyoharu},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{heidecker_towards_2020,
	title = {Towards Corner Case Detection by Modeling the Uncertainty of Instance Segmentation Networks},
	journaltitle = {{ICPR} Workshop on Integrated Artificial Intelligence in Data Science},
	author = {Heidecker, Florian},
	date = {2020},
	keywords = {\#nosource, ⛔ No {DOI} found},
}

@article{nister_safety_2019,
	title = {The Safety Force Field},
	journaltitle = {{NVIDIA} White Paper},
	author = {Nistér, David and Lee, Hon-Leung and Ng, Julia and Wang, Yizhou},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{mirus_detection_2020,
	title = {Detection of abnormal driving situations using distributed representations and unsupervised learning},
	abstract = {In this paper, we present an anomaly detection system employing an unsupervised learning model trained on the information encapsulated within distributed vector representations of automotive scenes. Our representations allows us to encode automotive scenes with a varying number of traffic participants in a vector of fixed length. We train a neural network autoencoder in unsupervised fashion to detect anomalies based on this representation. We demonstrate the usefulness of our approach through a quantitative analysis on two real-world data-sets.},
	pages = {363--368},
	issue = {October},
	journaltitle = {{ESANN} 2020 - Proceedings, 28th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
	author = {Mirus, Florian and Stewart, Terrence C. and Conradt, Jorg},
	date = {2020},
	note = {{ISBN}: 9782875870742},
	keywords = {⛔ No {DOI} found},
}

@article{okelly_scalable_2018,
	title = {Scalable end-to-end autonomous vehicle testing via rare-event simulation},
	volume = {2018-Decem},
	issn = {10495258},
	abstract = {While recent developments in autonomous vehicle ({AV}) technology highlight substantial progress, we lack tools for rigorous and scalable testing. Real-world testing, the de facto evaluation environment, places the public in danger, and, due to the rare nature of accidents, will require billions of miles in order to statistically validate performance claims. We implement a simulation framework that can test an entire modern autonomous driving system, including, in particular, systems that employ deep-learning perception and control algorithms. Using adaptive importance-sampling methods to accelerate rare-event probability evaluation, we estimate the probability of an accident under a base distribution governing standard traffic behavior. We demonstrate our framework on a highway scenario, accelerating system evaluation by 2-20 times over naive Monte Carlo sampling methods and 10-300P times (where P is the number of processors) over real-world testing.},
	pages = {9827--9838},
	issue = {{NeurIPS}},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {O'Kelly, Matthew and Duchi, John and Sinha, Aman and Namkoong, Hongseok and Tedrake, Russ},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1811.00145},
	keywords = {⛔ No {DOI} found},
}

@article{heidecker_towards_2019,
	title = {Towards Corner Case Identification in Cyclists’ Trajectories},
	author = {Heidecker, Florian and Bieshaar, Maarten and Sick, Bernhard},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{suo_trafficsim_2021,
	title = {{TrafficSim}: Learning to Simulate Realistic Multi-Agent Behaviors},
	url = {http://arxiv.org/abs/2101.06557},
	shorttitle = {{TrafficSim}},
	abstract = {Simulation has the potential to massively scale evaluation of self-driving systems enabling rapid development as well as safe deployment. To close the gap between simulation and the real world, we need to simulate realistic multi-agent behaviors. Existing simulation environments rely on heuristicbased models that directly encode trafﬁc rules, which cannot capture irregular maneuvers (e.g., nudging, U-turns) and complex interactions (e.g., yielding, merging). In contrast, we leverage real-world data to learn directly from human demonstration and thus capture a more diverse set of actor behaviors. To this end, we propose {TRAFFICSIM}, a multi-agent behavior model for realistic trafﬁc simulation. In particular, we leverage an implicit latent variable model to parameterize a joint actor policy that generates sociallyconsistent plans for all actors in the scene jointly. To learn a robust policy amenable for long horizon simulation, we unroll the policy in training and optimize through the fully differentiable simulation across time. Our learning objective incorporates both human demonstrations as well as common sense. We show {TRAFFICSIM} generates signiﬁcantly more realistic and diverse trafﬁc scenarios as compared to a diverse set of baselines. Notably, we can exploit trajectories generated by {TRAFFICSIM} as effective data augmentation for training better motion planner.},
	journaltitle = {{arXiv}:2101.06557 [cs]},
	author = {Suo, Simon and Regalado, Sebastian and Casas, Sergio and Urtasun, Raquel},
	urldate = {2021-04-08},
	date = {2021-01-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2101.06557},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-04-08]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, {ToDo}, ⛔ No {DOI} found},
}

@article{neumeier_variational_2021,
	title = {Variational Autoencoder-Based Vehicle Trajectory Prediction with an Interpretable Latent Space},
	url = {http://arxiv.org/abs/2103.13726},
	abstract = {This paper introduces the Descriptive Variational Autoencoder ({DVAE}), an unsupervised and end-to-end trainable neural network for predicting vehicle trajectories that provides partial interpretability. The novel approach is based on the architecture and objective of common variational autoencoders. By introducing expert knowledge within the decoder part of the autoencoder, the encoder learns to extract latent parameters that provide a graspable meaning in human terms. Such an interpretable latent space enables the validation by expert defined rule sets. The evaluation of the {DVAE} is performed using the publicly available {highD} dataset for highway traffic scenarios. In comparison to a conventional variational autoencoder with equivalent complexity, the proposed model provides a similar prediction accuracy but with the great advantage of having an interpretable latent space. For crucial decision making and assessing trustworthiness of a prediction this property is highly desirable.},
	journaltitle = {{arXiv}:2103.13726 [cs]},
	author = {Neumeier, Marion and Tollkühn, Andreas and Berberich, Thomas and Botsch, Michael},
	urldate = {2021-04-19},
	date = {2021-03-25},
	eprinttype = {arxiv},
	eprint = {2103.13726},
	keywords = {Computer Science - Machine Learning, ⛔ No {DOI} found},
}

@article{liu_unsupervised_2017,
	title = {Unsupervised image-to-image translation networks},
	volume = {2017-Decem},
	issn = {10495258},
	abstract = {Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled {GANs}. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit.},
	pages = {701--709},
	issue = {Nips},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Liu, Ming Yu and Breuel, Thomas and Kautz, Jan},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1703.00848},
	keywords = {⛔ No {DOI} found},
}

@article{yang_using_2019,
	title = {Using Simulation for 3D Perception and Navigation Quick Facts about Baidu},
	author = {Yang, Ruigang},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{chou_using_2018,
	title = {Using control synthesis to generate corner cases: A case study on autonomous driving},
	volume = {37},
	issn = {23318422},
	abstract = {This paper employs correct-by-construction control synthesis, in particular controlled invariant set computations, for falsification. Our hypothesis is that if it is possible to compute a “large enough" controlled invariant set either for the actual system model or some simplification of the system model, interesting corner cases for other control designs can be generated by sampling initial conditions from the boundary of this controlled invariant set. Moreover, if falsifying trajectories for a given control design can be found through such sampling, then the controlled invariant set can be used as a supervisor to ensure safe operation of the control design under consideration. In addition to interesting initial conditions, which are mostly related to safety violations in transients, we use solutions from a dual game, a reachability game for the safety specification, to find falsifying inputs. We also propose optimization-based heuristics for input generation for cases when the state is outside the winning set of the dual game. To demonstrate the proposed ideas, we consider case studies from basic autonomous driving functionality, in particular, adaptive cruise control and lane keeping. We show how the proposed technique can be used to find interesting falsifying trajectories for classical control designs like proportional controllers, proportional integral controllers and model predictive controllers, as well as an open source real-world autonomous driving package.},
	pages = {2906--2917},
	number = {11},
	journaltitle = {{arXiv}},
	author = {Chou, Glen and Sahin, Yunus E. and Yang, Liren and Rutledge, Kwesi J. and Nilsson, Petter and Ozay, Necmiye},
	date = {2018},
	note = {Publisher: {IEEE}},
	keywords = {⛔ No {DOI} found},
}

@article{gupta_unsupervised_2019,
	title = {Unsupervised automated event detection using an iterative clustering based segmentation approach},
	issn = {23318422},
	abstract = {A class of vision problems, less commonly studied, consists of detecting objects in imagery obtained from physics-based experiments. These objects can span in 4D (x, y, z, t) and are visible as disturbances (caused due to physical phenomena) in the image with background distribution being approximately uniform. Such objects, occasionally referred to as 'events', can be considered as high energy blobs in the image. Unlike the images analyzed in conventional vision problems, very limited features are associated with such events, and their shape, size and count can vary significantly. This poses a challenge on the use of pre-trained models obtained from supervised approaches. In this paper, we propose an unsupervised approach involving iterative clustering based segmentation ({ICS}) which can detect target objects (events) in real-time. In this approach, a test image is analyzed over several cycles, and one event is identified per cycle. Each cycle consists of the following steps: (1) image segmentation using a modified k-means clustering method, (2) elimination of empty (with no events) segments based on statistical analysis of each segment, (3) merging segments that overlap (correspond to same event), and (4) selecting the strongest event. These four steps are repeated until all the events have been identified. The {ICS} approach consists of a few hyper-parameters that have been chosen based on statistical study performed over a set of test images. The applicability of {ICS} method is demonstrated on several 2D and 3D test examples.},
	pages = {1--14},
	journaltitle = {{arXiv}},
	author = {Gupta, Deepak K. and Shrivastava, Rohit K. and Phadke, Suhas and Goudswaard, Jeroen},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1901.07222},
	keywords = {⛔ No {DOI} found},
}

@article{farooq_virtual_2018,
	title = {Virtual Immersive reality for stated preference travel behaviour experiments: A case study of autonomous Vehicles on Urban Roads},
	issn = {23318422},
	abstract = {Stated preference experiments have been known to suffer from the lack of realism. This issue is particularly visible when the scenario doesn't have a well understood prior reference e.g. in case of the autonomous vehicles related scenarios. We present Virtual Immersive Reality Environment ({VIRE}) that is capable of developing highly realistic, immersive, and interactive choice scenario. We demonstrate the use of {VIRE} in the pedestrian preferences related to autonomous vehicles and associated infrastructure changes on urban streets of Montreál. The results are compared with predominantly used approaches i.e. text-only and visual aid. We show that {VIRE} results in better understanding of the scenario and consistent results.},
	journaltitle = {{arXiv}},
	author = {Farooq, Bilal and Cherchi, Elisabetta and Sobhani, Anae},
	date = {2018},
	keywords = {Augmented Reality, Autonomous Vehicle, Behavioural Analysis, Pedestrian Behaviour, Stated Preference Experiment, Virtual Reality, ⛔ No {DOI} found},
}

@article{norden_efficient_2019,
	title = {Efficient black-box assessment of autonomous vehicle safety},
	abstract = {While autonomous vehicle ({AV}) technology has shown substantial progress, we still lack tools for rigorous and scalable testing. Real-world testing, the de-facto evaluation method, is dangerous to the public. Moreover, due to the rare nature of failures, billions of miles of driving are needed to statistically validate performance claims. Thus, the industry has largely turned to simulation to evaluate {AV} systems. However, having a simulation stack alone is not a solution. A simulation testing framework needs to prioritize which scenarios to run, learn how the chosen scenarios provide coverage of failure modes, and rank failure scenarios in order of importance. We implement a simulation testing framework that evaluates an entire modern {AV} system as a black box. This framework estimates the probability of accidents under a base distribution governing standard traffic behavior. In order to accelerate rare-event probability evaluation, we efficiently learn to identify and rank failure scenarios via adaptive importance-sampling methods. Using this framework, we conduct the first independent evaluation of a full-stack commercial {AV} system, Comma {AI}’s {OpenPilot}.},
	journaltitle = {{arXiv}},
	author = {Norden, Justin and O’Kelly, Matthew and Sinha, Aman},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1912.03618},
	keywords = {⛔ No {DOI} found},
}

@article{kaelbling_domain_2011,
	title = {Domain and Plan Representation for Task and Motion Planning in Uncertain Domains},
	abstract = {As robots become more physically robust and capable of sophisticated sensing, navigation, and manipulation, we want them to carry out increasingly complex tasks. A robot that helps in a household must plan over the scale of hours or days, considering abstract features such as the desires of the occupants of the house, as well as detailed models that support locating and getting objects, whether ingredients and cooking tools for preparing a meal or finding medicines for an elderly patient. The complexity of such tasks derives from very long time horizons, large numbers of objects to be considered and manipulated, and fundamental uncertainty about properties and locations of those objects. This paper describes a tightly integrated approach, weav- ing together perception, estimation, geometric reasoning, symbolic task planning, and control to generate behavior in real robots that robustly achieves tasks in complex, uncertain domains. It is founded on four main principles: • Planning explicitly in the space of the robot’s beliefs about the state of the world is necessary for intelligent information-gathering behavior; • Planning with highly simplified domain models is ef- ficient and can be made robust by using perception to detect execution failures and replanning online; • Combining logical and geometric reasoning enables effective planning in extremely large state spaces; • Online hierarchical planning interleaved with execution},
	journaltitle = {{IROS} 2011 Workshop: Knowledge Representation for Autonomous Robots},
	author = {Kaelbling, Lp and Lozano-Pérez, T},
	date = {2011},
	keywords = {⛔ No {DOI} found},
}

@article{scharei_knowledge_2020,
	title = {Knowledge Representations in Technical Systems A Taxonomy},
	issn = {23318422},
	abstract = {The recent usage of technical systems in human-centric environments leads to the question, how to teach technical systems, e.g., robots, to understand, learn, and perform tasks desired by the human. Therefore, an accurate representation of knowledge is essential for the system to work as expected. This article mainly gives insight into different knowledge representation techniques and their categorization into various problem domains in artificial intelligence. Additionally, applications of presented knowledge representations are introduced in everyday robotics tasks. By means of the provided taxonomy, the search for a proper knowledge representation technique regarding a specific problem should be facilitated.},
	journaltitle = {{arXiv}},
	author = {Scharei, Kristina and Heidecker, Florian and Bieshaar, Maarten},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2001.04835},
	keywords = {⛔ No {DOI} found},
}

@article{kleijn_wavenet_2017,
	title = {Wavenet based low rate speech coding},
	url = {http://arxiv.org/abs/1712.01120},
	abstract = {Traditional parametric coding of speech facilitates low rate but provides poor reconstruction quality because of the inadequacy of the model used. We describe how a {WaveNet} generative speech model can be used to generate high quality speech from the bit stream of a standard parametric coder operating at 2.4 kb/s. We compare this parametric coder with a waveform coder based on the same generative model and show that approximating the signal waveform incurs a large rate penalty. Our experiments conﬁrm the high performance of the {WaveNet} based coder and show that the speech produced by the system is able to additionally perform implicit bandwidth extension and does not signiﬁcantly impair recognition of the original speaker for the human listener, even when that speaker has not been used during the training of the generative model.},
	journaltitle = {{arXiv}:1712.01120 [cs, eess]},
	author = {Kleijn, W. Bastiaan and Lim, Felicia S. C. and Luebs, Alejandro and Skoglund, Jan and Stimberg, Florian and Wang, Quan and Walters, Thomas C.},
	urldate = {2021-04-28},
	date = {2017-12-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1712.01120},
	note = {50 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing, ⛔ No {DOI} found},
}

@article{li_deep_2021,
	title = {Deep Unsupervised Anomaly Detection},
	pages = {3636--3645},
	journaltitle = {Proceedings of the {IEEE}/{CVF} Winter Conference on Applications of Computer Vision},
	author = {Li, Tangqing and Wang, Zheng and Liu, Siying and Lin, Wen-Yan},
	date = {2021},
	keywords = {⛔ No {DOI} found},
}

@article{tian_unsupervised_2020,
	title = {Unsupervised object detection with {LiDAR} clues},
	issn = {23318422},
	abstract = {Despite the importance of unsupervised object detection, to the best of our knowledge, there is no previous work addressing this problem. One main issue, widely known to the community, is that object boundaries derived only from 2D image appearance are ambiguous and unreliable. To address this, we exploit {LiDAR} clues to aid unsupervised object detection. By exploiting the 3D scene structure, the issue of localization can be considerably mitigated. We further identify another major issue, seldom noticed by the community, that the long-tailed and open-ended (sub)category distribution should be accommodated. In this paper, we present the first practical method for unsupervised object detection with the aid of {LiDAR} clues. In our approach, candidate object segments based on 3D point clouds are firstly generated. Then, an iterative segment labeling process is conducted to assign segment labels and to train a segment labeling network, which is based on features from both 2D images and 3D point clouds. The labeling process is carefully designed so as to mitigate the issue of long-tailed and open-ended distribution. The final segment labels are set as pseudo annotations for object detection network training. Extensive experiments on the large-scale Waymo Open dataset suggest that the derived unsupervised object detection method achieves reasonable accuracy compared with that of strong supervision within the {LiDAR} visible range. Code shall be released.},
	journaltitle = {{arXiv}},
	author = {Tian, Hao and Chen, Yuntao and Dai, Jifeng and Zhang, Zhaoxiang and Zhu, Xizhou},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2011.12953},
	keywords = {⛔ No {DOI} found},
}

@article{mallya_world-consistent_2020,
	title = {World-Consistent Video-to-Video Synthesis},
	author = {Mallya, Arun and Wang, Ting-chun and Sapra, Karan and Liu, Ming-yu},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2007.08509v1},
	keywords = {gan, neural rendering, video synthesis, ⛔ No {DOI} found},
}

@article{kursuncu_knowledge_2020,
	title = {Knowledge Infused Learning (K-{IL}): Towards Deep Incorporation of Knowledge in Deep Learning},
	url = {http://arxiv.org/abs/1912.00512},
	shorttitle = {Knowledge Infused Learning (K-{IL})},
	abstract = {Learning the underlying patterns in data goes beyond instance-based generalization to external knowledge represented in structured graphs or networks. Deep learning that primarily constitutes neural computing stream in {AI} has shown signiﬁcant advances in probabilistically learning latent patterns using a multi-layered network of computational nodes (i.e., neurons/hidden units). Structured knowledge that underlies symbolic computing approaches and often supports reasoning, has also seen signiﬁcant growth in recent years, in the form of broad-based (e.g., {DBPedia}, Yago) and domain, industry or application speciﬁc knowledge graphs. A common substrate with careful integration of the two will raise opportunities to develop neuro-symbolic learning approaches for {AI}, where conceptual and probabilistic representations are combined. As the incorporation of external knowledge will aid in supervising the learning of features for the model, deep infusion of representational knowledge from knowledge graphs within hidden layers will further enhance the learning process. Although much work remains, we believe that knowledge graphs will play an increasing role in developing hybrid neuro-symbolic intelligent systems (bottomup deep learning with top-down symbolic computing) as well as in building explainable {AI} systems for which knowledge graphs will provide scaffolding for punctuating neural computing. In this position paper, we describe our motivation for such a neuro-symbolic approach and framework that combines knowledge graph and neural networks.},
	journaltitle = {{arXiv}:1912.00512 [cs]},
	author = {Kursuncu, Ugur and Gaur, Manas and Sheth, Amit},
	urldate = {2021-03-25},
	date = {2020-02-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1912.00512},
	note = {17 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, ⛔ No {DOI} found},
}

@article{wang_video--video_2018,
	title = {Video-to-video synthesis},
	volume = {2018-Decem},
	issn = {10495258},
	abstract = {We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our method to future video prediction, outperforming several competing systems. Code, models, and more results are available at our website.},
	pages = {1144--1156},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Wang, Ting Chun and Liu, Ming Yu and Zhu, Jun Yan and Liu, Guilin and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1808.06601},
	keywords = {⛔ No {DOI} found},
}

@article{wang_frustratingly_2020,
	title = {Frustratingly Simple Few-Shot Object Detection},
	url = {http://arxiv.org/abs/2003.06957},
	abstract = {Detecting rare objects from a few examples is an emerging problem. Prior works show meta-learning is a promising approach. But, fine-tuning techniques have drawn scant attention. We find that fine-tuning only the last layer of existing detectors on rare classes is crucial to the few-shot object detection task. Such a simple approach outperforms the meta-learning methods by roughly 2{\textasciitilde}20 points on current benchmarks and sometimes even doubles the accuracy of the prior methods. However, the high variance in the few samples often leads to the unreliability of existing benchmarks. We revise the evaluation protocols by sampling multiple groups of training examples to obtain stable comparisons and build new benchmarks based on three datasets: {PASCAL} {VOC}, {COCO} and {LVIS}. Again, our fine-tuning approach establishes a new state of the art on the revised benchmarks. The code as well as the pretrained models are available at https://github.com/ucbdrive/few-shot-object-detection.},
	journaltitle = {{arXiv}:2003.06957 [cs]},
	author = {Wang, Xin and Huang, Thomas E. and Darrell, Trevor and Gonzalez, Joseph E. and Yu, Fisher},
	urldate = {2021-03-25},
	date = {2020-03-15},
	eprinttype = {arxiv},
	eprint = {2003.06957},
	note = {28 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No {DOI} found},
}

@article{wang_variational_2021,
	title = {Variational Prototype Inference for Few-Shot Semantic Segmentation},
	abstract = {In this paper, we propose variational prototype inference to address few-shot semantic segmentation in a probabilis-tic framework. A probabilistic latent variable model infers the distribution of the prototype that is treated as the latent variable. We formulate the optimization as a variational inference problem, which is established with an amortized inference network based on an auto-encoder architecture. The probabilistic modeling of the prototype enhances its generalization ability to handle the inherent uncertainty caused by limited data and the huge intra-class variations of objects. Moreover, it offers a principled way to incorporate the prototype extracted from support images into the prediction of the segmentation maps for query images. We conduct extensive experimental evaluations on three benchmark datasets. Ablation studies show the effectiveness of variational prototype inference for few-shot semantic seg-mentation by probabilistic modeling. On all three benchmarks , our proposal achieves high segmentation accuracy and surpasses previous methods by considerable margins.},
	pages = {525--534},
	journaltitle = {Proceedings of the {IEEE}/{CVF} Winter Conference on Applications of Computer Vision ({WACV})},
	author = {Wang, Haochen and Yang, Yandan and Cao, Xianbin and Zhen, Xiantong and Snoek, Cees and Shao, Ling},
	date = {2021},
	keywords = {⛔ No {DOI} found},
}

@article{mittal_just_2019,
	title = {Just go with the flow: Self-supervised scene flow estimation},
	issn = {23318422},
	abstract = {When interacting with highly dynamic environments, scene flow allows autonomous systems to reason about the non-rigid motion of multiple independent objects. This is of particular interest in the field of autonomous driving, in which many cars, people, bicycles, and other objects need to be accurately tracked. Current state of the art methods require annotated scene flow data from autonomous driving scenes to train scene flow networks with supervised learning. As an alternative, we present a method of training scene flow that uses two self-supervised losses, based on nearest neighbors and cycle consistency. These self-supervised losses allow us to train our method on large unlabeled autonomous driving datasets; the resulting method matches current state-of-the-art supervised performance using no real world annotations and exceeds state-of-the-art performance when combining our self-supervised approach with supervised learning on a smaller labeled dataset.},
	pages = {11177--11185},
	journaltitle = {{arXiv}},
	author = {Mittal, Himangi and Okorn, Brian and Held, David},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{oruganti_image_2016,
	title = {Image Description using Deep Neural Networks},
	author = {Oruganti, Ram Manohar},
	date = {2016},
	keywords = {⛔ No {DOI} found},
}

@article{koopman_how_2019,
	title = {How many operational design domains, objects, and events?},
	volume = {2301},
	issn = {16130073},
	abstract = {A first step toward validating an autonomous vehicle is deciding what aspects of the system need to be validated. This paper lists factors we have found to be relevant in the areas of operational design domain, object and event detection and response, vehicle maneuvers, and fault management. While any such list is unlikely to be complete, our contribution can form a starting point for a publicly available master list of considerations to ensure that autonomous vehicle validation efforts do not contain crucial gaps due to missing known issues.},
	pages = {1--4},
	journaltitle = {{CEUR} Workshop Proceedings},
	author = {Koopman, Philip and Fratrik, Frank},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{congress_issues_2020,
	title = {Issues in Autonomous Vehicle Testing and Deployment},
	url = {https://crsreports.congress.gov},
	abstract = {Congressional Research Service https://crsreports.congress.gov R45985},
	journaltitle = {Congressional Research Service},
	author = {Congress, {US}},
	date = {2020},
	keywords = {⛔ No {DOI} found},
}
@article{zhao_online_2007,
	title = {Online Rare Events Detection},
	pages = {1114--1121},
	journaltitle = {Evaluation},
	author = {Zhao, Jun Hua and Li, Xue and Dong, Zhao Yang},
	date = {2007},
	keywords = {⛔ No {DOI} found},
}

@article{ki_delta_learning_ki-delta_2020,
	title = {{KI}-Delta Learning Licenses of datasets in Full Project Proposal},
	pages = {3},
	author = {{KI} Delta Learning},
	date = {2020},
	keywords = {⛔ No {DOI} found},
}

@article{moriarty_learning_1998,
	title = {Learning cooperative lane selection strategies for highways},
	abstract = {This paper presents a novel approach to traffic management by coordinating driver behaviors. Current traffic management systems do not consider lane organization of the cars and only affect traffic flows by controlling traffic signals or ramp meters. However, drivers should be able to increase traffic throughput and more consistently maintain desired speeds by selecting lanes intelligently. We pose the problem of intelligent lane selection as a challenging and potentially rewarding problem for artificial intelligence, and we propose a methodology that uses supervised and reinforcement learning to form distributed control strategies. Initial results are promising and demonstrate that intelligent lane selection can better approximate desired speeds and reduce the total number of lane changes.},
	pages = {684--691},
	journaltitle = {Proceedings of the National Conference on Artificial Intelligence},
	author = {Moriarty, David E. and Langley, Pat},
	date = {1998},
	keywords = {⛔ No {DOI} found},
}

@article{mohammadi_imagevideo_2021,
	title = {Image/Video Deep Anomaly Detection: A Survey},
	url = {http://arxiv.org/abs/2103.01739},
	abstract = {The considerable significance of Anomaly Detection ({AD}) problem has recently drawn the attention of many researchers. Consequently, the number of proposed methods in this research field has been increased steadily. {AD} strongly correlates with the important computer vision and image processing tasks such as image/video anomaly, irregularity and sudden event detection. More recently, Deep Neural Networks ({DNNs}) offer a high performance set of solutions, but at the expense of a heavy computational cost. However, there is a noticeable gap between the previously proposed methods and an applicable real-word approach. Regarding the raised concerns about {AD} as an ongoing challenging problem, notably in images and videos, the time has come to argue over the pitfalls and prospects of methods have attempted to deal with visual {AD} tasks. Hereupon, in this survey we intend to conduct an in-depth investigation into the images/videos deep learning based {AD} methods. We also discuss current challenges and future research directions thoroughly.},
	author = {Mohammadi, Bahram and Fathy, Mahmood and Sabokrou, Mohammad},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.01739},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{shen_learning_2020,
	title = {Learning the distribution: A unified distillation paradigm for fast uncertainty estimation in computer vision},
	issn = {23318422},
	abstract = {Calibrated estimates of uncertainty are critical for many real-world computer vision applications of deep learning. While there are several widely-used uncertainty estimation methods, dropout inference [11] stands out for its simplicity and efficacy. This technique, however, requires multiple forward passes through the network during inference and therefore can be too resource-intensive to be deployed in real-time applications. To tackle this issue, we propose a unified distillation paradigm for learning the conditional predictive distribution of a pre-trained dropout model for fast uncertainty estimation of both aleatoric and epistemic uncertainty at the same time. We empirically test the effectiveness of the proposed method on both semantic segmentation and depth estimation tasks, and observe that the student model can well approximate the probability distribution generated by the teacher model, i.e the pre-trained dropout model. In addition to a significant boost in speed, we demonstrate the quality of uncertainty estimates and the overall predictive performance can also be improved with the proposed method.},
	journaltitle = {{arXiv}},
	author = {Shen, Yichen and Zhang, Zhilu and Sabuncu, Mert R. and Sun, Lin},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2007.15857},
	keywords = {⛔ No {DOI} found},
}

@article{shalev-shwartz_formal_2017,
	title = {On a Formal Model of Safe and Scalable Self-driving Cars},
	issn = {23318422},
	abstract = {In recent years, car makers and tech companies have been racing towards self driving cars. It seems that the main parameter in this race is who will have the first car on the road. The goal of this paper is to add to the equation two additional crucial parameters. The first is standardization of safety assurance — what are the minimal requirements that every self-driving car must satisfy, and how can we verify these requirements. The second parameter is scalability — engineering solutions that lead to unleashed costs will not scale to millions of cars, which will push interest in this field into a niche academic corner, and drive the entire field into a “winter of autonomous driving”. In the first part of the paper we propose a white-box, interpretable, mathematical model for safety assurance, which we call Responsibility-Sensitive Safety ({RSS}). In the second part we describe a design of a system that adheres to our safety assurance requirements and is scalable to millions of cars.},
	pages = {1--37},
	journaltitle = {{arXiv}},
	author = {Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1708.06374},
	keywords = {⛔ No {DOI} found},
}

@article{wang_learning_2020,
	title = {Learning hierarchical behavior and motion planning for autonomous driving},
	url = {http://arxiv.org/abs/2005.03863},
	abstract = {Learning-based driving solution, a new branch for autonomous driving, is expected to simplify the modeling of driving by learning the underlying mechanisms from data. To improve the tactical decision-making for learning-based driving solution, we introduce hierarchical behavior and motion planning ({HBMP}) to explicitly model the behavior in learning-based solution. Due to the coupled action space of behavior and motion, it is challenging to solve {HBMP} problem using reinforcement learning ({RL}) for long-horizon driving tasks. We transform {HBMP} problem by integrating a classical sampling-based motion planner, of which the optimal cost is regarded as the rewards for high-level behavior learning. As a result, this formulation reduces action space and diversifies the rewards without losing the optimality of {HBMP}. In addition, we propose a sharable representation for input sensory data across simulation platforms and real-world environment, so that models trained in a fast event-based simulator, {SUMO}, can be used to initialize and accelerate the {RL} training in a dynamics based simulator, {CARLA}. Experimental results demonstrate the effectiveness of the method. Besides, the model is successfully transferred to the real-world, validating the generalization capability.},
	journaltitle = {{arXiv}:2005.03863 [cs]},
	author = {Wang, Jingke and Wang, Yue and Zhang, Dongkun and Yang, Yezhou and Xiong, Rong},
	urldate = {2021-04-14},
	date = {2020-05-08},
	eprinttype = {arxiv},
	eprint = {2005.03863},
	note = {2 citations (Semantic Scholar/{arXiv}) [2021-04-14]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, {ToDo}, ⛔ No {DOI} found},
}

@article{xie_learning_2021,
	title = {Learning Class-Agnostic Pseudo Mask Generation for Box-Supervised Semantic Segmentation},
	url = {http://arxiv.org/abs/2103.05463},
	abstract = {Recently, several weakly supervised learning methods have been devoted to utilize bounding box supervision for training deep semantic segmentation models. Most existing methods usually leverage the generic proposal generators ({\textbackslash}eg, dense {CRF} and {MCG}) to produce enhanced segmentation masks for further training segmentation models. These proposal generators, however, are generic and not specifically designed for box-supervised semantic segmentation, thereby leaving some leeway for improving segmentation performance. In this paper, we aim at seeking for a more accurate learning-based class-agnostic pseudo mask generator tailored to box-supervised semantic segmentation. To this end, we resort to a pixel-level annotated auxiliary dataset where the class labels are non-overlapped with those of the box-annotated dataset. For learning pseudo mask generator from the auxiliary dataset, we present a bi-level optimization formulation. In particular, the lower subproblem is used to learn box-supervised semantic segmentation, while the upper subproblem is used to learn an optimal class-agnostic pseudo mask generator. The learned pseudo segmentation mask generator can then be deployed to the box-annotated dataset for improving weakly supervised semantic segmentation. Experiments on {PASCAL} {VOC} 2012 dataset show that the learned pseudo mask generator is effective in boosting segmentation performance, and our method can further close the performance gap between box-supervised and fully-supervised models. Our code will be made publicly available at https://github.com/Vious/{LPG}\_BBox\_Segmentation .},
	author = {Xie, Chaohao and Ren, Dongwei and Wang, Lei and Hu, Qinghua and Lin, Liang and Zuo, Wangmeng},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.05463},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{lim_radar_2019,
	title = {Radar and Camera Early Fusion for Vehicle Detection in Advanced Driver Assistance Systems},
	abstract = {Perception module is at the heart of modern Advanced Driver Assistance Systems ({ADAS}). To improve the quality and robustness of this module, especially in the presence of environmental noises such as varying lighting and weather conditions, fusion of sensors (mainly camera and {LiDAR}) has been the center of attention in the recent studies. In this paper, we focus on a relatively unexplored area which addresses the early fusion of camera and radar sensors. We feed a minimally processed radar signal to our deep learning architecture along with its corresponding camera frame to enhance the accuracy and robustness of our perception module. Our evaluation, performed on real world data, suggests that the complementary nature of radar and camera signals can be leveraged to reduce the lateral error by 15\% when applied to object detection.},
	pages = {11},
	author = {Lim, Teck-Yian and Ansari, Amin},
	date = {2019},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{liu_learning_2017,
	title = {Learning End-to-end Multimodal Sensor Policies for Autonomous Navigation},
	abstract = {Sensor fusion is indispensable to improve accuracy and robustness in an autonomous navigation setting. However, in the space of end-to-end sensorimotor control, this multimodal outlook has received limited attention. In this work, we propose a novel stochastic regularization technique, called Sensor Dropout, to robustify multimodal sensor policy learning outcomes. We also introduce an auxiliary loss on policy network along with the standard {DRL} loss in order to reduce variance in actions of the multimodal sensor policy. Through extensive empirical testing, we demonstrate that our proposed policy can 1) operate with minimal performance drop in noisy environments and 2) remain functional even in the face of a sensor subset failure. Finally, through the visualization of gradients, we show that the learned policies are conditioned on the same latent input distribution despite having multiple and diverse observations spaces - a hallmark of true sensorfusion. This efﬁcacy of a multimodal sensor policy is shown through simulations on {TORCS}, a popular open-source racing car game. A demo video can be seen here: https://youtu.be/{HC}3TcJjXf3Q.},
	pages = {13},
	author = {Liu, Guan-Horng and Siravuru, Avinash and Prabhakar, Sai and Veloso, Manuela and Kantor, George},
	date = {2017},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{deng_latent_2016,
	title = {Latent Space Model for Road Networks to Predict Time-Varying Traffic},
	volume = {1602},
	url = {http://adsabs.harvard.edu/abs/2016arXiv160204301D},
	abstract = {Real-time traffic prediction from high-fidelity spatiotemporal traffic sensor datasets is an important problem for intelligent transportation systems and sustainability. However, it is challenging due to the complex topological dependencies and high dynamics associated with changing road conditions. In this paper, we propose a Latent Space Model for Road Networks ({LSM}-{RN}) to address these challenges. In particular, given a series of road network snapshots, we learn the attributes of vertices in latent spaces which capture both topological and temporal properties. As these latent attributes are time-dependent, they can estimate how traffic patterns form and evolve. In addition, we present an incremental online algorithm which sequentially and adaptively learn the latent attributes from the temporal graph changes. Our framework enables real-time traffic prediction by 1) exploiting real-time sensor readings to adjust/update the existing latent spaces, and 2) training as data arrives and making predictions on-the-fly with given data. By conducting extensive experiments with a large volume of real-world traffic sensor data, we demonstrate the utility superiority of our framework for real-time traffic prediction on large road networks over competitors as well as a baseline graph-based {LSM}.},
	pages = {arXiv:1602.04301},
	journaltitle = {{arXiv} e-prints},
	shortjournal = {{arXiv} e-prints},
	author = {Deng, Dingxiong and Shahabi, Cyrus and Demiryurek, Ugur and Zhu, Linhong and Yu, Rose and Liu, Yan},
	urldate = {2021-04-18},
	date = {2016-02-01},
	keywords = {Computer Science - Databases, Computer Science - Social and Information Networks, ⛔ No {DOI} found},
}

@article{caine_pseudo-labeling_2021,
	title = {Pseudo-labeling for Scalable 3D Object Detection},
	url = {http://arxiv.org/abs/2103.02093},
	abstract = {To safely deploy autonomous vehicles, onboard perception systems must work reliably at high accuracy across a diverse set of environments and geographies. One of the most common techniques to improve the efficacy of such systems in new domains involves collecting large labeled datasets, but such datasets can be extremely costly to obtain, especially if each new deployment geography requires additional data with expensive 3D bounding box annotations. We demonstrate that pseudo-labeling for 3D object detection is an effective way to exploit less expensive and more widely available unlabeled data, and can lead to performance gains across various architectures, data augmentation strategies, and sizes of the labeled dataset. Overall, we show that better teacher models lead to better student models, and that we can distill expensive teachers into efficient, simple students. Specifically, we demonstrate that pseudo-label-trained student models can outperform supervised models trained on 3-10 times the amount of labeled examples. Using {PointPillars} [24], a two-year-old architecture, as our student model, we are able to achieve state of the art accuracy simply by leveraging large quantities of pseudo-labeled data. Lastly, we show that these student models generalize better than supervised models to a new domain in which we only have unlabeled data, making pseudo-label training an effective form of unsupervised domain adaptation.},
	author = {Caine, Benjamin and Roelofs, Rebecca and Vasudevan, Vijay and Ngiam, Jiquan and Chai, Yuning and Chen, Zhifeng and Shlens, Jonathon},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.02093},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {★, ⛔ No {DOI} found},
}

@article{fulger_daniel_scalas_enrico_germano_random_2013,
	title = {{RANDOM} {NUMBERS} {FROM} {THE} {TAILS} {OF} {PROBABILITY} {DISTRIBUTIONS} {USING} {THE} {TRANSFORMATION} {METHOD}},
	author = {Fulger, Daniel; Scalas, Enrico; Germano, Guido},
	date = {2013},
	keywords = {and phrases, bution, fractional diffusion, mittag-leffler distribution, random number generation, α -stable distri-, ⛔ No {DOI} found},
}

@article{chandramohan_thesis_machine_2018,
	title = {Machine Learning for Cooperative Automated Driving},
	author = {Chandramohan Thesis, Aashik and Meijerink, Bernd},
	date = {2018},
	keywords = {⛔ No {DOI} found},
}

@article{hajri_real_2019,
	title = {Real Time Lidar and Radar High-Level Fusion for Obstacle Detection and Tracking with evaluation on a ground truth},
	url = {http://arxiv.org/abs/1807.11264},
	abstract = {Both Lidars and Radars are sensors for obstacle detection. While Lidars are very accurate on obstacles positions and less accurate on their velocities, Radars are more precise on obstacles velocities and less precise on their positions. Sensor fusion between Lidar and Radar aims at improving obstacle detection using advantages of the two sensors. The present paper proposes a real-time Lidar/Radar data fusion algorithm for obstacle detection and tracking based on the global nearest neighbour standard ﬁlter ({GNN}). This algorithm is implemented and embedded in an automative vehicle as a component generated by a real-time multisensor software. The beneﬁts of data fusion comparing with the use of a single sensor are illustrated through several tracking scenarios (on a highway and on a bend) and using real-time kinematic sensors mounted on the ego and tracked vehicles as a ground truth.},
	journaltitle = {{arXiv}:1807.11264 [cs]},
	author = {Hajri, Hatem and Rahal, Mohamed-Cherif},
	urldate = {2021-03-29},
	date = {2019-07-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.11264},
	note = {8 citations (Semantic Scholar/{arXiv}) [2021-03-29]},
	keywords = {Computer Science - Performance, Computer Science - Robotics, ⛔ No {DOI} found},
}

@article{dhamija_self-supervised_2021,
	title = {Self-Supervised Features Improve Open-World Learning},
	url = {http://arxiv.org/abs/2102.07848},
	abstract = {This is a position paper that addresses the problem of Open-World learning while proposing for the underlying feature representation to be learnt using self-supervision. We also present an unifying open-world framework combining three individual research dimensions which have been explored independently {\textbackslash}ie Incremental Learning, Out-of-Distribution detection and Open-World learning. We observe that the supervised feature representations are limited and degenerate for the Open-World setting and unsupervised feature representation is native to each of these three problem domains. Under an unsupervised feature representation, we categorize the problem of detecting unknowns as either Out-of-Label-space or Out-of-Distribution detection, depending on the data used during system training versus system testing. The incremental learning component of our pipeline is a zero-exemplar online model which performs comparatively against state-of-the-art on {ImageNet}-100 protocol and does not require any back-propagation or retraining of the underlying deep-network. It further outperforms the current state-of-the-art by simply using the same number of exemplars as its counterparts. To evaluate our approach for Open-World learning, we propose a new comprehensive protocol and evaluate its performance in both Out-of-Label and Out-of-Distribution settings for each incremental stage. We also demonstrate the adaptability of our approach by showing how it can work as a plug-in with any of the recently proposed self-supervised feature representation methods.},
	author = {Dhamija, Akshay Raj and Ahmad, Touqeer and Schwan, Jonathan and Jafarzadeh, Mohsen and Li, Chunchun and Boult, Terrance E.},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.07848},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{bajcsy_mathematical_2020,
	title = {Mathematical Challenges and Opportunities for Autonomous Vehicles},
	pages = {2020},
	author = {Bajcsy, Ruzena and Goatin, Paola and Kosecka, jana},
	date = {2020},
	keywords = {⛔ No {DOI} found},
}

@article{li_paralleleye_2017,
	title = {The paralleleye dataset: Constructing large-scale artificial scenes for traffic vision research},
	issn = {23318422},
	abstract = {Video image datasets are playing an essential role in design and evaluation of traffic vision algorithms. Nevertheless, a longstanding inconvenience concerning image datasets is that manually collecting and annotating large-scale diversified datasets from real scenes is time-consuming and prone to error. For that virtual datasets have begun to function as a proxy of real datasets. In this paper, we propose to construct large-scale artificial scenes for traffic vision research and generate a new virtual dataset called “{ParallelEye}”. First of all, the street map data is used to build 3D scene model of Zhongguancun Area, Beijing. Then, the computer graphics, virtual reality, and rule modeling technologies are utilized to synthesize large-scale, realistic virtual urban traffic scenes, in which the fidelity and geography match the real world well. Furthermore, the Unity3D platform is used to render the artificial scenes and generate accurate ground-truth labels, e.g., semantic/instance segmentation, object bounding box, object tracking, optical flow, and depth. The environmental conditions in artificial scenes can be controlled completely. As a result, we present a viable implementation pipeline for constructing large-scale artificial scenes for traffic vision research. The experimental results demonstrate that this pipeline is able to generate photorealistic virtual datasets with low modeling time and high accuracy labeling.},
	journaltitle = {{arXiv}},
	author = {Li, Xuan and Wang, Kunfeng and Tian, Yonglin and Yan, Lan and Wang, Fei Yue},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1712.08394},
	keywords = {⛔ No {DOI} found},
}

@article{feng_look_2021,
	title = {Look, Evolve and Mold: Learning 3D Shape Manifold via Single-view Synthetic Data},
	url = {http://arxiv.org/abs/2103.04789},
	abstract = {With daily observation and prior knowledge, it is easy for us human to infer the stereo structure via a single view. However, to equip the deep models with such ability usually requires abundant supervision. It is promising that without the elaborated 3D annotation, we can simply profit from the synthetic data, where pairwise ground-truth is easy to access. Nevertheless, the domain gap is not neglectable considering the variant texture, shape and context. To overcome these difficulties, we propose a domain-adaptive network for single-view 3D reconstruction, dubbed {LEM}, to generalize towards the natural scenario by fulfilling several aspects: (1) Look: incorporating spatial structure from the single view to enhance the representation; (2) Evolve: leveraging the semantic information with unsupervised contrastive mapping recurring to the shape priors; (3) Mold: transforming into the desired stereo manifold with discernment and semantic knowledge. Extensive experiments on several benchmarks demonstrate the effectiveness and robustness of the proposed method, {LEM}, in learning the 3D shape manifold from the synthetic data via a single-view.},
	author = {Feng, Qianyu and Luo, Yawei and Luo, Keyang and Yang, Yi},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.04789},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{taherifard_prior_2021,
	title = {Prior Knowledge Input to Improve {LSTM} Auto-encoder-based Characterization of Vehicular Sensing Data},
	url = {http://arxiv.org/abs/2101.01259},
	abstract = {Precision in event characterization in connected vehicles has become increasingly important with the responsive connectivity that is available to the modern vehicles. Event characterization via vehicular sensors are utilized in safety and autonomous driving applications in vehicles. While characterization systems have been shown to be capable of predicting the risky driving patterns, precision of such systems still remains an open issue. The major issues against the driving event characterization systems need to be addressed in connected vehicle settings, which are the heavy imbalance and the event infrequency of the driving data and the existence of the time-series detection systems that are optimized for vehicular settings. To overcome the problems, we introduce the application of the prior-knowledge input method to the characterization systems. Furthermore, we propose a recurrent-based denoising auto-encoder network to populate the existing data for a more robust training process. The results of the conducted experiments show that the introduction of knowledge-based modelling enables the existing systems to reach significantly higher accuracy and F1-score levels. Ultimately, the combination of the two methods enables the proposed model to attain 14.7{\textbackslash}\% accuracy boost over the baseline by achieving an accuracy of 0.96.},
	author = {Taherifard, Nima and Simsek, Murat and Lascelles, Charles and Kantarci, Burak},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2101.01259},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{wang_seesaw_2020,
	title = {Seesaw loss for long-tailed instance segmentation},
	issn = {23318422},
	abstract = {This report presents the approach used in the submission of the {LVIS} Challenge 2020 of team {MMDet}. In the submission, we propose Seesaw Loss that dynamically rebalances the penalty to each category according to a relative ratio of cumulative training instances between different categories. Furthermore, we propose {HTC}-Lite, a light-weight version of Hybrid Task Cascade ({HTC}) which replaces the semantic segmentation branch by a global context encoder. Seesaw Loss improves the strong baseline by 6.9\% {AP} on {LVIS} v1 val split. With a single model, and without using external data and annotations except for standard {ImageNet}-1k classification dataset for backbone pre-training, our submission achieves 38.92\% {AP} on the test-dev split of the {LVIS} v1 benchmark.},
	journaltitle = {{arXiv}},
	author = {Wang, Jiaqi and Zhang, Wenwei and Zang, Yuhang and Cao, Yuhang and Pang, Jiangmiao and Gong, Tao and Chen, Kai and Liu, Ziwei and Loy, Chen Change and Lin, Dahua},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2008.10032},
	keywords = {Account: {MMDet}, Team: {MMDet}, ⛔ No {DOI} found},
}

@article{najm_pre-crash_2007,
	title = {Pre-Crash Scenario Typology for Crash Avoidance Research},
	abstract = {This report defines a new pre-crash scenario typology for crash avoidance research based on the 2004 General Estimates System ({GES}) crash database, which consists of pre-crash scenarios depicting vehicle movements and dynamics as well as the critical event immediately prior to a crash. This typology establishes a common vehicle safety research foundation for public and private organizations, which will allow researchers to determine which traffic safety issues should be of first priority to investigate and to develop concomitant crash avoidance systems. Its main objectives are to identify all common pre-crash scenarios of all police-reported crashes involving at least one light vehicle (i.e., passenger car, sports utility vehicle, van, minivan, and light pickup truck); quantify their severity in terms of frequency of occurrence, economic cost, and functional years lost; portray each scenario by crash contributing factors and circumstances in terms of the driving environment, driver, and vehicle; and provide nationally representative crash statistics that can be annually updated using national crash databases such as {GES}. This new typology includes 37 pre-crash scenarios accounting for approximately 5,942,000 police-reported light-vehicle crashes, an estimated economic cost of 120 billion dollars, and 2,767,000 functional years lost. These statistics do not incorporate data from non-police-reported crashes.},
	pages = {128},
	issue = {April},
	journaltitle = {Security},
	author = {Najm, Wassim G. and Smith, John D. and Yanagisawa, Mikio},
	date = {2007},
	note = {{ISBN}: {DOT} {HS} 810 767},
	keywords = {⛔ No {DOI} found},
}

@article{cheraghian_semantic-aware_2021,
	title = {Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning},
	url = {http://arxiv.org/abs/2103.04059},
	abstract = {Few-shot class incremental learning ({FSCIL}) portrays the problem of learning new concepts gradually, where only a few examples per concept are available to the learner. Due to the limited number of examples for training, the techniques developed for standard incremental learning cannot be applied verbatim to {FSCIL}. In this work, we introduce a distillation algorithm to address the problem of {FSCIL} and propose to make use of semantic information during training. To this end, we make use of word embeddings as semantic information which is cheap to obtain and which facilitate the distillation process. Furthermore, we propose a method based on an attention mechanism on multiple parallel embeddings of visual data to align visual and semantic vectors, which reduces issues related to catastrophic forgetting. Via experiments on {MiniImageNet}, {CUB}200, and {CIFAR}100 dataset, we establish new state-of-the-art results by outperforming existing approaches.},
	author = {Cheraghian, Ali and Rahman, Shafin and Fang, Pengfei and Roy, Soumava Kumar and Petersson, Lars and Harandi, Mehrtash},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.04059},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{corso_scalable_2020,
	title = {Scalable Autonomous Vehicle Safety Validation through Dynamic Programming and Scene Decomposition},
	abstract = {An open question in autonomous driving is how best to use simulation to validate the safety of autonomous vehicles. Existing techniques rely on simulated rollouts, which can be inefficient for finding rare failure events, while other techniques are designed to only discover a single failure. In this work, we present a new safety validation approach that attempts to estimate the distribution over failures of an autonomous policy using approximate dynamic programming. Knowledge of this distribution allows for the efficient discovery of many failure examples. To address the problem of scalability, we decompose complex driving scenarios into subproblems consisting of only the ego vehicle and one other vehicle. These subproblems can be solved with approximate dynamic programming and their solutions are recombined to approximate the solution to the full scenario. We apply our approach to a simple two-vehicle scenario to demonstrate the technique as well as a more complex five-vehicle scenario to demonstrate scalability. In both experiments, we observed an order of magnitude increase in the number of failures discovered compared to baseline approaches.},
	pages = {1--6},
	journaltitle = {{arXiv}},
	author = {Corso, Anthony and Lee, Ritchie and Kochenderfer, Mykel J.},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2004.06801},
	keywords = {⛔ No {DOI} found},
}

@article{pereira_when_2016,
	title = {When Backscatter Communication Meets Vehicular Networks: Boosting Crosswalk Awareness},
	volume = {4},
	abstract = {The research of safety applications in vehicular networks has been a popular research topic in an effort to reduce the number of road victims. Advances on vehicular communications are facilitating information sharing through real time communications, critical for the development of driving assistance systems. However, the communication by itself is not enough to reach the most desired target as we need to know which safety-related information should be disseminated. In this work, we bring passive sensors and backscatter communication to the vehicular network world. The idea is to increase the driver (or vehicle) awareness regarding the presence of pedestrians in a crosswalk. Passive sensors and backscatter communication technologies are used for the pedestrians’ detection phase, while the vehicular network is used during the dissemination of the detection information to surrounding vehicles. The proposed solution was validated through end-to-end experimentation, with real hardware and in a real crosswalk with real pedestrians and vehicles, demonstrating its applicability.},
	pages = {15},
	journaltitle = {{IEEE} Access},
	author = {Pereira, Felisberto and Correia, Ricardo and Luís, Miguel and Sargento, Susana and Jordão, Marina and Almeida, Luís and Carvalho, Nuno Borges},
	date = {2016},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{fernandez-ares_studying_2017,
	title = {Studying real traffic and mobility scenarios for a Smart City using a new nonitoring and tracking system},
	abstract = {This paper presents a novel mobility monitoring system and some of its applications to address problems that would be solved in a Smart City, such as the optimization of traﬃc ﬂows in terms of trip-time and security (Smart Traﬃc), and the improvement of security or energetic issues inside buildings. The system tracks the movement of people and vehicles monitoring the radioelectric space, catching the {WiFi} and Bluetooth signals emitted by personal (smartphones) or on-board (hands-free) devices. A study has been conducted in four diﬀerent real scenarios, i.e. with real data gathered by the system: two related with people’s mobility (a public building and a discotheque); and two focused in traﬃc tracking (urban and intercity roads). The analysis has consisted on the application of diﬀerent data mining techniques to extract useful knowledge, traﬃc forecasting methods to perform accurate predictions, and statistical analyses to model and validate the system reliability (comparting to other real data sources). The obtained results show the viability and utility of the system in all the cases, along with some of its multiple applications for solving diﬀerent issues in a city.},
	pages = {40},
	author = {Fernandez-Ares, A J and Mora, A M and Arenas, M G and Garcıa-Sanchez, P and Rivas, V and Castillo, P A and Merelo, J J},
	date = {2017},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{sharma_sketching_2021,
	title = {Sketching Curvature for Efficient Out-of-Distribution Detection for Deep Neural Networks},
	url = {http://arxiv.org/abs/2102.12567},
	abstract = {In order to safely deploy Deep Neural Networks ({DNNs}) within the perception pipelines of real-time decision making systems, there is a need for safeguards that can detect out-of-training-distribution ({OoD}) inputs both efficiently and accurately. Building on recent work leveraging the local curvature of {DNNs} to reason about epistemic uncertainty, we propose Sketching Curvature of {OoD} Detection ({SCOD}), an architecture-agnostic framework for equipping any trained {DNN} with a task-relevant epistemic uncertainty estimate. Offline, given a trained model and its training data, {SCOD} employs tools from matrix sketching to tractably compute a low-rank approximation of the Fisher information matrix, which characterizes which directions in the weight space are most influential on the predictions over the training data. Online, we estimate uncertainty by measuring how much perturbations orthogonal to these directions can alter predictions at a new test input. We apply {SCOD} to pre-trained networks of varying architectures on several tasks, ranging from regression to classification. We demonstrate that {SCOD} achieves comparable or better {OoD} detection performance with lower computational burden relative to existing baselines.},
	author = {Sharma, Apoorva and Azizan, Navid and Pavone, Marco},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.12567},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{neunert_continuous-discrete_2020,
	title = {Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics},
	url = {http://arxiv.org/abs/2001.00449},
	abstract = {Many real-world control problems involve both discrete decision variables – such as the choice of control modes, gear switching or digital outputs – as well as continuous decision variables – such as velocity setpoints, control gains or analogue outputs. However, when deﬁning the corresponding optimal control or reinforcement learning problem, it is commonly approximated with fully continuous or fully discrete action spaces. These simpliﬁcations aim at tailoring the problem to a particular algorithm or solver which may only support one type of action space. Alternatively, expert heuristics are used to remove discrete actions from an otherwise continuous space. In contrast, we propose to treat hybrid problems in their ‘native’ form by solving them with hybrid reinforcement learning, which optimizes for discrete and continuous actions simultaneously. In our experiments, we ﬁrst demonstrate that the proposed approach efﬁciently solves such natively hybrid reinforcement learning problems. We then show, both in simulation and on robotic hardware, the beneﬁts of removing possibly imperfect expert-designed heuristics. Lastly, hybrid reinforcement learning encourages us to rethink problem deﬁnitions. We propose reformulating control problems, e.g. by adding meta actions, to improve exploration or reduce mechanical wear and tear.},
	journaltitle = {{arXiv}:2001.00449 [cs, stat]},
	author = {Neunert, Michael and Abdolmaleki, Abbas and Wulfmeier, Markus and Lampe, Thomas and Springenberg, Jost Tobias and Hafner, Roland and Romano, Francesco and Buchli, Jonas and Heess, Nicolas and Riedmiller, Martin},
	urldate = {2021-04-21},
	date = {2020-01-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2001.00449},
	note = {7 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning, ⛔ No {DOI} found},
}

@article{nobis_deep_2020,
	title = {A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection},
	url = {http://arxiv.org/abs/2005.07431},
	abstract = {Object detection in camera images, using deep learning has been proven successfully in recent years. Rising detection rates and computationally efficient network structures are pushing this technique towards application in production vehicles. Nevertheless, the sensor quality of the camera is limited in severe weather conditions and through increased sensor noise in sparsely lit areas and at night. Our approach enhances current 2D object detection networks by fusing camera data and projected sparse radar data in the network layers. The proposed {CameraRadarFusionNet} ({CRF}-Net) automatically learns at which level the fusion of the sensor data is most beneficial for the detection result. Additionally, we introduce {BlackIn}, a training strategy inspired by Dropout, which focuses the learning on a specific sensor type. We show that the fusion network is able to outperform a state-of-the-art image-only network for two different datasets. The code for this research will be made available to the public at: https://github.com/{TUMFTM}/{CameraRadarFusionNet}.},
	journaltitle = {{arXiv}:2005.07431 [cs]},
	author = {Nobis, Felix and Geisslinger, Maximilian and Weber, Markus and Betz, Johannes and Lienkamp, Markus},
	urldate = {2021-04-19},
	date = {2020-05-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.07431},
	note = {30 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No {DOI} found},
}

@article{bansal_chauffeurnet_2018,
	title = {{ChauffeurNet}: Learning to Drive by Imitating the Best and Synthesizing the Worst},
	url = {http://arxiv.org/abs/1812.03079},
	shorttitle = {{ChauffeurNet}},
	abstract = {Our goal is to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. We find that standard behavior cloning is insufficient for handling complex driving scenarios, even when we leverage a perception system for preprocessing the input and a controller for executing the output on the car: 30 million examples are still not enough. We propose exposing the learner to synthesized data in the form of perturbations to the expert's driving, which creates interesting situations such as collisions and/or going off the road. Rather than purely imitating all data, we augment the imitation loss with additional losses that penalize undesirable events and encourage progress -- the perturbations then provide an important signal for these losses and lead to robustness of the learned model. We show that the {ChauffeurNet} model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of our proposed changes and show that the model is responding to the appropriate causal factors. Finally, we demonstrate the model driving a car in the real world.},
	journaltitle = {{arXiv}:1812.03079 [cs]},
	author = {Bansal, Mayank and Krizhevsky, Alex and Ogale, Abhijit},
	urldate = {2021-04-14},
	date = {2018-12-07},
	eprinttype = {arxiv},
	eprint = {1812.03079},
	note = {251 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, {ToDo}, ⛔ No {DOI} found},
}

@article{webb_waymos_2020,
	title = {Waymo’s safety methodologies and safety readiness determinations},
	issn = {23318422},
	abstract = {As the world’s most experienced developer of automated driving systems (“{ADSs}”), Waymo has extensive experience in developing and applying state-of-the-art safety methodologies.1 Waymo’s methodologies help implement Waymo’s forward-looking safety philosophy: Waymo will reduce traffic injuries and fatalities by driving safely and responsibly, and will carefully manage risk as we scale our operations. Waymo’s safety methodologies, which draw on well established engineering processes and address new safety challenges specific to Automated Vehicle (“{AV}”) technology, provide a firm foundation for safe deployment of our Level 4 {ADS}, which we also refer to as the Waymo Driver™.2 Waymo’s determination of its readiness to deploy its {AVs} safely in different settings rests on that firm foundation and on a thorough analysis of risks specific to a particular Operational Design Domain (“{ODD}”). Waymo’s Current Operations. Waymo currently operates our {AVs} primarily in California and Arizona, with additional testing in several other states including Michigan,Texas, Florida and Washington. In Metro Phoenix, our fleet of Chrysler Pacifica {AVs} has been transporting passengers in various types of self-driving (i.e., {ADS}-operated) service since 2017, including a driverless service. To date, Waymo has compiled over 20 million self-driving miles on public roads operating in over 25 cities, including 74,000 driverless miles. Approaches to Assessing Automated Vehicle Safety. Various private and governmental organizations have proposed a wide range of methodologies for measuring or demonstrating {AV} safety. Several common themes emerge from these various approaches, including: simulation-based testing is an extremely important part of development and safety validation of an {ADS}; test scenario selection should be tailored for risks likely to occur in a given {ODD}; and, as in all transportation, transportation in an {ADS}-equipped vehicle involves some level of risk, and a guiding principle should be to avoid risks that are unreasonable. Waymo’s Safety Methodologies. Waymo’s safety methodologies focus on the development, qualification, deployment and sustained field operation of a unique product: a Level 4 {ADS} that can perform the entire Dynamic Driving Task (“{DDT}”), with no human driver present, in both normal traffic conditions and very challenging scenarios that we have reason to expect may occur in a specific {ODD}. While we learn from certain widely accepted engineering processes and principles, we tailor them-as informed by our extensive experience in Level 4 technology-for this purpose. We continuously refine those methodologies in an incremental way as we scale our operations. Waymo’s various safety methodologies are supported by three basic types of system-level testing (simulation, closed-course, and public road), which are supplemented by various forms of component and subsystem testing. These types of testing are in constant interaction; each informs and complements the other. We describe the essence of our methodologies under headings that refer to the three layers of our technology: hardware, {ADS} behavior, and vehicle operations. Hardware layer. We begin by purchasing safe, fully certified vehicles from experienced vehicle manufacturers. As part of development of our base vehicles we specify the inclusion of redundant braking and steering actuators, which we feel is necessary for safe, driverless vehicles. The performance and fault tolerance of these motion control actuators is dictated by a thorough set of technical requirements, specifying performance in both nominal and faulted conditions. These requirements are backed by extensive verification including hardware-in-the-loop and closed course testing, as well as validated on closed courses and in the field. To the base vehicle and motion control systems, Waymo adds our own sensing systems, including a multitude of lidar, radar, camera, inertial and audio units that provide an expansive understanding of the driving environment. These sensing systems are designed to meet rigorously defined performance and safety requirements. To run our advanced behavioral software, Waymo has developed a state-of-the-art computational platform that combines extreme performance with proven reliability and fault tolerance. We have designed our system to have a portfolio of fault responses tailored and matched to any failure. Waymo has also developed a robust process to identify, prioritize, and mitigate cybersecurity threats. Behavioral Layer. The behavioral layer describes the software that is capable of directing safe driverless movement of our {AVs} on public roads. There are three primary capabilities on which we evaluate the performance of the {ADS}’s behavioral layer: avoidance of crashes, completion of trips in driverless mode, and adherence to applicable driving rules. Our approach begins with hazard analysis, by which robustness is built into our designs from the beginning. We then heavily leverage scenario-based verification, to ensure that the {ADS} behavior is in line with our requirements and expectations. Finally, we subject our system to large scale simulated deployments (either through large scale log playback or public roads operations with counterfactual simulations after vehicle operator dis-engage) which allow us to empirically measure aggregate performance metrics. Operations Layer. Waymo’s safety program ensures application of industry-leading safety practices in the operation of our {AVs}, such as a fatigue management program for our trained vehicle operators, incident response planning and preparation, and coordination with law enforcement and emergency responders on how to deal safely with driverless vehicles. Waymo also recognizes the importance of seat belt use in any vehicle and we take a number of steps to encourage our {AV} passengers to use their belts. We have a fleet response team that can provide remote assistance to the {ADS} if needed. Waymo’s Risk Management Program identifies, prioritizes, and drives the resolution of potential safety issues before new or updated features or software are used on public roads or tested at our structured test facility. Our Field Safety Program identifies and effectuates appropriate disposition of potential safety issues based on information collected after updated features or software have been released for driving on public roads. Consistent with Waymo’s strong safety culture, the field safety process collects and helps resolve potential safety concerns from many other sources, including employees, our riders, the public or suppliers. Safety Governance. Waymo’s governance process includes a tiered system of analyzing safety issues that arise from the field safety process, risk management, impending deployment decisions, or any other source. The Waymo Safety Board brings together executive leaders from our Safety, Engineering and Product teams to resolve these safety issues, approve new safety activities, and ensure our entire safety framework is kept current. Waymo’s Readiness Determinations. At certain points in time Waymo needs to make a discrete determination resting on its safety methodologies with regard to the readiness of a specific configuration of the {ADS} for a specific deployment. The determination is focused on the {ODD}, the specific use case, and the particular vehicle platform. All of Waymo’s operations-closed course or public roads, with or without a trained vehicle operator-require a high level of scrutiny, and each assessment is tailored to the risks that are relevant to the intended operating mode. Of course, the level of detail in which we explore the capabilities of the {AV} is the highest when removing the trained operator due to the absence of the operator as a risk mitigation. Waymo’s process for making these readiness determinations entails an ordered examination of the relevant outputs from all of our safety methodologies combined with careful safety and engineering judgment focused on the specific facts relevant for a particular determination. Waymo will approve when it determines the {ADS} is ready for the new conditions without creating any unreasonable risks to safety. Waymo will continue to apply and adapt those methodologies, and to learn from the important contributions of others in the {AV} industry, as we continue to build an ever safer and more able {ADS}.},
	issue = {October},
	journaltitle = {{arXiv}},
	author = {Webb, Nick and Smith, Daniel and Ludwick, Christopher and Victor, Trent and Hommes, Qi and Favarò, Francesca and Ivanov, George and Daniel, Tom},
	date = {2020},
	keywords = {⛔ No {DOI} found},
}

@article{computing_special_2020,
	title = {Special Report Edge Computing : A New Architecture for a Hyperconnected World},
	author = {Computing, Edge and Architecture, New and World, Hyperconnected},
	date = {2020},
	keywords = {⛔ No {DOI} found},
}

@article{lastname_use_2020,
	title = {Use Of Smartphones for Ensuring Vulnerable Road User Safety through Path Prediction and Early Warning: An In-Depth Review of Capabilities, Limitations and Their Applications in Cooperative Intelligent Transport Systems},
	abstract = {The ﬁeld of cooperative intelligent transport systems and more speciﬁcally pedestrians to vehicles could be characterized as quite challenging, since there is a broad research area to be studied, with direct positive results to society. Pedestrians to vehicles is a type of cooperative intelligent transport system, within the group of early warning collision/safety system. In this article, we examine the research and applications carried out so far within the ﬁeld of pedestrians to vehicles cooperative transport systems by leveraging the information coming from vulnerable road users’ smartphones. Moreover, an extensive literature review has been carried out in the ﬁelds of vulnerable road users outdoor localisation via smartphones and vulnerable road users next step/movement prediction, which are closely related to pedestrian to vehicle applications and research. We identify gaps that exist in these ﬁelds that could be improved/extended/enhanced or newly developed, while we address future research objectives and methodologies that could support the improvement/development of those identiﬁed gaps.},
	pages = {20},
	author = {Lastname, Firstname and Lastname, Firstname and Lastname, Firstname},
	date = {2020},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{mobileye_unedited_2020,
	title = {Unedited 40 Minute Ride in Mobileye's Autonomous Car},
	url = {https://www.youtube.com/watch?v=kJD5R_yQ9aw},
	author = {Mobileye},
	date = {2020},
	keywords = {⛔ No {DOI} found},
}

@article{schwall_waymo_2020,
	title = {Waymo Public Road Safety Performance Data.},
	volume = {abs/2011.0},
	url = {https://arxiv.org/abs/2011.00038},
	abstract = {Waymo's mission to reduce traffic injuries and fatalities and improve mobility for all has led us to expand deployment of automated vehicles ({AVs}) on public roads without a human driver behind the wheel. As part of this process, Waymo is committed to providing the public with informative and relevant data regarding the demonstrated safety of Waymo's automated driving system ({ADS}), which we call the Waymo Driver. The data presented in this paper represents more than 6.1 million miles of automated driving in the Phoenix, Arizona metropolitan area, including operations with a trained operator behind the steering wheel from calendar year 2019 and 65,000 miles of driverless operation without a human behind the steering wheel from 2019 and the first nine months of 2020. The paper includes every collision and minor contact experienced during these operations as well as every predicted contact identified using Waymo's counterfactual ("what if") simulation of events had the vehicle's trained operator not disengaged automated driving. There were 47 contact events that occurred over this time period, consisting of 18 actual and 29 simulated contact events, none of which would be expected to result in severe or life-threatening injuries. This paper presents the collision typology and severity for each actual and simulated event, along with diagrams depicting each of the most significant events. Nearly all the events involved one or more road rule violations or other errors by a human driver or road user, including all eight of the most severe events (which we define as involving actual or expected airbag deployment in any involved vehicle). When compared to national collision statistics, the Waymo Driver completely avoided certain collision modes that human-driven vehicles are frequently involved in, including road departure and collisions with fixed objects. While data related to these collision modes is very promising, the presence of collisions that resulted from challenging situations induced by other drivers serves as a reminder of the limits of {AV} collision avoidance as long as {AVs} share roadways with human drivers. Analysis of events from {AV} operation on public roads is only one of many complementary safety evaluation methods that Waymo uses, and we are sharing it because it is objective and directly relevant to public road operation of {AVs}. As automated vehicles continue to improve and fleet mileage continues to grow, so will public understanding of their safety impact. The long-term contributions of this paper are not only the events and mileages shared, but the example set by publicly sharing this type of safety information.},
	journaltitle = {{CoRR}},
	author = {Schwall, Matthew and Daniel, Tom and Victor, Trent and Favaro, Francesca and Hohnhold, Henning},
	date = {2020},
	note = {2 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{van_gansbeke_unsupervised_2021,
	title = {Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals},
	url = {http://arxiv.org/abs/2102.06191},
	abstract = {Being able to learn dense semantic representations of images without supervision is an important problem in computer vision. However, despite its significance, this problem remains rather unexplored, with a few exceptions that considered unsupervised semantic segmentation on small-scale datasets with a narrow visual domain. In this paper, we make a first attempt to tackle the problem on datasets that have been traditionally utilized for the supervised case. To achieve this, we introduce a novel two-step framework that adopts a predetermined prior in a contrastive optimization objective to learn pixel embeddings. This marks a large deviation from existing works that relied on proxy tasks or end-to-end clustering. Additionally, we argue about the importance of having a prior that contains information about objects, or their parts, and discuss several possibilities to obtain such a prior in an unsupervised manner. Extensive experimental evaluation shows that the proposed method comes with key advantages over existing works. First, the learned pixel embeddings can be directly clustered in semantic groups using K-Means. Second, the method can serve as an effective unsupervised pre-training for the semantic segmentation task. In particular, when fine-tuning the learned representations using just 1\% of labeled examples on {PASCAL}, we outperform supervised {ImageNet} pre-training by 7.1\% {mIoU}. The code is available at https://github.com/wvangansbeke/Unsupervised-Semantic-Segmentation.},
	author = {Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis, Stamatios and Van Gool, Luc},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.06191},
	note = {1 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {⛔ No {DOI} found},
}

@article{ackerman_what_2021,
	title = {What Full Autonomy Means for the Waymo Driver},
	url = {https://spectrum.ieee.org/cars-that-think/transportation/self-driving/full-autonomy-waymo-driver},
	author = {Ackerman, Evan},
	date = {2021},
	keywords = {⛔ No {DOI} found},
}

@article{zhang_simple_2021,
	title = {A Simple and Effective Use of Object-Centric Images for Long-Tailed Object Detection},
	url = {http://arxiv.org/abs/2102.08884},
	abstract = {Object frequencies in daily scenes follow a long-tailed distribution. Many objects do not appear frequently enough in scene-centric images (e.g., sightseeing, street views) for us to train accurate object detectors. In contrast, these objects are captured at a higher frequency in object-centric images, which are intended to picture the objects of interest. Motivated by this phenomenon, we propose to take advantage of the object-centric images to improve object detection in scene-centric images. We present a simple yet surprisingly effective framework to do so. On the one hand, our approach turns an object-centric image into a useful training example for object detection in scene-centric images by mitigating the domain gap between the two image sources in both the input and label space. On the other hand, our approach employs a multi-stage procedure to train the object detector, such that the detector learns the diverse object appearances from object-centric images while being tied to the application domain of scene-centric images. On the {LVIS} dataset, our approach can improve the object detection (and instance segmentation) accuracy of rare objects by 50\% (and 33\%) relatively, without sacrificing the performance of other classes.},
	author = {Zhang, Cheng and Pan, Tai-Yu and Li, Yandong and Hu, Hexiang and Xuan, Dong and Changpinyo, Soravit and Gong, Boqing and Chao, Wei-Lun},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.08884},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {⛔ No {DOI} found},
}

@article{edge_use-case_2019,
	title = {Use-case and Requirement Document ({URD})},
	journaltitle = {Aecc},
	author = {Edge, Automotive and Consortium, Computing},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@article{cevikalp_deep_2021,
	title = {Deep Compact Polyhedral Conic Classifier for Open and Closed Set Recognition},
	url = {http://arxiv.org/abs/2102.12570},
	abstract = {In this paper, we propose a new deep neural network classifier that simultaneously maximizes the inter-class separation and minimizes the intra-class variation by using the polyhedral conic classification function. The proposed method has one loss term that allows the margin maximization to maximize the inter-class separation and another loss term that controls the compactness of the class acceptance regions. Our proposed method has a nice geometric interpretation using polyhedral conic function geometry. We tested the proposed method on various visual classification problems including closed/open set recognition and anomaly detection. The experimental results show that the proposed method typically outperforms other state-of-the art methods, and becomes a better choice compared to other tested methods especially for open set recognition type problems.},
	author = {Cevikalp, Hakan and Uzun, Bedirhan and Köpüklü, Okan and Ozturk, Gurkan},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.12570},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {⛔ No {DOI} found},
}

@article{michieli_continual_2021,
	title = {Continual Semantic Segmentation via Repulsion-Attraction of Sparse and Disentangled Latent Representations},
	url = {http://arxiv.org/abs/2103.06342},
	abstract = {Deep neural networks suffer from the major limitation of catastrophic forgetting old tasks when learning new ones. In this paper we focus on class incremental continual learning in semantic segmentation, where new categories are made available over time while previous training data is not retained. The proposed continual learning scheme shapes the latent space to reduce forgetting whilst improving the recognition of novel classes. Our framework is driven by three novel components which we also combine on top of existing techniques effortlessly. First, prototypes matching enforces latent space consistency on old classes, constraining the encoder to produce similar latent representation for previously seen classes in the subsequent steps. Second, features sparsification allows to make room in the latent space to accommodate novel classes. Finally, contrastive learning is employed to cluster features according to their semantics while tearing apart those of different classes. Extensive evaluation on the Pascal {VOC}2012 and {ADE}20K datasets demonstrates the effectiveness of our approach, significantly outperforming state-of-the-art methods.},
	author = {Michieli, Umberto and Zanuttigh, Pietro},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.06342},
	note = {2 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {⛔ No {DOI} found},
}

@article{rhinehart_deep_2019,
	title = {Deep Imitative Models for Flexible Inference, Planning, and Control},
	url = {http://arxiv.org/abs/1810.06544},
	abstract = {Imitation Learning ({IL}) is an appealing approach to learn desirable autonomous behavior. However, directing {IL} to achieve arbitrary goals is difﬁcult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difﬁcult to specify. In this paper, we propose “Imitative Models” to combine the beneﬁts of {IL} and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve speciﬁed goals. We derive families of ﬂexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six {IL} approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efﬁciently learned from expert demonstrations without online data collection. We also show our approach is robust to poorly speciﬁed goals, such as goals on the wrong side of the road.},
	journaltitle = {{arXiv}:1810.06544 [cs, stat]},
	author = {Rhinehart, Nicholas and {McAllister}, Rowan and Levine, Sergey},
	urldate = {2021-04-14},
	date = {2019-09-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.06544},
	note = {59 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning, {ToDo}, ⛔ No {DOI} found},
}

@article{gordon_splitnet_2019,
	title = {{SplitNet}: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation},
	url = {http://arxiv.org/abs/1905.07512},
	shorttitle = {{SplitNet}},
	abstract = {We propose {SplitNet}, a method for decoupling visual perception and policy learning. By incorporating auxiliary tasks and selective learning of portions of the model, we explicitly decompose the learning objectives for visual navigation into perceiving the world and acting on that perception. We show dramatic improvements over baseline models on transferring between simulators, an encouraging step towards Sim2Real. Additionally, {SplitNet} generalizes better to unseen environments from the same simulator and transfers faster and more effectively to novel embodied navigation tasks. Further, given only a small sample from a target domain, {SplitNet} can match the performance of traditional end-to-end pipelines which receive the entire dataset. Code is available https://github.com/facebookresearch/splitnet},
	journaltitle = {{arXiv}:1905.07512 [cs]},
	author = {Gordon, Daniel and Kadian, Abhishek and Parikh, Devi and Hoffman, Judy and Batra, Dhruv},
	urldate = {2021-05-05},
	date = {2019-10-23},
	eprinttype = {arxiv},
	eprint = {1905.07512},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No {DOI} found},
}

@article{korthals_coordinated_2018,
	title = {Coordinated Heterogeneous Distributed Perception based on Latent Space Representation},
	url = {http://arxiv.org/abs/1809.04558},
	abstract = {We investigate a reinforcement approach for distributed sensing based on the latent space derived from multi-modal deep generative models. Our contribution provides insights to the following benefits: Detections can be exchanged effectively between robots equipped with uni-modal sensors due to a shared latent representation of information that is trained by a Variational Auto Encoder ({VAE}). Sensor-fusion can be applied asynchronously due to the generative feature of the {VAE}. Deep Q-Networks ({DQNs}) are trained to minimize uncertainty in latent space by coordinating robots to a Point-of-Interest ({PoI}) where their sensor modality can provide beneficial information about the {PoI}. Additionally, we show that the decrease in uncertainty can be defined as the direct reward signal for training the {DQN}.},
	journaltitle = {{arXiv}:1809.04558 [cs]},
	author = {Korthals, Timo and Leitner, Jürgen and Rückert, Ulrich},
	urldate = {2021-04-18},
	date = {2018-09-12},
	eprinttype = {arxiv},
	eprint = {1809.04558},
	note = {3 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, ⛔ No {DOI} found},
}

@article{sun_corner_2021,
	title = {Corner Case Generation and Analysis for Safety Assessment of Autonomous Vehicles},
	url = {http://arxiv.org/abs/2102.03483},
	abstract = {Testing and evaluation is a crucial step in the development and deployment of Connected and Automated Vehicles ({CAVs}). To comprehensively evaluate the performance of {CAVs}, it is of necessity to test the {CAVs} in safety-critical scenarios, which rarely happen in naturalistic driving environment. Therefore, how to purposely and systematically generate these corner cases becomes an important problem. Most existing studies focus on generating adversarial examples for perception systems of {CAVs}, whereas limited efforts have been put on the decision-making systems, which is the highlight of this paper. As the {CAVs} need to interact with numerous background vehicles ({BVs}) for a long duration, variables that define the corner cases are usually high dimensional, which makes the generation a challenging problem. In this paper, a unified framework is proposed to generate corner cases for the decision-making systems. To address the challenge brought by high dimensionality, the driving environment is formulated based on Markov Decision Process, and the deep reinforcement learning techniques are applied to learn the behavior policy of {BVs}. With the learned policy, {BVs} will behave and interact with the {CAVs} more aggressively, resulting in more corner cases. To further analyze the generated corner cases, the techniques of feature extraction and clustering are utilized. By selecting representative cases of each cluster and outliers, the valuable corner cases can be identified from all generated corner cases. Simulation results of a highway driving environment show that the proposed methods can effectively generate and identify the valuable corner cases.},
	author = {Sun, Haowei and Feng, Shuo and Yan, Xintao and Liu, Henry X.},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.03483},
	note = {2 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {{ToDo}, ⛔ No {DOI} found},
}

@article{ouyang_corner_2021,
	title = {Corner case data description and detection},
	url = {http://arxiv.org/abs/2101.02494},
	abstract = {As the major factors affecting the safety of deep learning models, corner cases and related detection are crucial in {AI} quality assurance for constructing safety- and security-critical systems. The generic corner case researches involve two interesting topics. One is to enhance {DL} models robustness to corner case data via the adjustment on parameters/structure. The other is to generate new corner cases for model retraining and improvement. However, the complex architecture and the huge amount of parameters make the robust adjustment of {DL} models not easy, meanwhile it is not possible to generate all real-world corner cases for {DL} training. Therefore, this paper proposes to a simple and novel study aiming at corner case data detection via a specific metric. This metric is developed on surprise adequacy ({SA}) which has advantages on capture data behaviors. Furthermore, targeting at characteristics of corner case data, three modifications on distanced-based {SA} are developed for classification applications in this paper. Consequently, through the experiment analysis on {MNIST} data and industrial data, the feasibility and usefulness of the proposed method on corner case data detection are verified.},
	author = {Ouyang, Tinghui and Marco, Vicent Sant and Isobe, Yoshinao and Asoh, Hideki and Oiwa, Yutaka and Seo, Yoshiki},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2101.02494},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {{ToDo}, ⛔ No {DOI} found},
}

@article{yue_counterfactual_2021,
	title = {Counterfactual Zero-Shot and Open-Set Visual Recognition},
	url = {http://arxiv.org/abs/2103.00887},
	abstract = {We present a novel counterfactual framework for both Zero-Shot Learning ({ZSL}) and Open-Set Recognition ({OSR}), whose common challenge is generalizing to the unseen-classes by only training on the seen-classes. Our idea stems from the observation that the generated samples for unseen-classes are often out of the true distribution, which causes severe recognition rate imbalance between the seen-class (high) and unseen-class (low). We show that the key reason is that the generation is not Counterfactual Faithful, and thus we propose a faithful one, whose generation is from the sample-specific counterfactual question: What would the sample look like, if we set its class attribute to a certain class, while keeping its sample attribute unchanged? Thanks to the faithfulness, we can apply the Consistency Rule to perform unseen/seen binary classification, by asking: Would its counterfactual still look like itself? If ``yes'', the sample is from a certain class, and ``no'' otherwise. Through extensive experiments on {ZSL} and {OSR}, we demonstrate that our framework effectively mitigates the seen/unseen imbalance and hence significantly improves the overall performance. Note that this framework is orthogonal to existing methods, thus, it can serve as a new baseline to evaluate how {ZSL}/{OSR} models generalize. Codes are available at https://github.com/yue-zhongqi/gcm-cf.},
	author = {Yue, Zhongqi and Wang, Tan and Zhang, Hanwang and Sun, Qianru and Hua, Xian-Sheng},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.00887},
	note = {1 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {⛔ No {DOI} found},
}

@article{toromanoff_end--end_2020,
	title = {End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances},
	url = {http://arxiv.org/abs/1911.10868},
	abstract = {Reinforcement Learning ({RL}) aims at learning an optimal behavior policy from its own experiments and not rule-based control methods. However, there is no {RL} algorithm yet capable of handling a task as difficult as urban driving. We present a novel technique, coined implicit affordances, to effectively leverage {RL} for urban driving thus including lane keeping, pedestrians and vehicles avoidance, and traffic light detection. To our knowledge we are the first to present a successful {RL} agent handling such a complex task especially regarding the traffic light detection. Furthermore, we have demonstrated the effectiveness of our method by winning the Camera Only track of the {CARLA} challenge.},
	journaltitle = {{arXiv}:1911.10868 [cs, stat]},
	author = {Toromanoff, Marin and Wirbel, Emilie and Moutarde, Fabien},
	urldate = {2021-05-05},
	date = {2020-03-16},
	eprinttype = {arxiv},
	eprint = {1911.10868},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Read, Statistics - Machine Learning, ⛔ No {DOI} found},
}

@article{teng_domain_2021,
	title = {Domain Adaptation for Learning Generator from Paired Few-Shot Data},
	url = {http://arxiv.org/abs/2102.12765},
	abstract = {We propose a Paired Few-shot {GAN} ({PFS}-{GAN}) model for learning generators with sufficient source data and a few target data. While generative model learning typically needs large-scale training data, our {PFS}-{GAN} not only uses the concept of few-shot learning but also domain shift to transfer the knowledge across domains, which alleviates the issue of obtaining low-quality generator when only trained with target domain data. The cross-domain datasets are assumed to have two properties: (1) each target-domain sample has its source-domain correspondence and (2) two domains share similar content information but different appearance. Our {PFS}-{GAN} aims to learn the disentangled representation from images, which composed of domain-invariant content features and domain-specific appearance features. Furthermore, a relation loss is introduced on the content features while shifting the appearance features to increase the structural diversity. Extensive experiments show that our method has better quantitative and qualitative results on the generated target-domain data with higher diversity in comparison to several baselines.},
	author = {Teng, Chun-Chih and Chen, Pin-Yu and Chiu, Wei-Chen},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.12765},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {⛔ No {DOI} found},
}

@article{ren_deep_2021,
	title = {Deep Texture-Aware Features for Camouflaged Object Detection},
	url = {http://arxiv.org/abs/2102.02996},
	abstract = {Camouflaged object detection is a challenging task that aims to identify objects having similar texture to the surroundings. This paper presents to amplify the subtle texture difference between camouflaged objects and the background for camouflaged object detection by formulating multiple texture-aware refinement modules to learn the texture-aware features in a deep convolutional neural network. The texture-aware refinement module computes the covariance matrices of feature responses to extract the texture information, designs an affinity loss to learn a set of parameter maps that help to separate the texture between camouflaged objects and the background, and adopts a boundary-consistency loss to explore the object detail structures.We evaluate our network on the benchmark dataset for camouflaged object detection both qualitatively and quantitatively. Experimental results show that our approach outperforms various state-of-the-art methods by a large margin.},
	author = {Ren, Jingjing and Hu, Xiaowei and Zhu, Lei and Xu, Xuemiao and Xu, Yangyang and Wang, Weiming and Deng, Zijun and Heng, Pheng-Ann},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.02996},
	note = {1 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {⛔ No {DOI} found},
}

@article{huegle_deep_2020,
	title = {Deep Surrogate Q-Learning for Autonomous Driving},
	url = {http://arxiv.org/abs/2010.11278},
	abstract = {Challenging problems of deep reinforcement learning systems with regard to the application on real systems are their adaptivity to changing environments and their efficiency w.r.t. computational resources and data. In the application of learning lane-change behavior for autonomous driving, agents have to deal with a varying number of surrounding vehicles. Furthermore, the number of required transitions imposes a bottleneck, since test drivers cannot perform an arbitrary amount of lane changes in the real world. In the off-policy setting, additional information on solving the task can be gained by observing actions from others. While in the classical {RL} setup this knowledge remains unused, we use other drivers as surrogates to learn the agent's value function more efficiently. We propose Surrogate Q-learning that deals with the aforementioned problems and reduces the required driving time drastically. We further propose an efficient implementation based on a permutation-equivariant deep neural network architecture of the Q-function to estimate action-values for a variable number of vehicles in sensor range. We show that the architecture leads to a novel replay sampling technique we call Scene-centric Experience Replay and evaluate the performance of Surrogate Q-learning and Scene-centric Experience Replay in the open traffic simulator {SUMO}. Additionally, we show that our methods enhance real-world applicability of {RL} systems by learning policies on the real {highD} dataset.},
	author = {Huegle, Maria and Kalweit, Gabriel and Werling, Moritz and Boedecker, Joschka},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2010.11278},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {⛔ No {DOI} found},
}

@article{yan_exposing_2021,
	title = {Exposing Semantic Segmentation Failures via Maximum Discrepancy Competition},
	url = {http://arxiv.org/abs/2103.00259},
	abstract = {Semantic segmentation is an extensively studied task in computer vision, with numerous methods proposed every year. Thanks to the advent of deep learning in semantic segmentation, the performance on existing benchmarks is close to saturation. A natural question then arises: Does the superior performance on the closed (and frequently re-used) test sets transfer to the open visual world with unconstrained variations? In this paper, we take steps toward answering the question by exposing failures of existing semantic segmentation methods in the open visual world under the constraint of very limited human labeling effort. Inspired by previous research on model falsification, we start from an arbitrarily large image set, and automatically sample a small image set by {MAximizing} the Discrepancy ({MAD}) between two segmentation methods. The selected images have the greatest potential in falsifying either (or both) of the two methods. We also explicitly enforce several conditions to diversify the exposed failures, corresponding to different underlying root causes. A segmentation method, whose failures are more difficult to be exposed in the {MAD} competition, is considered better. We conduct a thorough {MAD} diagnosis of ten {PASCAL} {VOC} semantic segmentation algorithms. With detailed analysis of experimental results, we point out strengths and weaknesses of the competing algorithms, as well as potential research directions for further advancement in semantic segmentation. The codes are publicly available at {\textbackslash}url\{https://github.com/{QTJiebin}/{MAD}\_Segmentation\}.},
	author = {Yan, Jiebin and Zhong, Yu and Fang, Yuming and Wang, Zhangyang and Ma, Kede},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.00259},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {generalization, maximum discrepancy competition, performance evalua-, semantic segmentation, tion, ⛔ No {DOI} found},
}

@article{osinski_simulation-based_2020,
	title = {Simulation-based reinforcement learning for real-world autonomous driving},
	url = {http://arxiv.org/abs/1911.12905},
	abstract = {We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes {RGB} images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network. Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort. In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.},
	journaltitle = {{arXiv}:1911.12905 [cs]},
	author = {Osiński, Błażej and Jakubowski, Adam and Miłoś, Piotr and Zięcina, Paweł and Galias, Christopher and Homoceanu, Silviu and Michalewski, Henryk},
	urldate = {2021-05-05},
	date = {2020-03-04},
	eprinttype = {arxiv},
	eprint = {1911.12905},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, ⛔ No {DOI} found},
}

@article{mazumder_few-shot_2021,
	title = {Few-Shot Lifelong Learning},
	url = {http://arxiv.org/abs/2103.00991},
	abstract = {Many real-world classification problems often have classes with very few labeled training samples. Moreover, all possible classes may not be initially available for training, and may be given incrementally. Deep learning models need to deal with this two-fold problem in order to perform well in real-life situations. In this paper, we propose a novel Few-Shot Lifelong Learning ({FSLL}) method that enables deep learning models to perform lifelong/continual learning on few-shot data. Our method selects very few parameters from the model for training every new set of classes instead of training the full model. This helps in preventing overfitting. We choose the few parameters from the model in such a way that only the currently unimportant parameters get selected. By keeping the important parameters in the model intact, our approach minimizes catastrophic forgetting. Furthermore, we minimize the cosine similarity between the new and the old class prototypes in order to maximize their separation, thereby improving the classification performance. We also show that integrating our method with self-supervision improves the model performance significantly. We experimentally show that our method significantly outperforms existing methods on the {miniImageNet}, {CIFAR}-100, and {CUB}-200 datasets. Specifically, we outperform the state-of-the-art method by an absolute margin of 19.27\% for the {CUB} dataset.},
	author = {Mazumder, Pratik and Singh, Pravendra and Rai, Piyush},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.00991},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {⛔ No {DOI} found},
}

@article{jeong_few-shot_2021,
	title = {Few-shot Open-set Recognition by Transformation Consistency},
	url = {http://arxiv.org/abs/2103.01537},
	abstract = {In this paper, we attack a few-shot open-set recognition ({FSOSR}) problem, which is a combination of few-shot learning ({FSL}) and open-set recognition ({OSR}). It aims to quickly adapt a model to a given small set of labeled samples while rejecting unseen class samples. Since {OSR} requires rich data and {FSL} considers closed-set classification, existing {OSR} and {FSL} methods show poor performances in solving {FSOSR} problems. The previous {FSOSR} method follows the pseudo-unseen class sample-based methods, which collect pseudo-unseen samples from the other dataset or synthesize samples to model unseen class representations. However, this approach is heavily dependent on the composition of the pseudo samples. In this paper, we propose a novel unknown class sample detector, named {SnaTCHer}, that does not require pseudo-unseen samples. Based on the transformation consistency, our method measures the difference between the transformed prototypes and a modified prototype set. The modified set is composed by replacing a query feature and its predicted class prototype. {SnaTCHer} rejects samples with large differences to the transformed prototypes. Our method alters the unseen class distribution estimation problem to a relative feature transformation problem, independent of pseudo-unseen class samples. We investigate our {SnaTCHer} with various prototype transformation methods and observe that our method consistently improves unseen class sample detection performance without closed-set classification reduction.},
	author = {Jeong, Minki and Choi, Seokeon and Kim, Changick},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.01537},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {⛔ No {DOI} found},
}

@article{sun_fair1m_2021,
	title = {{FAIR}1M: A Benchmark Dataset for Fine-grained Object Recognition in High-Resolution Remote Sensing Imagery},
	url = {http://arxiv.org/abs/2103.05569},
	abstract = {With the rapid development of deep learning, many deep learning based approaches have made great achievements in object detection task. It is generally known that deep learning is a data-driven method. Data directly impact the performance of object detectors to some extent. Although existing datasets have included common objects in remote sensing images, they still have some limitations in terms of scale, categories, and images. Therefore, there is a strong requirement for establishing a large-scale benchmark on object detection in high-resolution remote sensing images. In this paper, we propose a novel benchmark dataset with more than 1 million instances and more than 15,000 images for Fine-{grAined} object {recognItion} in high-Resolution remote sensing imagery which is named as {FAIR}1M. All objects in the {FAIR}1M dataset are annotated with respect to 5 categories and 37 sub-categories by oriented bounding boxes. Compared with existing detection datasets dedicated to object detection, the {FAIR}1M dataset has 4 particular characteristics: (1) it is much larger than other existing object detection datasets both in terms of the quantity of instances and the quantity of images, (2) it provides more rich fine-grained category information for objects in remote sensing images, (3) it contains geographic information such as latitude, longitude and resolution, (4) it provides better image quality owing to a careful data cleaning procedure. To establish a baseline for fine-grained object recognition, we propose a novel evaluation method and benchmark fine-grained object detection tasks and a visual classification task using several State-Of-The-Art ({SOTA}) deep learning based models on our {FAIR}1M dataset. Experimental results strongly indicate that the {FAIR}1M dataset is closer to practical application and it is considerably more challenging than existing datasets.},
	author = {Sun, Xian and Wang, Peijin and Yan, Zhiyuan and Wang, Cheng and Diao, Wenhui and Chen, Jin and Li, Jihao and Feng, Yingchao and Xu, Tao and Weinmann, Martin and Hinz, Stefan and Fu, Kun},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.05569},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {fine-grained object detection and, remote sensing images, ⛔ No {DOI} found},
}

@article{sauer_conditional_2018,
	title = {Conditional Affordance Learning for Driving in Urban Environments},
	url = {http://arxiv.org/abs/1806.06498},
	abstract = {Most existing approaches to autonomous driving fall into one of two categories: modular pipelines, that build an extensive model of the environment, and imitation learning approaches, that map images directly to control outputs. A recently proposed third paradigm, direct perception, aims to combine the advantages of both by using a neural network to learn appropriate low-dimensional intermediate representations. However, existing direct perception approaches are restricted to simple highway situations, lacking the ability to navigate intersections, stop at traffic lights or respect speed limits. In this work, we propose a direct perception approach which maps video input to intermediate representations suitable for autonomous navigation in complex urban environments given high-level directional inputs. Compared to state-of-the-art reinforcement and conditional imitation learning approaches, we achieve an improvement of up to 68 \% in goal-directed navigation on the challenging {CARLA} simulation benchmark. In addition, our approach is the first to handle traffic lights and speed signs by using image-level labels only, as well as smooth car-following, resulting in a significant reduction of traffic accidents in simulation.},
	journaltitle = {{arXiv}:1806.06498 [cs]},
	author = {Sauer, Axel and Savinov, Nikolay and Geiger, Andreas},
	urldate = {2021-05-05},
	date = {2018-11-03},
	eprinttype = {arxiv},
	eprint = {1806.06498},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, ⛔ No {DOI} found},
}

@inproceedings{pan_semantic_2019,
	title = {Semantic Predictive Control for Explainable and Efficient Policy Learning},
	doi = {10/gjvtxp},
	abstract = {Visual anticipation of ego and object motion over a short time horizons is a key feature of human-level performance in complex environments. We propose a driving policy learning framework that predicts feature representations of future visual inputs; our predictive model infers not only future events but also semantics, which provide a visual explanation of policy decisions. Our Semantic Predictive Control ({SPC}) framework predicts future semantic segmentation and events by aggregating multi-scale feature maps. A guidance model assists action selection and enables efficient sampling-based optimization. Experiments on multiple simulation environments show that networks which implement {SPC} can outperform existing model-based reinforcement learning algorithms in terms of data efficiency and total rewards while providing clear explanations for the policy's behavior.},
	eventtitle = {2019 International Conference on Robotics and Automation ({ICRA})},
	pages = {3203--3209},
	booktitle = {2019 International Conference on Robotics and Automation ({ICRA})},
	author = {Pan, Xinlei and Chen, Xiangyu and Cai, Qizhi and Canny, John and Yu, Fisher},
	date = {2019-05},
	note = {{ISSN}: 2577-087X},
	keywords = {Feature extraction, Optimization, Predictive control, Predictive models, Semantics, Task analysis, Visualization},
}

@online{noauthor_lyra_2021,
	title = {Lyra: A New Very Low-Bitrate Codec for Speech Compression},
	url = {http://ai.googleblog.com/2021/02/lyra-new-very-low-bitrate-codec-for.html},
	shorttitle = {Lyra},
	abstract = {Posted by Alejandro Luebs, Software Engineer and Jamieson Brettle, Product Manager, Chrome    Connecting to others online via voice and vide...},
	titleaddon = {Google {AI} Blog},
	urldate = {2021-04-28},
	date = {2021},
	langid = {english},
}

@article{braun_eurocity_2019,
	title = {The {EuroCity} Persons Dataset: A Novel Benchmark for Object Detection},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/1805.07193},
	doi = {10/gjhnsz},
	shorttitle = {The {EuroCity} Persons Dataset},
	abstract = {Big data has had a great share in the success of deep learning in computer vision. Recent works suggest that there is signiﬁcant further potential to increase object detection performance by utilizing even bigger datasets. In this paper, we introduce the {EuroCity} Persons dataset, which provides a large number of highly diverse, accurate and detailed annotations of pedestrians, cyclists and other riders in urban trafﬁc scenes. The images for this dataset were collected on-board a moving vehicle in 31 cities of 12 European countries. With over 238200 person instances manually labeled in over 47300 images, {EuroCity} Persons is nearly one order of magnitude larger than person datasets used previously for benchmarking. The dataset furthermore contains a large number of person orientation annotations (over 211200). We optimize four state-of-the-art deep learning approaches (Faster R-{CNN}, R-{FCN}, {SSD} and {YOLOv}3) to serve as baselines for the new object detection benchmark. In experiments with previous datasets we analyze the generalization capabilities of these detectors when trained with the new dataset. We furthermore study the effect of the training set size, the dataset diversity (day- vs. night-time, geographical region), the dataset detail (i.e. availability of object orientation information) and the annotation quality on the detector performance. Finally, we analyze error sources and discuss the road ahead.},
	pages = {1844--1861},
	number = {8},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Braun, Markus and Krebs, Sebastian and Flohr, Fabian and Gavrila, Dariu M.},
	urldate = {2021-05-04},
	date = {2019-08-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.07193},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{lu_fine-grained_2020,
	title = {Fine-Grained Vehicle Perception via 3D Part-Guided Visual Data Augmentation},
	url = {http://arxiv.org/abs/2012.08055},
	abstract = {Holistically understanding an object and its 3D movable parts through visual perception models is essential for enabling an autonomous agent to interact with the world. For autonomous driving, the dynamics and states of vehicle parts such as doors, the trunk, and the bonnet can provide meaningful semantic information and interaction states, which are essential to ensure the safety of the self-driving vehicle. Existing visual perception models mainly focus on coarse parsing such as object bounding box detection or pose estimation and rarely tackle these situations. In this paper, we address this important problem for autonomous driving by solving two critical issues using visual data augmentation. First, to deal with data scarcity, we propose an effective training data generation process by fitting a 3D car model with dynamic parts to vehicles in real images and then reconstructing human-vehicle interaction scenarios. This allows us to directly edit the real images using the aligned 3D parts, yielding effective training data generation for learning robust deep neural networks ({DNNs}). Second, to benchmark the quality of 3D part understanding, we collect a large dataset in real world driving scenarios with vehicles in uncommon states ({VUS}), i.e. with the door or trunk opened, etc. Experiments demonstrate our trained network with visual data augmentation largely outperforms other baselines in terms of 2D detection and instance segmentation accuracy. Our network yields large improvements in discovering and understanding these uncommon cases. Moreover, we plan to release all of the source code, the dataset, and the trained model on {GitHub}.},
	pages = {1--15},
	author = {Lu, Feixiang and Liu, Zongdai and Miao, Hui and Wang, Peng and Zhang, Liangjun and Yang, Ruigang and Manocha, Dinesh and Zhou, Bin},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2012.08055},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
}

@article{lyu_embedded_2021,
	title = {Embedded Knowledge Distillation in Depth-level Dynamic Neural Network},
	url = {http://arxiv.org/abs/2103.00793},
	abstract = {In real applications, different computation-resource devices need different-depth networks (e.g., {ResNet}-18/34/50) with high-accuracy. Usually, existing strategies either design multiple networks (nets) and train them independently, or utilize compression techniques (e.g., low-rank decomposition, pruning, and teacher-to-student) to evolve a trained large model into a small net. These methods are subject to the low-accuracy of small nets, or complicated training processes induced by the dependence of accompanying assistive large models. In this article, we propose an elegant Depth-level Dynamic Neural Network ({DDNN}) integrated different-depth sub-nets of similar architectures. Instead of training individual nets with different-depth configurations, we only train a {DDNN} to dynamically switch different-depth sub-nets at runtime using one set of shared weight parameters. To improve the generalization of sub-nets, we design the Embedded-Knowledge-Distillation ({EKD}) training mechanism for the {DDNN} to implement semantic knowledge transfer from the teacher (full) net to multiple sub-nets. Specifically, the Kullback-Leibler divergence is introduced to constrain the posterior class probability consistency between full-net and sub-net, and self-attention on the same resolution feature of different depth is addressed to drive more abundant feature representations of sub-nets. Thus, we can obtain multiple high accuracy sub-nets simultaneously in a {DDNN} via the online knowledge distillation in each training iteration without extra computation cost. Extensive experiments on {CIFAR}-10, {CIFAR}-100, and {ImageNet} datasets demonstrate that sub-nets in {DDNN} with {EKD} training achieves better performance than the depth-level pruning or individually training while preserving the original performance of full-net.},
	pages = {1--11},
	author = {Lyu, Shuchang and Xu, Ting-Bing and Cheng, Guangliang},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.00793},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
}

@article{feng_deep_2021,
	title = {Deep Multi-modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges},
	volume = {22},
	issn = {1524-9050, 1558-0016},
	url = {http://arxiv.org/abs/1902.07830},
	doi = {10/ggwdc4},
	shorttitle = {Deep Multi-modal Object Detection and Semantic Segmentation for Autonomous Driving},
	abstract = {Recent advancements in perception for autonomous driving are driven by deep learning. In order to achieve robust and accurate scene understanding, autonomous vehicles are usually equipped with different sensors (e.g. cameras, {LiDARs}, Radars), and multiple sensing modalities can be fused to exploit their complementary properties. In this context, many methods have been proposed for deep multi-modal perception problems. However, there is no general guideline for network architecture design, and questions of “what to fuse”, “when to fuse”, and “how to fuse” remain open. This review paper attempts to systematically summarize methodologies and discuss challenges for deep multi-modal object detection and semantic segmentation in autonomous driving. To this end, we ﬁrst provide an overview of on-board sensors on test vehicles, open datasets, and background information for object detection and semantic segmentation in autonomous driving research. We then summarize the fusion methodologies and discuss challenges and open questions. In the appendix, we provide tables that summarize topics and methods. We also provide an interactive online platform to navigate each reference: https://boschresearch.github.io/multimodalperception/.},
	pages = {1341--1360},
	number = {3},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	shortjournal = {{IEEE} Trans. Intell. Transport. Syst.},
	author = {Feng, Di and Haase-Schütz, Christian and Rosenbaum, Lars and Hertlein, Heinz and Glaeser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
	urldate = {2021-04-18},
	date = {2021-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1902.07830},
	note = {118 citations (Semantic Scholar/{arXiv}) [2021-05-04]
114 citations (Semantic Scholar/{DOI}) [2021-04-18]},
	keywords = {Computer Science - Robotics},
}

@article{yin_deep_2021,
	title = {Deep Learning on Traffic Prediction: Methods, Analysis and Future Directions},
	url = {http://arxiv.org/abs/2004.08555},
	doi = {10.1109/TITS.2021.3054840},
	shorttitle = {Deep Learning on Traffic Prediction},
	abstract = {Trafﬁc prediction plays an essential role in intelligent trafﬁc system. Accurate trafﬁc prediction can assist route planing, guide vehicle dispatching, and mitigate trafﬁc congestion. This problem is challenging due to the complicated and dynamic spatio-temporal dependencies between different regions in the road network. Recently, a signiﬁcant amount of research efforts have been devoted to this area, greatly advancing trafﬁc prediction abilities. The purpose of this paper is to provide a comprehensive survey for trafﬁc prediction. Speciﬁcally, we ﬁrst summarize the existing trafﬁc prediction methods, and give a taxonomy of them. Second, we list the common applications of trafﬁc prediction and the state-of-the-art in these applications. Third, We collect and organize several related public datasets in the existing literature. Furthermore, we give an evaluation by conducting extensive experiments to compare the performance of methods related to trafﬁc demand and speed prediction respectively on two datasets. Finally, we discuss potential future directions.},
	journaltitle = {{arXiv}:2004.08555 [cs, eess]},
	author = {Yin, Xueyan and Wu, Genze and Wei, Jinze and Shen, Yanming and Qi, Heng and Yin, Baocai},
	urldate = {2021-04-17},
	date = {2021-03-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.08555},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]
0 citations (Semantic Scholar/{DOI}) [2021-05-04]},
	keywords = {Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Signal Processing, ⚠️ Invalid {DOI}},
}

@article{cui_deep_2021,
	title = {Deep Learning for Image and Point Cloud Fusion in Autonomous Driving: A Review},
	issn = {1524-9050, 1558-0016},
	url = {http://arxiv.org/abs/2004.05224},
	doi = {10/gjn28f},
	shorttitle = {Deep Learning for Image and Point Cloud Fusion in Autonomous Driving},
	abstract = {Autonomous vehicles were experiencing rapid development in the past few years. However, achieving full autonomy is not a trivial task, due to the nature of the complex and dynamic driving environment. Therefore, autonomous vehicles are equipped with a suite of different sensors to ensure robust, accurate environmental perception. In particular, the {cameraLiDAR} fusion is becoming an emerging research theme. However, so far there has been no critical review that focuses on deeplearning-based camera-{LiDAR} fusion methods. To bridge this gap and motivate future research, this paper devotes to review recent deep-learning-based data fusion approaches that leverage both image and point cloud. This review gives a brief overview of deep learning on image and point cloud data processing. Followed by in-depth reviews of camera-{LiDAR} fusion methods in depth completion, object detection, semantic segmentation, tracking and online cross-sensor calibration, which are organized based on their respective fusion levels. Furthermore, we compare these methods on publicly available datasets. Finally, we identiﬁed gaps and over-looked challenges between current academic researches and real-world applications. Based on these observations, we provide our insights and point out promising research directions.},
	pages = {1--18},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	shortjournal = {{IEEE} Trans. Intell. Transport. Syst.},
	author = {Cui, Yaodong and Chen, Ren and Chu, Wenbo and Chen, Long and Tian, Daxin and Li, Ying and Cao, Dongpu},
	urldate = {2021-04-19},
	date = {2021},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.05224},
	note = {6 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{bernhard_bark_2020,
	title = {{BARK}: Open Behavior Benchmarking in Multi-Agent Environments},
	url = {http://arxiv.org/abs/2003.02604},
	doi = {10/gjp4jn},
	shorttitle = {{BARK}},
	abstract = {Predicting and planning interactive behaviors in complex traffic situations presents a challenging task. Especially in scenarios involving multiple traffic participants that interact densely, autonomous vehicles still struggle to interpret situations and to eventually achieve their own mission goal. As driving tests are costly and challenging scenarios are hard to find and reproduce, simulation is widely used to develop, test, and benchmark behavior models. However, most simulations rely on datasets and simplistic behavior models for traffic participants and do not cover the full variety of real-world, interactive human behaviors. In this work, we introduce {BARK}, an open-source behavior benchmarking environment designed to mitigate the shortcomings stated above. In {BARK}, behavior models are (re-)used for planning, prediction, and simulation. A range of models is currently available, such as Monte-Carlo Tree Search and Reinforcement Learning-based behavior models. We use a public dataset and sampling-based scenario generation to show the inter-exchangeability of behavior models in {BARK}. We evaluate how well the models used cope with interactions and how robust they are towards exchanging behavior models. Our evaluation shows that {BARK} provides a suitable framework for a systematic development of behavior models.},
	pages = {6201--6208},
	journaltitle = {2020 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	author = {Bernhard, Julian and Esterle, Klemens and Hart, Patrick and Kessler, Tobias},
	urldate = {2021-04-14},
	date = {2020-10-24},
	eprinttype = {arxiv},
	eprint = {2003.02604},
	note = {5 citations (Semantic Scholar/{arXiv}) [2021-05-04]
5 citations (Semantic Scholar/{DOI}) [2021-04-14]},
	keywords = {Computer Science - Multiagent Systems},
}

@article{zhong_autonomous_2020,
	title = {Autonomous and Semi-Autonomous Intersection Management: A Survey},
	issn = {1939-1390, 1941-1197},
	url = {http://arxiv.org/abs/2006.13133},
	doi = {10/gjp324},
	shorttitle = {Autonomous and Semi-Autonomous Intersection Management},
	abstract = {Intersection is a major source of trafﬁc delays and accidents within modern transportation systems. Compared to signalized intersection management, autonomous intersection management ({AIM}) coordinates the intersection crossing at an individual vehicle level, which provides additional ﬂexibility. {AIM} can potentially eliminate stopping in intersection crossing due to trafﬁc lights while maintaining a safe separation among conﬂicting movements. In this paper, the state-of-the-art {AIM} research among various disciplines (e.g., trafﬁc engineering, control engineering) is surveyed from the perspective of three hierarchical layers: corridor coordination layer, intersection management layer, and vehicle control layer. The key aspects of {AIM} designs are discussed in details, including conﬂict detection schemes, priority rules, control centralization, computation complexity, etc. The potential improvements for {AIM} evaluation with the emphasis of realistic scenarios are provided. This survey serves as a comprehensive review of {AIM} design and provides promising directions for future research.},
	pages = {0--0},
	journaltitle = {{IEEE} Intelligent Transportation Systems Magazine},
	shortjournal = {{IEEE} Intell. Transport. Syst. Mag.},
	author = {Zhong, Zijia and Nejad, Mark and Lee, Earl E.},
	urldate = {2021-04-13},
	date = {2020},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.13133},
	note = {1 citations (Semantic Scholar/{arXiv}) [2021-05-04]
1 citations (Semantic Scholar/{DOI}) [2021-04-14]},
	keywords = {Computer Science - Multiagent Systems},
}

@article{kumaran_anomaly_2021,
	title = {Anomaly Detection in Road Traffic Using Visual Surveillance: A Survey},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	url = {http://arxiv.org/abs/1901.08292},
	doi = {10/gjp329},
	shorttitle = {Anomaly Detection in Road Traffic Using Visual Surveillance},
	abstract = {Computer vision has evolved in the last decade as a key technology for numerous applications replacing human supervision. In this paper, we present a survey on relevant visual surveillance related researches for anomaly detection in public places, focusing primarily on roads. Firstly, we revisit the surveys done in the last 10 years in this field. Since the underlying building block of a typical anomaly detection is learning, we emphasize more on learning methods applied on video scenes. We then summarize the important contributions made during last six years on anomaly detection primarily focusing on features, underlying techniques, applied scenarios and types of anomalies using single static camera. Finally, we discuss the challenges in the computer vision related anomaly detection techniques and some of the important future possibilities.},
	pages = {1--26},
	number = {6},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Kumaran, Santhosh Kelathodi and Dogra, Debi Prosad and Roy, Partha Pratim},
	urldate = {2021-04-13},
	date = {2021-02},
	eprinttype = {arxiv},
	eprint = {1901.08292},
	note = {17 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_adversarial_2021,
	title = {Adversarial Reciprocal Points Learning for Open Set Recognition},
	url = {http://arxiv.org/abs/2103.00953},
	abstract = {Open set recognition ({OSR}), aiming to simultaneously classify the seen classes and identify the unseen classes as 'unknown', is essential for reliable machine learning.The key challenge of {OSR} is how to reduce the empirical classification risk on the labeled known data and the open space risk on the potential unknown data simultaneously. To handle the challenge, we formulate the open space risk problem from the perspective of multi-class integration, and model the unexploited extra-class space with a novel concept Reciprocal Point. Follow this, a novel learning framework, termed Adversarial Reciprocal Point Learning ({ARPL}), is proposed to minimize the overlap of known distribution and unknown distributions without loss of known classification accuracy. Specifically, each reciprocal point is learned by the extra-class space with the corresponding known category, and the confrontation among multiple known categories are employed to reduce the empirical classification risk. Then, an adversarial margin constraint is proposed to reduce the open space risk by limiting the latent open space constructed by reciprocal points. To further estimate the unknown distribution from open space, an instantiated adversarial enhancement method is designed to generate diverse and confusing training samples, based on the adversarial mechanism between the reciprocal points and known classes. This can effectively enhance the model distinguishability to the unknown classes. Extensive experimental results on various benchmark datasets indicate that the proposed method is significantly superior to other existing approaches and achieves state-of-the-art performance.},
	pages = {1--16},
	author = {Chen, Guangyao and Peng, Peixi and Wang, Xiangqian and Tian, Yonghong},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.00953},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
}

@article{ristoski_rdf2vec_2020,
	title = {{RDF}2Vec: {RDF} Graph Embeddings and Their Applications},
	abstract = {Linked Open Data has been recognized as a valuable source for background information in many data mining and information retrieval tasks. However, most of the existing tools require features in propositional form, i.e., a vector of nominal or numerical features associated with an instance, while Linked Open Data sources are graphs by nature. In this paper, we present {RDF}2Vec, an approach that uses language modeling approaches for unsupervised feature extraction from sequences of words, and adapts them to {RDF} graphs. We generate sequences by leveraging local information from graph sub-structures, harvested by Weisfeiler-Lehman Subtree {RDF} Graph Kernels and graph walks, and learn latent numerical representations of entities in {RDF} graphs. We evaluate our approach on three different tasks: (i) standard machine-learning tasks (ii) entity and document modeling (iii) content-based recommender systems. The evaluation shows that the proposed entity embeddings outperform existing techniques, and that feature vector representations of general knowledge graphs such as {DBpedia} and Wikidata can be easily reused for different tasks.},
	pages = {25},
	author = {Ristoski, Petar and Rosati, Jessica and Noia, Tommaso Di and Leone, Renato De and Paulheim, Heiko},
	date = {2020},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{lassance_representing_2020,
	title = {Representing Deep Neural Networks Latent Space Geometries with Graphs},
	url = {http://arxiv.org/abs/2011.07343},
	abstract = {Deep Learning ({DL}) has attracted a lot of attention for its ability to reach state-of-the-art performance in many machine learning tasks. The core principle of {DL} methods consists in training composite architectures in an end-to-end fashion, where inputs are associated with outputs trained to optimize an objective function. Because of their compositional nature, {DL} architectures naturally exhibit several intermediate representations of the inputs, which belong to so-called latent spaces. When treated individually, these intermediate representations are most of the time unconstrained during the learning process, as it is unclear which properties should be favored. However, when processing a batch of inputs concurrently, the corresponding set of intermediate representations exhibit relations (what we call a geometry) on which desired properties can be sought. In this work, we show that it is possible to introduce constraints on these latent geometries to address various problems. In more details, we propose to represent geometries by constructing similarity graphs from the intermediate representations obtained when processing a batch of inputs. By constraining these Latent Geometry Graphs ({LGGs}), we address the three following problems: i) Reproducing the behavior of a teacher architecture is achieved by mimicking its geometry, ii) Designing efficient embeddings for classification is achieved by targeting specific geometries, and iii) Robustness to deviations on inputs is achieved via enforcing smooth variation of geometry between consecutive latent spaces. Using standard vision benchmarks, we demonstrate the ability of the proposed geometry-based methods in solving the considered problems.},
	journaltitle = {{arXiv}:2011.07343 [cs]},
	author = {Lassance, Carlos and Gripon, Vincent and Ortega, Antonio},
	urldate = {2021-05-01},
	date = {2020-11-14},
	eprinttype = {arxiv},
	eprint = {2011.07343},
	note = {1 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Machine Learning, ⛔ No {DOI} found},
}

@article{grimm_knowledge_2007,
	title = {Knowledge Representation and Ontologies},
	abstract = {In Artiﬁcial Intelligence, knowledge representation studies the formalisation of knowledge and its processing within machines. Techniques of automated reasoning allow a computer system to draw conclusions from knowledge represented in a machine-interpretable form. Recently, ontologies have evolved in computer science as computational artefacts to provide computer systems with a conceptual yet computational model of a particular domain of interest. In this way, computer systems can base decisions on reasoning about domain knowledge, similar to humans. This chapter gives an overview on basic knowledge representation aspects and on ontologies as used within computer systems. After introducing ontologies in terms of their appearance, usage and classiﬁcation, it addresses concrete ontology languages that are particularly important in the context of the Semantic Web. The most recent and predominant ontology languages and formalisms are presented in relation to each other and a selection of them is discussed in more detail.},
	pages = {51},
	author = {Grimm, Stephan and Hitzler, Pascal and Abecker, Andreas},
	date = {2007},
	langid = {english},
	keywords = {{ToDo}, ⛔ No {DOI} found},
}

@article{jurisch_graph-convolution-based_2019,
	title = {Graph-Convolution-Based Classiﬁcation for Ontology Alignment Change Prediction},
	abstract = {Finding alignments between ontologies is a challenging and time-consuming task. When the aligned ontologies change, these alignments need to be changed as well. A recent approach to this problem proposes using embeddings as a representation for classifying changes. In this work, we compare embedding-based approaches to a neural network architecture built for node classiﬁcation in knowledge graphs, namely relational graph convolutional networks. In our evaluation on two datasets from the biomedical domain, the best-performing embedding-based methods are {RDF}2Vec and {TransE}. The Graph convolution approach achieves similar results as the best-performing embedding based methods on a smaller dataset but outperforms all other approaches in standard classiﬁcation metrics on a bigger dataset.},
	pages = {10},
	author = {Jurisch, Matthias and Igler, Bodo},
	date = {2019},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{guo_ontologies_2019,
	title = {From Ontologies to Learning-Based Programs},
	abstract = {In this paper, we discuss the work in progress on designing a prototype for a novel declarative learningbased programming system and present our preliminary results. The main idea is to express learningbased programs in terms of declarative domain knowledge. Given that the existing ontologies contain rich domain and world knowledge, we propose to automatically generate the learning-based programs from the current ontology representation languages such as {OWL}. The ontological concepts and domain relationships are compiled to a graph which is a partial program. The nodes in the graph are connected to data sensors and learners. Local training algorithms can use data and train learners corresponding to each concept and domain relationship in the graph. Global inference mechanisms make the ﬁnal decisions based on the local prediction of the learners and under the ontological constraints. We test our framework on the entitymention-relation extraction task.},
	pages = {6},
	author = {Guo, Quan and Uszok, Andrzej and Kordjamshidi, Parisa},
	date = {2019},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{trebacz_using_2020,
	title = {Using ontology embeddings for structural inductive bias in gene expression data analysis},
	url = {http://arxiv.org/abs/2011.10998},
	abstract = {Stratifying cancer patients based on their gene expression levels allows improving diagnosis, survival analysis and treatment planning. However, such data is extremely highly dimensional as it contains expression values for over 20000 genes per patient, and the number of samples in the datasets is low. To deal with such settings, we propose to incorporate prior biological knowledge about genes from ontologies into the machine learning system for the task of patient classification given their gene expression data. We use ontology embeddings that capture the semantic similarities between the genes to direct a Graph Convolutional Network, and therefore sparsify the network connections. We show this approach provides an advantage for predicting clinical targets from high-dimensional low-sample data.},
	journaltitle = {{arXiv}:2011.10998 [cs, q-bio]},
	author = {Trębacz, Maja and Shams, Zohreh and Jamnik, Mateja and Scherer, Paul and Simidjievski, Nikola and Terre, Helena Andres and Liò, Pietro},
	urldate = {2021-05-01},
	date = {2020-11-22},
	eprinttype = {arxiv},
	eprint = {2011.10998},
	note = {2 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Genomics, ⛔ No {DOI} found},
}

@article{mohamed_unmanned_2020,
	title = {Unmanned aerial vehicles applications in future smart cities},
	doi = {10/gjvpkq},
	abstract = {Foreseeing changes in how smart cities manage their resources and provide services to the residents; research, development and production in various relevant technology ﬁelds is accelerating. Taking advantage of recent advances and innovations in Information and Communication Technologies ({ICT}), robotics and software; smart cities can optimize resources utilization and enhance operations in health, transportation, energy, and water services, as well as elevating the level of comfort of residents. Eﬀectively and eﬃciently utilizing {ICT} and robotics in smart cities will result in reducing costs and resources consumption in addition to engaging more eﬀectively and actively with the citizens. One of these technologies is the unmanned aerial vehicle ({UAV}), which can provide many applications for smart cities and create a positive impact on the society. For example, {UAVs} can be used for environmental monitoring, traﬃc management, pollution monitoring, civil security control, and merchandise delivery. {UAV} applications among several others can provide cost-eﬀective services to help achieve the objectives of smart cities. However, the integration of {UAVs} in smart cities is very challenging due to several issues and concerns such as safety, privacy and ethical/legal use. This paper reviews the potential applications integrating {UAVs} in smart cities, their implications, and the technical and non-technical issues facing such integration. It also discusses regulations and enabling technologies currently available and being developed that can be utilized to support such integration.},
	pages = {15},
	journaltitle = {Technological Forecasting},
	author = {Mohamed, Nader},
	date = {2020},
	langid = {english},
}

@inproceedings{kirman_sensor_1991,
	title = {Sensor abstractions for control of navigation},
	doi = {10/b9tsq6},
	abstract = {An approach to building high-level control systems for robotics that is based on Bayesian decision theory is presented. The authors show how this approach provides a natural and modular way of integrating sensing and planning. They develop a simple solution for a particular problem as an illustration. They also examine the cost of using such a model and consider the areas in which abstraction can reduce this cost. The authors focus on the area of spatial abstraction. They discuss an abstraction that has been used to solve problems involving robot navigation and give a detailed account of the mapping from raw sensor data to the abstraction.{\textless}{\textgreater}},
	eventtitle = {1991 {IEEE} International Conference on Robotics and Automation Proceedings},
	pages = {2812--2817 vol.3},
	booktitle = {1991 {IEEE} International Conference on Robotics and Automation Proceedings},
	author = {Kirman, J. and Basye, K. and Dean, T.},
	date = {1991-04},
	note = {24 citations (Semantic Scholar/{DOI}) [2021-03-29]},
	keywords = {Bayesian methods, Costs, Decision theory, Encoding, Military computing, Navigation, Orbital robotics, Read Abstract, Robot control, Robot sensing systems, Statistics},
}

@inproceedings{gaskell_sensor_1993,
	title = {Sensor models and a framework for sensor management},
	volume = {2059},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/2059/0000/Sensor-models-and-a-framework-for-sensor-management/10.1117/12.150237.short},
	doi = {10/d5cmmq},
	abstract = {We describe the use of Bayesian belief networks and decision theoretic principles for sensor management in multi-sensor systems. This framework provides a way of representing sensory data and choosing actions under uncertainty. The work considers how to distribute functionality between sensors and the controller. Use is made of logical sensors based on complementary physical sensors to provide information at the task level of abstraction represented within the network. We are applying these methods in the area of low level planning in mobile robotics. A key feature of the work is the development of quantified models to represent diverse sensors, in particular the sonar array and infra-red triangulation sensors we use on our {AGV}. We need to develop a model which can handle these very different sensors but provides a common interface to the sensor management process. We do this by quantifying the uncertainty through probabilistic models of the sensors, taking into account their physical characteristics and interaction with the expected environment. Modelling the sensor characteristics to an appropriate level of detail has the advantage of giving more accurate and robust mapping between the physical and logical sensor, as well as a better understanding of environmental dependency and its limitations. We describe a model of a sonar array, which explicitly takes into account features such as beam-width and ranging errors, and its integration into the sensor management process.},
	eventtitle = {Sensor Fusion {VI}},
	pages = {2--13},
	booktitle = {Sensor Fusion {VI}},
	publisher = {International Society for Optics and Photonics},
	author = {Gaskell, Alex P. and Probert, Penelope J.},
	urldate = {2021-03-29},
	date = {1993-08-20},
	note = {7 citations (Semantic Scholar/{DOI}) [2021-03-29]},
}

@article{brewster_data_2004,
	title = {Data Driven Ontology Evaluation},
	abstract = {The evaluation of ontologies is vital for the growth of the Semantic Web. We consider a number of problems in evaluating a knowledge artifact like an ontology. We propose in this paper that one approach to ontology evaluation should be corpus or data driven. A corpus is the most accessible form of knowledge and its use allows a measure to be derived of the ‘fit’ between an ontology and a domain of knowledge. We consider a number of methods for measuring this ‘fit’ and propose a measure to evaluate structural fit, and a probabilistic approach to identifying the best ontology.},
	author = {Brewster, Christopher and Alani, Harith and Dasmahapatra, Srinandan and Wilks, Yorick},
	date = {2004-01-01},
	keywords = {⛔ No {DOI} found},
}

@article{hovi_data-driven_2019,
	title = {Data-driven generation of rules for ontology-based decision making systems in autonomous vehicles},
	abstract = {Autonomous vehicles can be controlled based on semantically abstracted knowledge of the surrounding environment of the vehicle. One such approach to knowledge based decision making is an ontology-based decision making system. This system requires a set of logical rules based on which the reasoning of correct action for each time instant is performed. Writing these rules by hand can prove challenging as covering all traffic scenarios produces a large number of rules to consider. However, these rules can also be obtained by learning them from data. In this work, creation of two datasets is covered as well as the generation of logical rules from these datasets. Two methods of learning the rules are implemented: association rule learning and a deep learning based approach. Both methods implemented produce a correct set of rules compliant with traffic rules.},
	pages = {70},
	author = {Hovi, Juha},
	date = {2019},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{cherix_crocus_2014,
	title = {{CROCUS}: Cluster-based Ontology Data Cleansing},
	abstract = {Over the past years, a vast number of datasets have been published based on Semantic Web standards, which provides an opportunity for creating novel industrial applications. However, industrial requirements on data quality are high while the time to market as well as the required costs for data preparation have to be kept low. Unfortunately, many Linked Data sources are error-prone which prevents their direct use in productive systems. Hence, (semi-)automatic quality assurance processes are needed as manual ontology repair procedures by domain experts are expensive and time consuming. In this article, we present {CROCUS} – a pipeline for cluster-based ontology data cleansing. Our system provides a semi-automatic approach for instance-level error detection in ontologies which is agnostic of the underlying Linked Data knowledge base and works at very low costs. {CROCUS} was evaluated on two datasets. The experiments show that we are able to detect errors with high recall.},
	pages = {8},
	author = {Cherix, Didier and Usbeck, Ricardo and Both, Andreas and Lehmann, Jens},
	date = {2014},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@inproceedings{behl_label_2020,
	location = {Las Vegas, {NV}, {USA}},
	title = {Label Efficient Visual Abstractions for Autonomous Driving},
	isbn = {978-1-72816-212-6},
	url = {https://ieeexplore.ieee.org/document/9340641/},
	doi = {10/gjvpkm},
	abstract = {It is well known that semantic segmentation can be used as an effective intermediate representation for learning driving policies. However, the task of street scene semantic segmentation requires expensive annotations. Furthermore, segmentation algorithms are often trained irrespective of the actual driving task, using auxiliary image-space loss functions which are not guaranteed to maximize driving metrics such as safety or distance traveled per intervention. In this work, we seek to quantify the impact of reducing segmentation annotation costs on learned behavior cloning agents. We analyze several segmentation-based intermediate representations. We use these visual abstractions to systematically study the trade-off between annotation efﬁciency and driving performance, i.e., the types of classes labeled, the number of image samples used to learn the visual abstraction model, and their granularity (e.g., object masks vs. 2D bounding boxes). Our analysis uncovers several practical insights into how segmentation-based visual abstractions can be exploited in a more label efﬁcient manner. Surprisingly, we ﬁnd that state-of-the-art driving performance can be achieved with orders of magnitude reduction in annotation cost. Beyond label efﬁciency, we ﬁnd several additional training beneﬁts when leveraging visual abstractions, such as a signiﬁcant reduction in the variance of the learned policy when compared to state-of-the-art end-to-end driving models.},
	eventtitle = {2020 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages = {2338--2345},
	booktitle = {2020 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	publisher = {{IEEE}},
	author = {Behl, Aseem and Chitta, Kashyap and Prakash, Aditya and Ohn-Bar, Eshed and Geiger, Andreas},
	urldate = {2021-03-29},
	date = {2020-10-24},
	langid = {english},
	note = {1 citations (Semantic Scholar/{DOI}) [2021-03-29]},
	keywords = {Read Abstract},
}

@article{chu_feature_2020,
	title = {Feature Space Augmentation for Long-Tailed Data},
	doi = {10/gjvpkp},
	author = {Chu, Peng and Bian, Xiao and Liu, Shaopeng and Ling, Haibin},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2008.03673v1},
}

@inproceedings{wotawa_ontology-based_2020,
	location = {Porto, Portugal},
	title = {Ontology-based Testing: An Emerging Paradigm for Modeling and Testing Systems and Software},
	isbn = {978-1-72811-075-2},
	url = {https://ieeexplore.ieee.org/document/9155880/},
	doi = {10/gjvpkn},
	shorttitle = {Ontology-based Testing},
	abstract = {Model-based testing has been successfully applied for test case generation in practice. Its underlying idea is to utilize models of the system for obtaining system inputs and their corresponding expected outputs. In this paper, we report on experiences gained when using a different methodology relying on models, i.e., ontology-based testing, for generating test suites in practice. Instead of modeling the system’s behavior, ontologybased testing relies on models of the system’s environment, i.e., an environmental ontology. Test cases are generated from ontologies converting them into an input model for combinatorial testing, and using a combinatorial testing algorithm for ﬁnally computing the test cases. We show how ontology-based testing can be applied in three different application domains, i.e., testing autonomous driving functionality, security testing, and compiler testing, discuss issues arising and indicate future research objectives.},
	eventtitle = {2020 {IEEE} International Conference on Software Testing, Verification and Validation Workshops ({ICSTW})},
	pages = {14--17},
	booktitle = {2020 {IEEE} International Conference on Software Testing, Verification and Validation Workshops ({ICSTW})},
	publisher = {{IEEE}},
	author = {Wotawa, Franz and Bozic, Josip and Li, Yihao},
	urldate = {2021-05-01},
	date = {2020-10},
	langid = {english},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-05-04]},
}

@article{hohenecker_deep_2017,
	title = {Deep Learning for Ontology Reasoning},
	url = {http://arxiv.org/abs/1705.10342},
	abstract = {In this work, we present a novel approach to ontology reasoning that is based on deep learning rather than logic-based formal reasoning. To this end, we introduce a new model for statistical relational learning that is built upon deep recursive neural networks, and give experimental evidence that it can easily compete with, or even outperform, existing logic-based reasoners on the task of ontology reasoning. More precisely, we compared our implemented system with one of the best logic-based ontology reasoners at present, {RDFox}, on a number of large standard benchmark datasets, and found that our system attained high reasoning quality, while being up to two orders of magnitude faster.},
	journaltitle = {{arXiv}:1705.10342 [cs]},
	author = {Hohenecker, Patrick and Lukasiewicz, Thomas},
	urldate = {2021-05-01},
	date = {2017-05-29},
	eprinttype = {arxiv},
	eprint = {1705.10342},
	note = {21 citations (Semantic Scholar/{arXiv}) [2021-05-04]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, ⛔ No {DOI} found},
}

@inproceedings{fu_lidar_2019,
	location = {Auckland, New Zealand},
	title = {{LIDAR} and Monocular Camera Fusion: On-road Depth Completion for Autonomous Driving},
	isbn = {978-1-5386-7024-8},
	url = {https://ieeexplore.ieee.org/document/8917201/},
	doi = {10/ghmfzz},
	shorttitle = {{LIDAR} and Monocular Camera Fusion},
	abstract = {{LIDAR} and {RGB} cameras are commonly used sensors in autonomous vehicles. However, both of them have limitations: {LIDAR} provides accurate depth but is sparse in vertical and horizontal resolution; {RGB} images provide dense texture but lack depth information. In this paper, we fuse {LIDAR} and {RGB} images by a deep neural network, which completes a denser pixel-wise depth map. The proposed architecture reconstructs the pixel-wise depth map, taking advantage of both the dense color features and sparse 3D spatial features. We applied the early fusion technique and ﬁne-tuned the {ResNet} model as the encoder. The designed Residual {UpProjection} block recovers the spatial resolution of the feature map and captures context within the depth map. We introduced a depth feature tensor which propagates context information from encoder blocks to decoder blocks. Our proposed method is evaluated on the large-scale indoor {NYUdepthV}2 and {KITTI} odometry datasets which outperforms the state-of-the-art single {RGB} image and depth fusion method. The proposed method is also evaluated on a reduced-resolution {KITTI} dataset which synthesizes the planar {LIDAR} and {RGB} image fusion.},
	eventtitle = {2019 {IEEE} Intelligent Transportation Systems Conference - {ITSC}},
	pages = {273--278},
	booktitle = {2019 {IEEE} Intelligent Transportation Systems Conference ({ITSC})},
	publisher = {{IEEE}},
	author = {Fu, Chen and Mertz, Christoph and Dolan, John M.},
	urldate = {2021-03-29},
	date = {2019-10},
	langid = {english},
	note = {6 citations (Semantic Scholar/{DOI}) [2021-03-29]},
}

@article{klingner_online_2021,
	title = {Online Performance Prediction of Perception {DNNs} by Multi-Task Learning With Depth Estimation},
	issn = {15580016},
	doi = {10.1109/TITS.2021.3054437},
	abstract = {Online performance prediction (or: observation) of deep neural networks ({DNNs}) in highly automated driving presents an unsolved task until now, as most {DNNs} are evaluated offline requiring datasets with ground truth labels. In practice, however, {DNN} performance depends on the used camera type, lighting and weather conditions, and on various other kinds of domain shift. Also, the input to {DNN}-based perception systems can be perturbed by adversarial attacks requiring means to detect these input perturbations. In this work we propose a method to mitigate these problems by a multi-task learning approach with monocular depth estimation as a secondary task, which enables us to predict the {DNN}'s performance for various other (primary) tasks by evaluating only the depth estimation task with a physical depth measurement provided, e.g., by a {LiDAR} sensor. We show the effectiveness of our method for the primary task of semantic segmentation using various training datasets, test datasets, model architectures, and input perturbations. Our method provides an effective way to predict (observe) the performance of {DNNs} for semantic segmentation even on a single-image basis and is transferable to other primary {DNN}-based perception tasks in a straightforward manner.},
	pages = {1--14},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	author = {Klingner, Marvin and Fingscheidt, Tim},
	date = {2021},
	keywords = {Estimation, Laser radar, Measurement, Neural networks, Performance prediction, Semantics, Task analysis, Training, deep learning, depth estimation, neural networks, self-supervised learning., semantic segmentation, ⚠️ Invalid {DOI}},
}

@article{jonas_bargman__saferchalmers_udrive_2017,
	title = {the {UDrive} dataset and key analysis results},
	url = {https://doi.org/10.26323/UDRIVE_D53.1},
	doi = {10.26323/UDRIVE},
	author = {Jonas Bärgman ( {SAFER}/Chalmers, Sweden) and Nicole van Nes ( {SWOV}, the Netherlands) and Michiel Christoph ({SWOV}, the Netherlands) and Reinier Jansen ( {SWOV}, the Netherlands) and Veerle Heijne ({TNO}, the Netherlands) and Oliver Carsten ( Leeds, United Kingdom) and Mandy Dotzauer ({DLR}, Germany) and Utech, Fabian ({DLR}, Germany) and Erik Svanberg ({SAFER}, Sweden) and Marta Pereira Cocron ({TUC}, Germany) and Fabio Forcolin ({SAFER}/Chalmers, Sweden) and Jordanka Kovaceva ({SAFER}/Chalmers, Sweden)},
	date = {2017},
	keywords = {⚠️ Invalid {DOI}},
}

@article{zhao_fusion_2020,
	title = {Fusion of 3D {LIDAR} and Camera Data for Object Detection in Autonomous Vehicle Applications},
	volume = {20},
	issn = {1530-437X, 1558-1748, 2379-9153},
	url = {https://ieeexplore.ieee.org/document/8957313/},
	doi = {10/gjvpkk},
	pages = {4901--4913},
	number = {9},
	journaltitle = {{IEEE} Sensors Journal},
	shortjournal = {{IEEE} Sensors J.},
	author = {Zhao, Xiangmo and Sun, Pengpeng and Xu, Zhigang and Min, Haigen and Yu, Hongkai},
	urldate = {2021-03-29},
	date = {2020-05-01},
	langid = {english},
	note = {13 citations (Semantic Scholar/{DOI}) [2021-03-29]},
}

@article{martin_bridging_2020,
	title = {Bridging the gap between an ontology and deep neural models by pattern mining},
	abstract = {A domain ontology ({DO}) is a machine-readable knowledge repository compatible with the popular knowledge graph ({KG}) format. An intriguing question is how to leverage a {DO} plus a {KG} in a neural learning process. We propose to use ontology-rooted graph patterns mined from a {DO}-compatible graph translation of the raw data as a vector for injecting some domain knowledge into the neural network. Such patterns represent a frequently occurring regularities in the data yet they are expressed in terms of the ontological entities (classes, properties, etc.) and reﬂect additional knowledge from the {KG}. Using them as an additional input to the learning process seems a promising way to guide it towards improved explainability, accuracy and convergence, as well as, in a more general vein, increase the generalization power of the neural models.},
	pages = {10},
	author = {Martin, Tomas and Diallo, Abdoulaye Banire and Valtchev, Petko and Lacroix, Rene},
	date = {2020},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{chen_augmenting_2019,
	title = {Augmenting Anomaly Detection for Autonomous Vehicles With Symbolic Rules},
	abstract = {My research investigates the issues in anomaly detection as applied to autonomous driving created by the incompleteness of training data. I address these issues through the use of a commonsense knowledge base, a predefined set of rules regarding driving behavior, and a means of updating the base set of rules as anomalies are detected. In order to explore this problem I have built a hardware platform that was used to evaluate existing anomaly detection developed within the lab and that will serve as an evaluation platform for future work in this area. The platform is based on the open-source {MIT} {RACECAR} project that integrates the most basic aspect of an driving autonomous vehicle – lidar, camera, accelerometer, and computer – onto the frame of an {RC} car. We created a set of rules regarding traffic light color transitions to test the car’s ability to navigate cones (which represent traffic light colors) and detect anomalies in the traffic light transition order. Anomalies regularly occurred in the car’s driving environment and its driving rules were updated as a consequence of the logged anomalies. The car was able to successfully navigate the course and the rules (plausible traffic light color transitions) were updated when repeated anomalies were seen.},
	pages = {54},
	author = {Chen, Tianye},
	date = {2019},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@article{jayasankar_survey_2021,
	title = {A survey on data compression techniques: From the perspective of data quality, coding schemes, data type and applications},
	volume = {33},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157818301101},
	doi = {10/gjvpkj},
	shorttitle = {A survey on data compression techniques},
	abstract = {Explosive growth of data in digital world leads to the requirement of efficient technique to store and transmit data. Due to limited resources, data compression ({DC}) techniques are proposed to minimize the size of data being stored or communicated. As {DC} concepts results to effective utilization of available storage area and communication bandwidth, numerous approaches were developed in several aspects. In order to analyze how {DC} techniques and its applications have evolved, a detailed survey on many existing {DC} techniques is carried out to address the current requirements in terms of data quality, coding schemes, type of data and applications. A comparative analysis is also performed to identify the contribution of reviewed techniques in terms of their characteristics, underlying concepts, experimental factors and limitations. Finally, this paper insight to various open issues and research directions to explore the promising areas for future developments.},
	pages = {119--140},
	number = {2},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	shortjournal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Jayasankar, Uthayakumar and Thirumal, Vengattaraman and Ponnurangam, Dhavachelvan},
	urldate = {2021-04-28},
	date = {2021-02-01},
	langid = {english},
	note = {50 citations (Semantic Scholar/{DOI}) [2021-05-04]},
	keywords = {Data compression, Data redundancy, Entropy encoding, Image compression, Information theory, Text compression, Transform coding},
}

@article{weber_framework_2019,
	title = {A framework for definition of logical scenarios for safety assurance of automated driving},
	volume = {20},
	issn = {1538-9588},
	url = {https://doi.org/10.1080/15389588.2019.1630827},
	doi = {10/ghpkst},
	abstract = {Objective: In order to introduce automated vehicles on public roads, it is necessary to ensure that these vehicles are safe to operate in traffic. One challenge is to prove that all physically possible variations of situations can be handled safely within the operational design domain of the vehicle. A promising approach to handling the set of possible situations is to identify a manageable number of logical scenarios, which provide an abstraction for object properties and behavior within the situations. These can then be transferred into concrete scenarios defining all parameters necessary to reproduce the situation in different test environments.Methods: This article proposes a framework for defining safety-relevant scenarios based on the potential collision between the subject vehicle and a challenging object, which forces the subject vehicle to depart from its planned course of action to avoid a collision. This allows defining only safety-relevant scenarios, which can directly be related to accident classification. The first criterion for defining a scenario is the area of the subject vehicle with which the object would collide. As a second criterion, 8 different positions around the subject vehicle are considered. To account for other relevant objects in the scenario, factors that influence the challenge for the subject vehicle can be added to the scenario. These are grouped as action constraints, dynamic occlusions, and causal chains.Results: By applying the proposed systematics, a catalog of base scenarios for a vehicle traveling on controlled-access highways has been generated, which can directly be linked to parameters in accident classification. The catalog serves as a basis for scenario classification within the {PEGASUS} project.Conclusions: Defining a limited number of safety-relevant scenarios helps to realize a systematic safety assurance process for automated vehicles. Scenarios are defined based on the point of the potential collision of a challenging object with the subject vehicle and its initial position. This approach allows defining scenarios for different environments and different driving states of the subject vehicle using the same mechanisms. A next step is the generation of logical scenarios for other driving states of the subject vehicle and for other traffic environments.},
	pages = {S65--S70},
	issue = {sup1},
	journaltitle = {Traffic Injury Prevention},
	author = {Weber, Hendrik and Bock, Julian and Klimke, Jens and Roesener, Christian and Hiller, Johannes and Krajewski, Robert and Zlocki, Adrian and Eckstein, Lutz},
	urldate = {2021-03-26},
	date = {2019-06-12},
	pmid = {31381437},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/15389588.2019.1630827
8 citations (Semantic Scholar/{DOI}) [2021-03-26]},
	keywords = {Automated driving, automated driving systems, safety assurance, safety evaluation, scenarios},
}

@article{thorn_framework_2018,
	title = {A Framework for Automated Driving System Testable Cases and Scenarios},
	pages = {180},
	author = {Thorn, Eric and Kimmel, Shawn and Chaka, Michelle},
	date = {2018},
	langid = {english},
	keywords = {⛔ No {DOI} found},
}

@inproceedings{tao_industrial_2019,
	location = {Xi'an, China},
	title = {On the Industrial Application of Combinatorial Testing for Autonomous Driving Functions},
	isbn = {978-1-72810-888-9},
	url = {https://ieeexplore.ieee.org/document/8728928/},
	doi = {10/ggh934},
	abstract = {The growing importance of automated and autonomous driving systems becomes more and more visible in the industrial domain as well as in research. Validation and veriﬁcation of autonomous driving functions is one of the grand challenges in autonomous vehicle development. Virtual validation using simulation has been widely discussed and proposed as a method to solve the challenge. Furthermore, scenario-based approaches have been considered as proper methods combined with virtual validation in order to identify critical scenarios. In this paper, we discuss a method for testing automated and autonomous driving functions using ontologies and combinatorial testing that is able to automate test case generation. Moreover, we report on the application of the method at the industrial level. There we depict the comprehensive application process from the construction of the ontology to test suite execution in detail. This case study shows that the proposed approach can be used for testing and validation of autonomous driving functions in practice.},
	eventtitle = {2019 {IEEE} International Conference on Software Testing, Verification and Validation Workshops ({ICSTW})},
	pages = {234--240},
	booktitle = {2019 {IEEE} International Conference on Software Testing, Verification and Validation Workshops ({ICSTW})},
	publisher = {{IEEE}},
	author = {Tao, Jianbo and Li, Yihao and Wotawa, Franz and Felbinger, Hermann and Nica, Mihai},
	urldate = {2021-05-01},
	date = {2019-04},
	langid = {english},
	note = {5 citations (Semantic Scholar/{DOI}) [2021-05-04]},
}

@article{mabkhot_ontology-enabled_2019,
	title = {An Ontology-Enabled Case-Based Reasoning Decision Support System for Manufacturing Process Selection},
	volume = {2019},
	issn = {1687-8434, 1687-8442},
	url = {https://www.hindawi.com/journals/amse/2019/2505183/},
	doi = {10/gjvpcx},
	abstract = {In nowadays industry 4.0 and changeable manufacturing context, designers and manufacturing engineers struggle to determine appropriate quick, accurate (with flawless quality), and cost-effective processes to design highly customized products to meet customer requirements. To determine manufacturing processes, the matching between product features, material characteristics, and process capabilities needs to be optimized. Finding such an optimized matching is usually referred to as manufacturing process selection ({MPS}), which is not an easy task because of the infinite combinations of product features, numerous material characteristics, and various manufacturing processes. Although problems associated with {MPS} have received considerable attention, semantic web technologies are still underexplored and their potential is still uncovered. Almost no previous study has considered combining case-based reasoning ({CBR}) with ontologies, a famous and powerful semantic web enabler, to achieve {MPS}. In this study, we developed a decision support system ({DSS}) for {MPS} based on ontology-enabled {CBR}. By applying automatic reasoning and similarity retrieving on an industrial case study, we show that ontologies enable process selection by determining competitive matching between product features, material characteristics, and process capabilities and by endorsing effective case retrieval.},
	pages = {1--18},
	journaltitle = {Advances in Materials Science and Engineering},
	shortjournal = {Advances in Materials Science and Engineering},
	author = {Mabkhot, Mohammed M. and Al-Samhan, Ali M. and Hidri, Lotfi},
	urldate = {2021-05-01},
	date = {2019-08-21},
	langid = {english},
}

@article{futia_integration_2020,
	title = {On the Integration of Knowledge Graphs into Deep Learning Models for a More Comprehensible {AI}—Three Challenges for Future Research},
	volume = {11},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/11/2/122},
	doi = {10/ggt5pv},
	abstract = {Deep learning models contributed to reaching unprecedented results in prediction and classiﬁcation tasks of Artiﬁcial Intelligence ({AI}) systems. However, alongside this notable progress, they do not provide human-understandable insights on how a speciﬁc result was achieved. In contexts where the impact of {AI} on human life is relevant (e.g., recruitment tools, medical diagnoses, etc.), explainability is not only a desirable property, but it is -or, in some cases, it will be soon-a legal requirement. Most of the available approaches to implement {eXplainable} Artiﬁcial Intelligence ({XAI}) focus on technical solutions usable only by experts able to manipulate the recursive mathematical functions in deep learning algorithms. A complementary approach is represented by symbolic {AI}, where symbols are elements of a lingua franca between humans and deep learning. In this context, Knowledge Graphs ({KGs}) and their underlying semantic technologies are the modern implementation of symbolic {AI}—while being less ﬂexible and robust to noise compared to deep learning models, {KGs} are natively developed to be explainable. In this paper, we review the main {XAI} approaches existing in the literature, underlying their strengths and limitations, and we propose neural-symbolic integration as a cornerstone to design an {AI} which is closer to non-insiders comprehension. Within such a general direction, we identify three speciﬁc challenges for future research—knowledge matching, cross-disciplinary explanations and interactive explanations.},
	pages = {122},
	number = {2},
	journaltitle = {Information},
	shortjournal = {Information},
	author = {Futia, Giuseppe and Vetrò, Antonio},
	urldate = {2021-05-01},
	date = {2020-02-22},
	langid = {english},
	note = {9 citations (Semantic Scholar/{DOI}) [2021-05-04]},
}

@article{riccio_testing_2020,
	title = {Testing machine learning based systems: a systematic mapping},
	volume = {25},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-020-09881-0},
	doi = {10/gjvpcv},
	shorttitle = {Testing machine learning based systems},
	abstract = {Objective: To identify the existing solutions for functional testing of {MLSs}, and classify them from three different perspectives: (1) the context of the problem they address, (2) their features, and (3) their empirical evaluation. To report demographic information about the ongoing research. To identify open challenges for future research.
Method: We conducted a systematic mapping study about testing techniques for {MLSs} driven by 33 research questions. We followed existing guidelines when defining our research protocol so as to increase the repeatability and reliability of our results.
Results: We identified 70 relevant primary studies, mostly published in the last years. We identified 11 problems addressed in the literature. We investigated multiple aspects of the testing approaches, such as the used/proposed adequacy criteria, the algorithms for test input generation, and the test oracles.
Conclusions: The most active research areas in {MLS} testing address automated scenario/input generation and test oracle creation. {MLS} testing is a rapidly growing and developing research area, with many open challenges, such as the generation of realistic inputs and the definition of reliable evaluation metrics and benchmarks.},
	pages = {5193--5254},
	number = {6},
	journaltitle = {Empirical Software Engineering},
	shortjournal = {Empir Software Eng},
	author = {Riccio, Vincenzo and Jahangirova, Gunel and Stocco, Andrea and Humbatova, Nargiz and Weiss, Michael and Tonella, Paolo},
	urldate = {2021-05-01},
	date = {2020-11},
	langid = {english},
}

@article{rajabli_software_2021,
	title = {Software Verification and Validation of Safe Autonomous Cars: A Systematic Literature Review},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9310181/},
	doi = {10/ghs2pz},
	shorttitle = {Software Verification and Validation of Safe Autonomous Cars},
	abstract = {Autonomous, or self-driving, cars are emerging as the solution to several problems primarily caused by humans on roads, such as accidents and trafﬁc congestion. However, those beneﬁts come with great challenges in the veriﬁcation and validation (V\&V) for safety assessment. In fact, due to the possibly unpredictable nature of Artiﬁcial Intelligence ({AI}), its use in autonomous cars creates concerns that need to be addressed using appropriate V\&V processes that can address trustworthy {AI} and safe autonomy. In this study, the relevant research literature in recent years has been systematically reviewed and classiﬁed in order to investigate the state-of-the-art in the software V\&V of autonomous cars. By appropriate criteria, a subset of primary studies has been selected for more in-depth analysis. The ﬁrst part of the review addresses certiﬁcation issues against reference standards, challenges in assessing machine learning, as well as general V\&V methodologies. The second part investigates more speciﬁc approaches, including simulation environments and mutation testing, corner cases and adversarial examples, fault injection, software safety cages, techniques for cyber-physical systems, and formal methods. Relevant approaches and related tools have been discussed and compared in order to highlight open issues and opportunities.},
	pages = {4797--4819},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Rajabli, Nijat and Flammini, Francesco and Nardone, Roberto and Vittorini, Valeria},
	urldate = {2021-05-01},
	date = {2021},
	langid = {english},
}

@article{matthaei_autonomous_2015,
	title = {Autonomous driving – a top-down-approach},
	volume = {63},
	issn = {0178-2312, 2196-677X},
	url = {https://www.degruyter.com/document/doi/10.1515/auto-2014-1136/html},
	doi = {10/gjvpct},
	abstract = {This paper presents a functional system architecture for an “autonomous vehicle” in the sense of a modular building block system. It is developed in a topdown approach based on the definition of the functional requirements for an autonomous vehicle and explicitly combines perception-based and localization-based approaches. Both the definition and the functional system architecture consider the aspects operating by the human being, mission accomplishment, map data, localization, environmental and self-perception as well as cooperation. The functional system architecture is developed in the context of the research project “Stadtpilot” at the Technische Universität Braunschweig.},
	number = {3},
	journaltitle = {at - Automatisierungstechnik},
	author = {Matthaei, Richard and Maurer, Markus},
	urldate = {2021-05-01},
	date = {2015-01-28},
	langid = {english},
}
@article{joo_autonomous_2020,
	title = {Autonomous Navigation Framework for Intelligent Robots Based on a Semantic Environment Modeling},
	volume = {10},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/9/3219},
	doi = {10/gjvpcw},
	abstract = {Humans have an innate ability of environment modeling, perception, and planning while simultaneously performing tasks. However, it is still a challenging problem in the study of robotic cognition. We address this issue by proposing a neuro-inspired cognitive navigation framework, which is composed of three major components: semantic modeling framework ({SMF}), semantic information processing ({SIP}) module, and semantic autonomous navigation ({SAN}) module to enable the robot to perform cognitive tasks. The {SMF} creates an environment database using Triplet Ontological Semantic Model ({TOSM}) and builds semantic models of the environment. The environment maps from these semantic models are generated in an on-demand database and downloaded in {SIP} and {SAN} modules when required to by the robot. The {SIP} module contains active environment perception components for recognition and localization. It also feeds relevant perception information to behavior planner for safely performing the task. The {SAN} module uses a behavior planner that is connected with a knowledge base and behavior database for querying during action planning and execution. The main contributions of our work are the development of the {TOSM}, integration of {SMF}, {SIP}, and {SAN} modules in one single framework, and interaction between these components based on the ﬁndings of cognitive science. We deploy our cognitive navigation framework on a mobile robot platform, considering implicit and explicit constraints for autonomous robot navigation in a real-world environment. The robotic experiments demonstrate the validity of our proposed framework.},
	pages = {3219},
	number = {9},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Joo, Sung-Hyeon and Manzoor, Sumaira and Rocha, Yuri Goncalves and Bae, Sang-Hyeon and Lee, Kwang-Hee and Kuc, Tae-Yong and Kim, Minsung},
	urldate = {2021-05-01},
	date = {2020-05-05},
	langid = {english},
}

@article{schlenoff_using_2003,
	title = {Using ontologies to aid navigation planning in autonomous vehicles},
	volume = {18},
	issn = {0269-8889, 1469-8005},
	url = {https://www.cambridge.org/core/product/identifier/S0269888904000050/type/journal_article},
	doi = {10/br3nzs},
	abstract = {We report the results of a first implementation demonstrating the use of an ontology to support reasoning about obstacles to improve the capabilities and performance of on-board route planning for autonomous vehicles. This is part of an overall effort to evaluate the performance of ontologies in different components of an autonomous vehicle within the 4D/{RCS} system architecture developed at {NIST}. Our initial focus has been on simple roadway driving scenarios where the controlled vehicle encounters potential obstacles in its path. As reported elsewhere [9], our approach is to develop an ontology of objects in the environment, in conjunction with rules for estimating the damage that would be incurred by collisions with different objects in different situations. Automated reasoning is used to estimate collision damage; this information is fed to the route planner to help it decide whether to plan to avoid the object. We describe the results of the first implementation that integrates the ontology, the reasoner and the planner. We describe our insights and lessons learned and discuss resulting changes to our approach.},
	pages = {243--255},
	number = {3},
	journaltitle = {The Knowledge Engineering Review},
	shortjournal = {The Knowledge Engineering Review},
	author = {Schlenoff, Craig and Balakirsky, Stephen and Uschold, Mike and Provine, Ron and Smith, Scott},
	urldate = {2021-05-01},
	date = {2003-09},
	langid = {english},
}

@inproceedings{klueck_using_2018,
	location = {Memphis, {TN}},
	title = {Using Ontologies for Test Suites Generation for Automated and Autonomous Driving Functions},
	isbn = {978-1-5386-9443-5},
	url = {https://ieeexplore.ieee.org/document/8539174/},
	doi = {10/gghcnx},
	abstract = {In this paper, we outline a general automated testing approach to be applied for veriﬁcation and validation of automated and autonomous driving functions. The approach makes use of ontologies of environment the system under test is interacting with. Ontologies are automatically converted into input models for combinatorial testing, which are used to generate test cases. The obtained abstract test cases are used to generate concrete test scenarios that provide the basis for simulation used to verify the functionality of the system under test. We discuss the general approach including its potential for automation in the automotive domain where there is growing need for sophisticated veriﬁcation based on simulation in case of automated and autonomous vehicles.},
	eventtitle = {2018 {IEEE} International Symposium on Software Reliability Engineering Workshops ({ISSREW})},
	pages = {118--123},
	booktitle = {2018 {IEEE} International Symposium on Software Reliability Engineering Workshops ({ISSREW})},
	publisher = {{IEEE}},
	author = {Klueck, Florian and Li, Yihao and Nica, Mihai and Tao, Jianbo and Wotawa, Franz},
	urldate = {2021-05-01},
	date = {2018-10},
	langid = {english},
}

@inproceedings{breaux_using_2005,
	location = {Big Island, {HI}, {USA}},
	title = {Using Ontology in Hierarchical Information Clustering},
	isbn = {978-0-7695-2268-5},
	url = {http://ieeexplore.ieee.org/document/1385462/},
	doi = {10/d5g3c4},
	abstract = {The tools to analyze and visualize information from multiple, heterogeneous sources have often relied on innovations in statistical methods. The results from purely statistical methods, however, overlook relevant semantic features present within natural language and text-based information. Emerging research in ontology languages (e.g. {RDF}, {RDFS}, {SUO}-{KIF}, and {OWL}) offers promising avenues for overcoming these limitations by leveraging existing and future libraries of meta-data and semantic mark-up. Using semantic features (e.g. hypernyms, meronyms, synonyms, etc.) encoded in ontology languages, methods such as keyword search and clustering can be augmented to analyze and visualize documents at conceptually richer levels. We present findings from a hierarchical clustering system modified for ontological indexing and run on a topic-centric test collection of documents each with fewer than 200 words. Our findings show that ontologies can impose a complete interpretation or subjective clustering onto a document set that is at least as good as meta-word search.},
	eventtitle = {38th Annual Hawaii International Conference on System Sciences},
	pages = {111b--111b},
	booktitle = {Proceedings of the 38th Annual Hawaii International Conference on System Sciences},
	publisher = {{IEEE}},
	author = {Breaux, T.D. and Reed, J.W.},
	urldate = {2021-05-01},
	date = {2005},
	langid = {english},
	note = {23 citations (Semantic Scholar/{DOI}) [2021-05-04]},
}

@article{li_ontology_2019,
	title = {Ontology Completion Using Graph Convolutional Networks},
	doi = {10/gjvpcr},
	abstract = {Many methods have been proposed to automatically extend knowledge bases, but the vast majority of these methods focus on ﬁnding plausible missing facts, and knowledge graph triples in particular. In this paper, we instead focus on automatically extending ontologies that are encoded as a set of existential rules. In particular, our aim is to ﬁnd rules that are plausible, but which cannot be deduced from the given ontology. To this end, we propose a graphbased representation of rule bases. Nodes of the considered graphs correspond to predicates, and they are annotated with vectors encoding our prior knowledge about the meaning of these predicates. The vectors may be obtained from external resources such as word embeddings or they could be estimated from the rule base itself. Edges connect predicates that co-occur in the same rule and their annotations reﬂect the types of rules in which the predicates co-occur. We then use a neural network model based on Graph Convolutional Networks ({GCNs}) to reﬁne the initial vector representation of the predicates, to obtain a representation which is predictive of which rules are plausible. We present experimental results that demonstrate the strong performance of this method.},
	pages = {17},
	author = {Li, Na and Bouraoui, Zied and Schockaert, Steven},
	date = {2019},
	langid = {english},
}

@inproceedings{jesenski_scalable_2020,
	title = {Scalable Generation of Statistical Evidence for the Safety of Automated Vehicles by the Use of Importance Sampling},
	doi = {10/gjvpcq},
	pages = {1--8},
	author = {Jesenski, Stefan and Tiemann, Nils and Stellet, Jan and Zollner, J.},
	date = {2020-09-20},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-05-04]},
}

@article{sleimi_generating_2016,
	title = {Generating Paraphrases from {DBPedia} using Deep Learning},
	doi = {10/gjvpcs},
	abstract = {Recent deep learning approaches to Natural Language Generation mostly rely on sequence-to-sequence models. In these approaches, the input is treated as a sequence whereas in most cases, input to generation usually is either a tree or a graph. In this paper, we describe an experiment showing how enriching a sequential input with structural information improves results and help support the generation of paraphrases.},
	pages = {5},
	author = {Sleimi, Amin and Gardent, Claire},
	date = {2016},
	langid = {english},
}

@inproceedings{stellet_formalisation_2019,
	title = {Formalisation and algorithmic approach to the automated driving validation problem},
	doi = {10/ggkp3b},
	abstract = {Automated driving road vehicles are to operate in an unstructured, public real-world environment. The openness of the operational design domain, the serious safety risk, the complexity of the system itself, as well as the regulatory situation pose a large challenge to the automotive industry. Thus, a strategy is necessary to ascertain the validity of such systems. An extensive formalisation of the problem and its root cause, the deductive gap, is provided in the authors' work [1] and described in a compact version in this paper. Thereto, the interdependent aspects purpose, context and re-alisation are detailed. This allows us to establish why deductive gaps between the required, the specified and the eventually implemented behaviour can occur. These gaps are caused by violations of underlying assumptions. Identifying such violated assumptions is the main goal of a novel algorithmic approach. Furthermore, the contributions and aspects left uncovered by normative regulations, i.e. {ISO} 26262 and {ISO} {PAS} 21448, are established.},
	author = {Stellet, Jan and Brade, Tino and Poddey, Alexander and Jesenski, Stefan and Branz, Wolfgang},
	date = {2019-06-09},
	note = {3 citations (Semantic Scholar/{DOI}) [2021-05-04]},
}

@article{peleg_onto-clustmethodology_2009,
	title = {Onto-clust—A methodology for combining clustering analysis and ontological methods for identifying groups of comorbidities for developmental disorders},
	volume = {42},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046408000804},
	doi = {10/ck5gvq},
	abstract = {Children with developmental disorders usually exhibit multiple developmental problems (comorbidities). Hence, such diagnosis needs to revolve on developmental disorder groups. Our objective is to systematically identify developmental disorder groups and represent them in an ontology. We developed a methodology that combines two methods (1) a literature-based ontology that we created, which represents developmental disorders and potential developmental disorder groups, and (2) clustering for detecting comorbid developmental disorders in patient data. The ontology is used to interpret and improve clustering results and the clustering results are used to validate the ontology and suggest directions for its development. We evaluated our methodology by applying it to data of 1175 patients from a child development clinic. We demonstrated that the ontology improves clustering results, bringing them closer to an expert generated gold-standard. We have shown that our methodology successfully combines an ontology with a clustering method to support systematic identification and representation of developmental disorder groups.},
	pages = {165--175},
	number = {1},
	journaltitle = {Journal of Biomedical Informatics},
	shortjournal = {Journal of Biomedical Informatics},
	author = {Peleg, Mor and Asbeh, Nuaman and Kuflik, Tsvi and Schertz, Mitchell},
	urldate = {2021-05-01},
	date = {2009-02-01},
	langid = {english},
	keywords = {Clustering, Comorbidity, Developmental disorders, Ontology},
}

@inproceedings{maedche_clustering_2002,
	title = {Clustering Ontology-Based Metadata in the Semantic Web},
	volume = {2431},
	isbn = {978-3-540-44037-6},
	doi = {10/fvvtzz},
	abstract = {The Semantic Web is an extension of the current web in which infor- mation is given well-defined meaning, better enabling computers and people to work in cooperation. Recently, different applications based on this vision have been designed, e.g. in the fields of knowledge management, community web por- tals, e-learning, multimedia retrieval, etc. It is obvious that the complex metadata descriptions generated on the basis of pre-defined ontologies serve as perfect in- put data for machine learning techniques. In this paper we propose an approach for clustering ontology-based metadata. Main contributions of this paper are the definition of a set of similarity measures for comparing ontology-based metadata and an application study using these measures within a hierarchical clustering algorithm.},
	eventtitle = {Lecture Notes Comput Sci},
	pages = {348--360},
	author = {Maedche, Alexander and Zacharias, Valentin},
	date = {2002-08-19},
}

@misc{stellet_iv19_formalisation_and_algorithmic_approach_to_the_ad_validation_problem_slidespdf_2019,
	title = {{IV}19\_Formalisation\_and\_algorithmic\_approach\_to\_the\_AD\_validation\_problem\_slides.pdf},
	author = {Stellet, Jan and Brade, Tino and Poddey, Alexander and Jesenski, Stefan and Branz, Wolfgang},
	date = {2019-08-03},
}

@collection{studer_semantic_2007,
	location = {Berlin},
	title = {Semantic Web services: concepts, technologies, and applications},
	isbn = {978-3-540-70893-3},
	shorttitle = {Semantic Web services},
	pagetotal = {406},
	publisher = {Springer},
	editor = {Studer, Rudi},
	date = {2007},
	langid = {english},
	note = {{OCLC}: 600464014},
}

@book{pegasus_pegasus_2019,
	title = {{PEGASUS} {METHOD}},
	abstract = {{PEGASUS}

The dream of many car drivers appears like this: while driving, one simply switches to the autopilot, sits back, reads, … However, until these automation systems can actually be used on the roads, a million times, there are still many questions that have to be clarified. In particular: what are the requirements for self-driving vehicles? How can the safety and reliability of these systems be proven?

And not to forget: as nice as the future vision of self-driving cars might be – without people behind the wheel, it will not work. In particular, the transfer of responsibility from the driver to the automated system comes with high demands, since the humans no longer have to continuously monitor their driving task and can devote themselves to other activities. But what role will the human factor play in the future? What does technology have to guarantee? And how can optimally shape the interplay between humans and technology? In such situations, there is an enormous demand for research, when it comes to bringing highly-automated vehicles, quickly and safely on the market.

In order for such functions to be approved, new and standardized quality standards and methods must be developed through the close cooperation between the research and industry fields. This is what the {PEGASUS} joint project stands for: project for the establishment of generally accepted quality criteria, tools and methods as well as scenarios and situations for the release of highly-automated driving functions. The objective is to develop a procedure for the testing of automated driving functions, in order to facilitate the rapid implementation of automated driving into practice.},
	author = {{PEGASUS} and Mazzega, Jens and Lipinski, Daniel and Eberle, Ulrich and Schittenhelm, Helmut and Wachenfeld, Walther},
	date = {2019-05-14},
}

@report{leitner_enable-s3_2019,
	title = {{ENABLE}-S3 Summary of Results},
	author = {Leitner, Andrea and Akkermann, Arnold},
	date = {2019},
}

@article{riedmaier_survey_2020,
	title = {Survey on Scenario-Based Safety Assessment of Automated Vehicles},
	volume = {8},
	issn = {21693536},
	doi = {10/ggv7ng},
	abstract = {When will automated vehicles come onto the market? This question has puzzled the automotive industry and society for years. The technology and its implementation have made rapid progress over the last decade, but the challenge of how to prove the safety of these systems has not yet been solved. Since a market launch without proof of safety would neither be accepted by society nor by legislators, much time and many resources have been invested into safety assessment in recent years in order to develop new approaches for an efficient assessment. This paper therefore provides an overview of various approaches, and gives a comprehensive survey of the so-called scenario-based approach. The scenario-based approach is a promising method, in which individual traffic situations are typically tested by means of virtual simulation. Since an infinite number of different scenarios can theoretically occur in real-world traffic, even the scenario-based approach leaves the question unanswered as to how to break these down into a finite set of scenarios, and find those which are representative in order to render testing more manageable. This paper provides a comprehensive literature review of related safety-assessment publications that deal precisely with this question. Therefore, this paper develops a novel taxonomy for the scenario-based approach, and classifies all literature sources. Based on this, the existing methods will be compared with each other and, as one conclusion, the alternative concept of formal verification will be combined with the scenario-based approach. Finally, future research priorities are derived.},
	pages = {87456--87477},
	journaltitle = {{IEEE} Access},
	author = {Riedmaier, Stefan and Ponn, Thomas and Ludwig, Dieter and Schick, Bernhard and Diermeyer, Frank},
	date = {2020},
	note = {10 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Automated vehicles, {ToDo}, autonomous vehicles, data analysis, formal verification, intelligent vehicles, key performance indicators, simulation, vehicle safety},
}

@article{nutzel_ai-based_2018,
	title = {{AI}-based movement planning for autonomous and teleoperated vehicles including the development of a simulation environment and an intelligent agent},
	abstract = {Automated and teleoperated driving has recently faced increased interest in society and science. The latest developments and improvements in the fields of Artificial Intelligence and Machine Learning have resulted in new solutions that offer decisive advantages in the subject compared to conventional software solutions. Movement planning is considered a major task and difficulty in autonomous driving. Neural Networks offer a solution to predict the surrounding traffic and generate maneuver plans accordingly, while parallelly allowing a significant level of generalization to master unseen situations. Teleoperated driving, as the link between manual and autonomous driving, also benefits from precise movement planning. A machine computed trajectory helps the operator to compensate uncertainties caused by latency fluctuations in the mobile connection.},
	pages = {1--120},
	author = {Nützel, Thomas},
	date = {2018},
	keywords = {Autonomous driving, Autonomous vehicles, Neural Ne, {ToDo}},
}

@legislation{bundesregierung_entwurf_2021,
	title = {Entwurf eines Gesetzes zur Änderung des Straßenverkehrsgesetzes und des Pflichtversicherungsgesetzes – Gesetz zum autonomen Fahren},
	url = {https://www.bmvi.de/SharedDocs/DE/Anlage/Gesetze/Gesetze-19/gesetz-aenderung-strassenverkehrsgesetz-pflichtversicherungsgesetz-autonomes-fahren.pdf?__blob=publicationFile},
	editora = {Bundesregierung},
	editoratype = {collaborator},
	date = {2021-02-08},
}

@article{meir_are_2015,
	title = {Are child-pedestrians able to identify hazardous traffic situations? Measuring their abilities in a virtual reality environment},
	volume = {80},
	issn = {18791042},
	doi = {10/f7svst},
	abstract = {Background: Child-pedestrians are more prone to fail in identifying hazardous situations. Aiming to better understand the development of hazard-perception abilities in dynamic road situations we examined participants' hazard detection abilities in a virtual environment. Method: Experienced-adult participants and child-pedestrians observed typical road crossing related scenarios from a pedestrian's point of view and engaged in a hazard detection task. Results: Consistent with our hypotheses, less instances of obscured field of view by parked vehicles were reported as hazardous by 7-9-year-olds, who were also prone to linger more in identifying situations depicting field of view partially obscured by parked vehicles compared to all other age groups. Reports of obscured field of view by road curvature as hazardous increased with age. Conclusions: Understanding child-pedestrians' shortcomings in evaluating traffic situations contribute to the effort of producing intervention techniques which may increase their attentiveness toward potential hazards and lead toward reduction in their over-involvement in crashes.},
	pages = {33--40},
	journaltitle = {Safety Science},
	author = {Meir, Anat and Oron-Gilad, Tal and Parmet, Yisrael},
	urldate = {2021-02-09},
	date = {2015-12-01},
	note = {Publisher: Elsevier
36 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {\#nosource, Child pedestrian, Hazard perception, Road crossing, Simulation},
}

@inproceedings{caveney_cooperative_2012,
	title = {Cooperative driving: Beyond V2V as an {ADAS} sensor},
	isbn = {978-1-4673-2119-8},
	doi = {10/gjjg26},
	abstract = {Vehicle-to-Vehicle (V2V) communication systems utilize wireless communications for shared sensing between vehicles. This paper discusses how V2V systems could be utilized, beyond shared sensing, for shared decision making between cooperative vehicles. We propose distributed receding horizon control ({DRHC}) as an appropriate mechanism for scalable, shared decision making. Two automated driving applications, platooning and cooperative merging, illustrate the use of essential enabling technologies, including geo-spatial positions, digital road maps, collision avoidance, and path prediction, and how each is incorporated through our {DRHC}-centric framework. At the core of the framework is a four-task logic that allows partially-synchronous execution of local, computationally-efficient, optimization problems on board each vehicle. © 2012 {IEEE}.},
	booktitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Caveney, Derek and Dunbar, William B.},
	date = {2012},
	note = {24 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {\#nosource},
}

@inproceedings{caveney_vehicular_2009,
	title = {Vehicular path prediction for cooperative driving applications through digital map and dynamic vehicle model fusion},
	isbn = {978-1-4244-2515-0},
	doi = {10/bhd66s},
	abstract = {This paper investigates the fusion of digital maps and dynamical models to accurately predict the future path of a vehicle. This path prediction is generated by a vehicle for sharing with other vehicles through wireless communication channels. This sharing is an enabler of Vehicle-to-Vehicle (V2V) cooperative systems. In particular, this work looks at cooperative safety and comfort systems, which benefit from accurate path predictions for collision avoidance and coordination with other vehicles. This work proposes that digital map information is valuable in giving a nominal estimate of the future path ofthe vehicle, and that this estimate can be augmented with real-time vehicle information, such as wheel speeds, yaw rates, and accelerations. Furthermore, dynamic short-term situations such as lane changes can be cleanly incorporated into the vehicular path prediction. Depending on the driving environment, the sensor quality, the map accuracy, and the map matching precision, this paper shows that this fusion has the potential to provide where-in-lane path predictions. © 2009 {IEEE}.},
	booktitle = {{IEEE} Vehicular Technology Conference},
	author = {Caveney, Derek},
	date = {2009},
	note = {{ISSN}: 15502252
5 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {\#nosource},
}

@article{jia_enhanced_2016,
	title = {Enhanced cooperative car-following traffic model with the combination of V2V and V2I communication},
	issn = {01912615},
	doi = {10/f8x74z},
	abstract = {Vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication are emerging components of intelligent transport systems ({ITS}) based on which vehicles can drive in a cooperative way and, hence, significantly improve traffic flow efficiency. However, due to the high vehicle mobility, the unreliable vehicular communications such as packet loss and transmission delay can impair the performance of the cooperative driving system ({CDS}). In addition, the downstream traffic information collected by roadside sensors in the V2I communication may introduce measurement errors, which also affect the performance of the {CDS}. The goal of this paper is to bridge the gap between traffic flow modelling and communication approaches in order to build up better cooperative traffic systems. To this end, we aim to develop an enhanced cooperative microscopic (car-following) traffic model considering V2V and V2I communication (or V2X for short), and investigate how vehicular communications affect the vehicle cooperative driving, especially in traffic disturbance scenarios. For these purposes, we design a novel consensus-based vehicle control algorithm for the {CDS}, in which not only the local traffic flow stability is guaranteed, but also the shock waves are supposed to be smoothed. The {IEEE} 802.11p, the defacto vehicular networking standard, is selected as the communication protocols, and the roadside sensors are deployed to collect the average speed in the targeted area as the downstream traffic reference. Specifically, the imperfections of vehicular communication as well as the measured information noise are taken into account. Numerical results show the efficiency of the proposed scheme. This paper attempts to theoretically investigate the relationship between vehicular communications and cooperative driving, which is needed for the future deployment of both connected vehicles and infrastructure (i.e. V2X).},
	journaltitle = {Transportation Research Part B: Methodological},
	author = {Jia, Dongyao and Ngoduy, Dong},
	date = {2016},
	note = {82 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {\#nosource, Connected vehicle, Consensus control, Cooperative driving, Traffic flow stability, V2I communication, V2V communication},
}

@inproceedings{chandramohan_machine_2019,
	title = {Machine Learning for Cooperative Driving in a Multi-Lane Highway Environment},
	isbn = {978-1-72810-117-0},
	doi = {10/gjjg27},
	abstract = {Most of the research in automated driving currently involves using the on-board sensors on the vehicle to collect information regarding surrounding vehicles to maneuver around them. In this paper we discuss how information communicated through vehicular networking can be used for controlling an autonomous vehicle in a multi-lane highway environment. A driving algorithm is designed using deep Q learning, a type of reinforcement learning. In order to train and test driving algorithms, we deploy a simulated traffic system, using {SUMO} (Simulation of Urban Mobility). The performance of the driving algorithm is tested for perfect knowledge regarding surrounding vehicles. Furthermore, the impact of limited communication range and random packet loss is investigated. Currently the performance of the driving algorithm is far from ideal with the collision ratios being quite high. We propose directions for additional research to improve the performance of the algorithm.},
	booktitle = {{IFIP} Wireless Days},
	author = {Chandramohan, Aashik and Poel, Mannes and Meijerink, Bernd and Heijenk, Geert},
	date = {2019},
	note = {{ISSN}: 2156972X
1 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {\#nosource, Autonomous driving, Cooperative driving, Highway environment, Q learning, Reinforcement Learning, {SUMO}},
}

@inproceedings{campolo_towards_2018,
	title = {Towards 5G Network Slicing for the V2X Ecosystem},
	isbn = {978-1-5386-4633-5},
	doi = {10/gjjg25},
	abstract = {The automotive vertical market is currently undergoing key technological transformations as the number of connected and more autonomous vehicles grows, thus realizing the Vehicle-to-Everything (V2X) communication ecosystem. Such a revolution raises unprecedented challenges to the {ICT} players that have to guarantee ultra-low latency and ultra-high reliable connectivity under high-mobility and high-density conditions. By allowing an operator to flexibly provide dedicated logical networks with customer-specific (virtualized) functionalities over a common physical infrastructure, network slicing candidates itself as a prominent solution to support V2X over upcoming programmable and softwarized 5G systems. In this paper, we share our vision about V2X network slicing, by pinpointing key requirements and providing a set of design guidelines, aligned with ongoing 3GPP standard specifications and network softwarization directions.},
	pages = {303--307},
	booktitle = {2018 4th {IEEE} Conference on Network Softwarization and Workshops, {NetSoft} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Campolo, Claudia and Molinaro, Antonella and Iera, Antonio and Fontes, Ramon R. and Rothenberg, Christian E.},
	date = {2018-09-10},
	note = {28 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {\#nosource, 5G, Network slicing, Network softwarization, V2X},
}

@article{wang_networking_2019,
	title = {Networking and Communications in Autonomous Driving: A Survey},
	volume = {21},
	issn = {1553877X},
	doi = {10/gfsjps},
	abstract = {The development of light detection and ranging, Radar, camera, and other advanced sensor technologies inaugurated a new era in autonomous driving. However, due to the intrinsic limitations of these sensors, autonomous vehicles are prone to making erroneous decisions and causing serious disasters. At this point, networking and communication technologies can greatly make up for sensor deficiencies, and are more reliable, feasible and efficient to promote the information interaction, thereby improving autonomous vehicle's perception and planning capabilities as well as realizing better vehicle control. This paper surveys the networking and communication technologies in autonomous driving from two aspects: intra-and inter-vehicle. The intra-vehicle network as the basis of realizing autonomous driving connects the on-board electronic parts. The inter-vehicle network is the medium for interaction between vehicles and outside information. In addition, we present the new trends of communication technologies in autonomous driving, as well as investigate the current mainstream verification methods and emphasize the challenges and open issues of networking and communications in autonomous driving.},
	pages = {1243--1274},
	number = {2},
	journaltitle = {{IEEE} Communications Surveys and Tutorials},
	author = {Wang, Jiadai and Liu, Jiajia and Kato, Nei},
	date = {2019-04-01},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
105 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {\#nosource, Autonomous driving, inter-vehicle network, intra-vehicle network, survey, verification method},
}

@misc{ravi_kiran_deep_2020,
	title = {Deep reinforcement learning for autonomous driving: A survey},
	abstract = {With the development of deep representation learning, the domain of reinforcement learning ({RL}) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning ({DRL}) algorithms, provides a taxonomy of automated driving tasks where (D){RL} methods have been employed, highlights the key challenges algorithmically as well as in terms of deployment of real world autonomous driving agents, the role of simulators in training agents, and finally methods to evaluate, test and robustifying existing solutions in {RL} and imitation learning.},
	author = {Ravi Kiran, B. and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Al Sallab, Ahmad A. and Yogamani, Senthil and Pérez, Patrick},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2002.00444},
	note = {Publication Title: {arXiv}},
	keywords = {\#nosource, Autonomous driving, Controller learning, Deep reinforcement learning, Imitation learning, Inverse reinforcement learning, Motion planning, Safe reinforcement learning, Trajectory optimisation},
}

@article{bhatt_smart_2019,
	title = {Smart Traffic Sign Boards ({STSB}) for Smart Cities},
	doi = {10/gjqr99},
	abstract = {Ubiquitous nature of smart cities requires multiple technologies to be implemented in this area. To develop the smart cities in practice, there is huge need of “Smart Traffic Management”. Smart Traffic Management is a system to monitor and control the traffic signals using sensors to regulate the flow of traffic and to avoid the congestion for smooth flow of traffic. Prioritizing the traffic like ambulance, police etc. is also one application comes under smart traffic management. Traffic sign board plays important role to make the traffic in shape and to control and manage the traffic on roads. Many at times the driver misses the sign boards while driving due to various reasons like insufficient light, fog, rain, traffic etc. In this paper, a framework of the Smart Traffic Sign Boards ({STSB}) is proposed, which can communicate with the system deployed in all the vehicles to make the drivers of those vehicles aware of speed breakers, speed limits, schools, or ‘U’ turn ahead, etc. beforehand, to avoid the mishap due to sudden appearing of such unusual features of the road during the road journey.},
	pages = {4},
	author = {Bhatt, Dr Devershi Pallavi and Tiwari, Manish},
	date = {2019},
	langid = {english},
}

@article{bijelic_seeing_2019,
	title = {Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather},
	doi = {10/ghbcsp},
	abstract = {The fusion of multimodal sensor streams, such as camera, lidar, and radar measurements, plays a critical role in object detection for autonomous vehicles, which base their decision making on these inputs. While existing methods exploit redundant information in good environmental conditions, they fail in adverse weather where the sensory streams can be asymmetrically distorted. These rare “edgecase” scenarios are not represented in available datasets, and existing fusion architectures are not designed to handle them. To address this challenge we present a novel multimodal dataset acquired in over 10,000 km of driving in northern Europe. Although this dataset is the ﬁrst large multimodal dataset in adverse weather, with 100k labels for lidar, camera, radar, and gated {NIR} sensors, it does not facilitate training as extreme weather is rare. To this end, we present a deep fusion network for robust fusion without a large corpus of labeled training data covering all asymmetric distortions. Departing from proposal-level fusion, we propose a single-shot model that adaptively fuses features, driven by measurement entropy. We validate the proposed method, trained on clean data, on our extensive validation dataset. Code and data are available here https:// github.com/princeton-computational-imaging/ {SeeingThroughFog}.},
	pages = {11},
	author = {Bijelic, Mario and Gruber, Tobias and Mannan, Fahim and Kraus, Florian and Ritter, Werner and Dietmayer, Klaus and Heide, Felix},
	date = {2019},
	langid = {english},
}

@article{jang_smart_2010,
	title = {Smart Roadside Server for Driver Assistance and Safety Warning: Framework and Applications},
	doi = {10/c3tvht},
	abstract = {In many countries, various in-vehicle Telematics systems are already available, while new systems are being currently designed. This paper introduces a smart roadside server for driver assistance and traffic safety warning. In recent world wide projects, roadside equipment is used for wider applications and becomes a server with more intelligent functions. This paper introduces a concept and framework of smart roadside server in various road systems using Telematics system. We suggest two service and application models in signalized intersection and {SMART} highway. This system models provide a vehicular safety service to a vehicle terminal of a vehicle located on a road within a service area from a smart roadside server. The smart roadside server is useful for driver assistance and warning by Telematics devices.},
	pages = {5},
	author = {Jang, {JeongAh} and Kim, {HyunSuk} and Cho, {HanByeog}},
	date = {2010},
	langid = {english},
}

@article{jurgenson_harnessing_2019,
	title = {Harnessing Reinforcement Learning for Neural Motion Planning},
	doi = {10/gjrr2p},
	abstract = {Motion planning is an essential component in most of today’s robotic applications. In this work, we consider the learning setting, where a set of solved motion planning problems is used to improve the efﬁciency of motion planning on different, yet similar problems. This setting is important in applications with rapidly changing environments such as in e-commerce, among others. We investigate a general deep learning based approach, where a neural network is trained to map an image of the domain, the current robot state, and a goal robot state to the next robot state in the plan. We focus on the learning algorithm, and compare supervised learning methods with reinforcement learning ({RL}) algorithms. We ﬁrst establish that supervised learning approaches are inferior in their accuracy due to insufﬁcient data on the boundary of the obstacles, an issue that {RL} methods mitigate by actively exploring the domain. We then propose a modiﬁcation of the popular {DDPG} {RL} algorithm that is tailored to motion planning domains, by exploiting the known model in the problem and the set of solved plans in the data. We show that our algorithm, dubbed {DDPG}-{MP}, signiﬁcantly improves the accuracy of the learned motion planning policy. Finally, we show that given enough training data, our method can plan signiﬁcantly faster on novel domains than off-the-shelf sampling based motion planners. Results of our experiments are shown in https://youtu.be/{wHQ}4Y4mBRb8.},
	pages = {10},
	author = {Jurgenson, Tom and Tamar, Aviv},
	date = {2019},
	langid = {english},
}

@article{pusse_hybrid_2019,
	title = {Hybrid Online {POMDP} Planning and Deep Reinforcement Learning for Safer Self-Driving Cars},
	doi = {10/gjrr2k},
	abstract = {The problem of pedestrian collision-free navigation of self-driving cars modeled as a partially observable Markov decision process can be solved with either deep reinforcement learning or approximate {POMDP} planning. However, it is not known whether some hybrid approach that combines advantages of these fundamentally different solution categories could be superior to them in this context. This paper presents the ﬁrst hybrid solution {HyLEAP} for collision-free navigation of self-driving cars together with a comparative experimental performance evaluation over the ﬁrst benchmark {OpenDSCTS} of simulated car-pedestrian accident scenarios based on the major German in-depth road accident study {GIDAS}. Our experiments revealed that {HyLEAP} can outperform each of its integrated state of the art methods for approximate {POMDP} planning and deep reinforcement learning in most {GIDAS} accident scenarios regarding safety, while they appear to be equally competitive regarding smoothness of driving and time to goal on average.},
	pages = {8},
	author = {Pusse, Florian and Klusch, Matthias},
	date = {2019},
	langid = {english},
}

@article{rabe_dense_2010,
	title = {Dense, Robust, and Accurate Motion Field Estimation from Stereo Image Sequences in Real-time},
	doi = {10/bpr5g7},
	abstract = {In this paper a novel approach for estimating the three dimensional motion ﬁeld of the visible world from stereo image sequences is proposed. This approach combines dense variational optical ﬂow estimation, including spatial regularization, with Kalman ﬁltering for temporal smoothness and robustness. The result is a dense, robust, and accurate reconstruction of the three-dimensional motion ﬁeld of the current scene that is computed in real-time. Parallel implementation on a {GPU} and an {FPGA} yields a vision-system which is directly applicable in realworld scenarios, like automotive driver assistance systems or in the ﬁeld of surveillance. Within this paper we systematically show that the proposed algorithm is physically motivated and that it outperforms existing approaches with respect to computation time and accuracy.},
	pages = {14},
	author = {Rabe, Clemens and Muller, Thomas and Wedel, Andreas and Franke, Uwe},
	date = {2010},
	langid = {english},
}

@incollection{hutchison_6d-vision_2005,
	location = {Berlin, Heidelberg},
	title = {6D-Vision: Fusion of Stereo and Motion for Robust Environment Perception},
	volume = {3663},
	isbn = {978-3-540-28703-2 978-3-540-31942-9},
	url = {http://link.springer.com/10.1007/11550518_27},
	shorttitle = {6D-Vision},
	abstract = {Obstacle avoidance is one of the most important challenges for mobile robots as well as future vision based driver assistance systems. This task requires a precise extraction of depth and the robust and fast detection of moving objects. In order to reach these goals, this paper considers vision as a process in space and time. It presents a powerful fusion of depth and motion information for image sequences taken from a moving observer. 3D-position and 3D-motion for a large number of image points are estimated simultaneously by means of Kalman-Filters. There is no need of prior error-prone segmentation. Thus, one gets a rich 6D representation that allows the detection of moving obstacles even in the presence of partial occlusion of foreground or background.},
	pages = {216--223},
	booktitle = {Pattern Recognition},
	publisher = {Springer Berlin Heidelberg},
	author = {Franke, Uwe and Rabe, Clemens and Badino, Hernán and Gehrig, Stefan},
	editor = {Kropatsch, Walter G. and Sablatnig, Robert and Hanbury, Allan},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2021-04-21},
	date = {2005},
	langid = {english},
	doi = {10.1007/11550518_27},
	note = {Series Title: Lecture Notes in Computer Science},
}

@article{yu_path_2020,
	title = {The Path Planning of Mobile Robot by Neural Networks and Hierarchical Reinforcement Learning},
	volume = {14},
	issn = {1662-5218},
	url = {https://www.frontiersin.org/article/10.3389/fnbot.2020.00063/full},
	doi = {10/gjrr2n},
	abstract = {Existing mobile robots cannot complete some functions. To solve these problems, which include autonomous learning in path planning, the slow convergence of path planning, and planned paths that are not smooth, it is possible to utilize neural networks to enable to the robot to perceive the environment and perform feature extraction, which enables them to have a ﬁtness of environment to state action function. By mapping the current state of these actions through Hierarchical Reinforcement Learning ({HRL}), the needs of mobile robots are met. It is possible to construct a path planning model for mobile robots based on neural networks and {HRL}. In this article, the proposed algorithm is compared with different algorithms in path planning. It underwent a performance evaluation to obtain an optimal learning algorithm system. The optimal algorithm system was tested in different environments and scenarios to obtain optimal learning conditions, thereby verifying the effectiveness of the proposed algorithm. Deep Deterministic Policy Gradient ({DDPG}), a path planning algorithm for mobile robots based on neural networks and hierarchical reinforcement learning, performed better in all aspects than other algorithms. Speciﬁcally, when compared with Double Deep Q-Learning ({DDQN}), {DDPG} has a shorter path planning time and a reduced number of path steps. When introducing an inﬂuence value, this algorithm shortens the convergence time by 91\% compared with the Q-learning algorithm and improves the smoothness of the planned path by 79\%. The algorithm has a good generalization effect in different scenarios. These results have signiﬁcance for research on guiding, the precise positioning, and path planning of mobile robots.},
	pages = {63},
	journaltitle = {Frontiers in Neurorobotics},
	shortjournal = {Front. Neurorobot.},
	author = {Yu, Jinglun and Su, Yuancheng and Liao, Yifan},
	urldate = {2021-04-21},
	date = {2020-10-02},
	langid = {english},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-04-21]},
}

@article{abdelwahed_solving_2020,
	title = {Solving the motion planning problem using learning experience through case-based reasoning and machine learning algorithms},
	volume = {11},
	issn = {20904479},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2090447919301431},
	doi = {10/gjrr2m},
	abstract = {This article presents two novel methodologies for solving the motion planning problem through retained experience. Both approaches employ {AI}’s case-based reasoning ({CBR}) technique. Case-based reasoning is an expert system development methodology which reuses past solutions to solve new problems. The ﬁrst approach uses {CBR} to retain K similar cases to solve the motion planning problem by merging those solutions into a set. Afterwards, it picks from this set based on a heuristic function to assemble a ﬁnal solution. Regarding the second approach, it employs the retained K similar cases differently. It uses those solution to build a graph which can be queried using traditional graph search algorithms. Results prove the success of such approaches concerning solution quality and success rate compared to different experience-based algorithms. Such utilization for {CBR} systems develops new research directions for building systems that can solve {NP} problems based on retained experiences exclusively.},
	pages = {133--142},
	number = {1},
	journaltitle = {Ain Shams Engineering Journal},
	shortjournal = {Ain Shams Engineering Journal},
	author = {Abdelwahed, Mustafa F. and Mohamed, Amr E. and Saleh, Mohamed Aly},
	urldate = {2021-04-21},
	date = {2020-03},
	langid = {english},
	note = {2 citations (Semantic Scholar/{DOI}) [2021-04-21]},
}

@incollection{vedaldi_pip_2020,
	location = {Cham},
	title = {{PiP}: Planning-Informed Trajectory Prediction for Autonomous Driving},
	volume = {12366},
	isbn = {978-3-030-58588-4 978-3-030-58589-1},
	url = {http://link.springer.com/10.1007/978-3-030-58589-1_36},
	shorttitle = {{PiP}},
	abstract = {It is critical to predict the motion of surrounding vehicles for self-driving planning, especially in a socially compliant and ﬂexible way. However, future prediction is challenging due to the interaction and uncertainty in driving behaviors. We propose planning-informed trajectory prediction ({PiP}) to tackle the prediction problem in the multi-agent setting. Our approach is diﬀerentiated from the traditional manner of prediction, which is only based on historical information and decoupled with planning. By informing the prediction process with the planning of the ego vehicle, our method achieves the state-of-the-art performance of multi-agent forecasting on highway datasets. Moreover, our approach enables a novel pipeline which couples the prediction and planning, by conditioning {PiP} on multiple candidate trajectories of the ego vehicle, which is highly beneﬁcial for autonomous driving in interactive scenarios.},
	pages = {598--614},
	booktitle = {Computer Vision – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Song, Haoran and Ding, Wenchao and Chen, Yuxuan and Shen, Shaojie and Wang, Michael Yu and Chen, Qifeng},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	urldate = {2021-04-21},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-58589-1_36},
	note = {Series Title: Lecture Notes in Computer Science},
}

@software{joanna_chjoannasimgan-simple_2020,
	title = {{CHJoanna}/{SimGan}-simple},
	url = {https://github.com/CHJoanna/SimGan-simple},
	abstract = {Contribute to {CHJoanna}/{SimGan}-simple development by creating an account on {GitHub}.},
	author = {Joanna},
	urldate = {2021-04-21},
	date = {2020-10-13},
	note = {original-date: 2017-11-14T03:42:07Z},
}

@article{shahian_jahromi_real-time_2019,
	title = {Real-Time Hybrid Multi-Sensor Fusion Framework for Perception in Autonomous Vehicles},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6833089/},
	doi = {10/ggch3b},
	abstract = {There are many sensor fusion frameworks proposed in the literature using different sensors and fusion methods combinations and configurations. More focus has been on improving the accuracy performance; however, the implementation feasibility of these frameworks in an autonomous vehicle is less explored. Some fusion architectures can perform very well in lab conditions using powerful computational resources; however, in real-world applications, they cannot be implemented in an embedded edge computer due to their high cost and computational need. We propose a new hybrid multi-sensor fusion pipeline configuration that performs environment perception for autonomous vehicles such as road segmentation, obstacle detection, and tracking. This fusion framework uses a proposed encoder-decoder based Fully Convolutional Neural Network ({FCNx}) and a traditional Extended Kalman Filter ({EKF}) nonlinear state estimator method. It also uses a configuration of camera, {LiDAR}, and radar sensors that are best suited for each fusion method. The goal of this hybrid framework is to provide a cost-effective, lightweight, modular, and robust (in case of a sensor failure) fusion system solution. It uses {FCNx} algorithm that improve road detection accuracy compared to benchmark models while maintaining real-time efficiency that can be used in an autonomous vehicle embedded computer. Tested on over 3K road scenes, our fusion algorithm shows better performance in various environment scenarios compared to baseline benchmark networks. Moreover, the algorithm is implemented in a vehicle and tested using actual sensor data collected from a vehicle, performing real-time environment perception.},
	number = {20},
	journaltitle = {Sensors (Basel, Switzerland)},
	shortjournal = {Sensors (Basel)},
	author = {Shahian Jahromi, Babak and Tulabandhula, Theja and Cetin, Sabri},
	urldate = {2021-04-19},
	date = {2019-10-09},
	pmid = {31600922},
	pmcid = {PMC6833089},
}

@article{yin_rall_2021,
	title = {{RaLL}: End-to-end Radar Localization on Lidar Map Using Differentiable Measurement Model},
	url = {http://arxiv.org/abs/2009.07061},
	doi = {10/gjrbsc},
	shorttitle = {{RaLL}},
	abstract = {Compared to the onboard camera and laser scanner, radar sensor provides lighting and weather invariant sensing, which is naturally suitable for long-term localization under adverse conditions. However, radar data is sparse and noisy, resulting in challenges for radar mapping. On the other hand, the most popular available map currently is built by lidar. In this paper, we propose an end-to-end deep learning framework for Radar Localization on Lidar Map ({RaLL}) to bridge the gap, which not only achieves the robust radar localization but also exploits the mature lidar mapping technique, thus reducing the cost of radar mapping. We first embed both sensor modals into a common feature space by a neural network. Then multiple offsets are added to the map modal for exhaustive similarity evaluation against the current radar modal, yielding the regression of the current pose. Finally, we apply this differentiable measurement model to a Kalman Filter ({KF}) to learn the whole sequential localization process in an end-to-end manner. {\textbackslash}textit\{The whole learning system is differentiable with the network based measurement model at the front-end and {KF} at the back-end.\} To validate the feasibility and effectiveness, we employ multi-session multi-scene datasets collected from the real world, and the results demonstrate that our proposed system achieves superior performance over \$90km\$ driving, even in generalization scenarios where the model training is in {UK}, while testing in South Korea. We also release the source code publicly.},
	journaltitle = {{arXiv}:2009.07061 [cs]},
	author = {Yin, Huan and Chen, Runjian and Wang, Yue and Xiong, Rong},
	urldate = {2021-04-19},
	date = {2021-03-05},
	eprinttype = {arxiv},
	eprint = {2009.07061},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{wulff_early_2018,
	title = {Early Fusion of Camera and Lidar for robust road detection based on U-Net {FCN}},
	doi = {10/gjrbrs},
	abstract = {Automated vehicles rely on the accurate and robust detection of the drivable area, often classified into free space, road area and lane information. Most current approaches use monocular or stereo cameras to detect these. However, {LiDAR} sensors are becoming more common and offer unique properties for road area detection such as precision and robustness to weather conditions. We therefore propose two approaches for a pixel-wise semantic binary segmentation of the road area based on a modified U-Net Fully Convolutional Network ({FCN}) architecture. The first approach {UView}-Cam employs a single camera image, whereas the second approach {UGrid}-Fused incorporates a early fusion of {LiDAR} and camera data into a multi-dimensional occupation grid representation as {FCN} input. The fusion of camera and {LiDAR} allows for efficient and robust leverage of individual sensor properties in a single {FCN}. For the training of {UView}-Cam, multiple publicly available datasets of street environments are used, while the {UGrid}-Fused is trained with the {KITTI} dataset. In the {KITTI} Road/Lane Detection benchmark, the proposed networks reach a {MaxF} score of 94.23\% and 93.81\% respectively. Both approaches achieve realtime performance with a detection rate of about 10 Hz.},
	eventtitle = {2018 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {1426--1431},
	booktitle = {2018 {IEEE} Intelligent Vehicles Symposium ({IV})},
	author = {Wulff, F. and Schäufele, B. and Sawade, O. and Becker, D. and Henke, B. and Radusch, I.},
	date = {2018-06},
	note = {{ISSN}: 1931-0587},
	keywords = {Cameras, Computer architecture, Laser radar, Microprocessors, Roads, Sensors, Training},
}

@article{malche_environmental_2019,
	title = {Environmental Monitoring System for Smart City Based on Secure Internet of Things ({IoT}) Architecture},
	volume = {107},
	issn = {0929-6212},
	url = {https://link.springer.com/epdf/10.1007/s11277-019-06376-0},
	doi = {10/gjp32z},
	abstract = {With increasing population, urbanization, energy, transportation, and agricultural developments, pollution is degrading the environment with ever-increasing pace. The degradation in the environment due to pollution can easily affect the quality of human life by increasing health issues. Therefore, in order to avoid health risks due to the polluted environment, it is essential to monitor its state. However, at present, monitoring of data on the state of the environment is not a well-researched field. Therefore, it is required to develop a system which can efficiently collect and analyze data on the environment in order to avoid any potential risks. The Internet is one of the necessary and important tools which can be used to develop a system capable of monitoring and sharing information on environmental pollution. This study proposes Internet of Things ({IoT}) based environment monitoring and alert system. The proposed system monitors the region specific environment for air quality, and sound pollution, while also facilitating secure data transmission over the network which solves the security issues in {IoT} system.},
	number = {4},
	journaltitle = {Wireless Personal Communications},
	author = {Malche, Timothy and Maheshwary, Priti and Kumar, Rakesh},
	urldate = {2021-04-14},
	date = {2019},
	langid = {english},
	note = {13 citations (Semantic Scholar/{DOI}) [2021-04-14]},
}

@article{i_shah_review_2021,
	title = {A review of {IoT}-based smart waste level monitoring system for smart cities},
	volume = {21},
	issn = {2502-4760, 2502-4752},
	url = {http://ijeecs.iaescore.com/index.php/IJEECS/article/view/22416},
	doi = {10/gjqbfk},
	abstract = {Smart cities are covering the population that are seeking the best lifestyle and fulfilling their needs. Through smart cities, necessary modern facilities using {ICT} emerging technologies such as the internet of things ({IoT}) had been installed to ensure the sustainability of the city. In the perspective of waste management, several different {IoT}-based solutions also had been proposed as an alternative to monitor and to ensure the health of communities. This paper reviews existing {IoT}-based solutions in smart cites’ waste level management system to bring together the state-of-the-art. We performed reviews on 16 research articles from the past 5 years in the literature to provide a comprehensive review of different works on {IoT}-based solutions related to the smart waste level monitoring system, possible solutions and technologies used. The results obtained shows that existing solutions were similar in the platform used to integrate with the {IoT} technologies but had some differences in term of the used of sensors and communication technologies. The study also shows that many of the prior studies used Arduino Uno. Results from this study will assist the researcher, focusing on expanding further the used of different technologies or improved the existing system.},
	pages = {450},
	number = {1},
	journaltitle = {Indonesian Journal of Electrical Engineering and Computer Science},
	shortjournal = {{IJEECS}},
	author = {I. Shah, A. A. and M. Fauzi, S. S. and J. M. Gining, R. A. and R. Razak, T. and F. Jamaluddin, M. N. and Maskat, R.},
	urldate = {2021-04-15},
	date = {2021-01-01},
	langid = {english},
}

@article{esmaeilian_future_2018,
	title = {The future of waste management in smart and sustainable cities: A review and concept paper},
	doi = {10/gfgvg7},
	abstract = {The potential of smart cities in remediating environmental problems in general and waste management, in particular, is an important question that needs to be investigated in academic research. Built on an integrative review of the literature, this study offers insights into the potential of smart cities and connected communities in facilitating waste management efforts. Shortcomings of existing waste management practices are highlighted and a conceptual framework for a centralized waste management system is proposed, where three interconnected elements are discussed: (1) an infrastructure for proper collection of product lifecycle data to facilitate full visibility throughout the entire lifespan of a product, (2) a set of new business models relied on product lifecycle data to prevent waste generation, and (3) an intelligent sensor-based infrastructure for proper upstream waste separation and on-time collection. The proposed framework highlights the value of product lifecycle data in reducing waste and enhancing waste recovery and the need for connecting waste management practices to the whole product lifecycle. An example of the use of tracking and data sharing technologies for investigating the waste management issues has been discussed. Finally, the success factors for implementing the proposed framework and some thoughts on future research directions have been discussed.},
	pages = {19},
	journaltitle = {Waste Management},
	author = {Esmaeilian, Behzad},
	date = {2018},
	langid = {english},
}

@article{shokravi_health_2020,
	title = {Health Monitoring of Civil Infrastructures by Subspace System Identification Method: An Overview},
	volume = {10},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/8/2786},
	doi = {10/gjqbcd},
	shorttitle = {Health Monitoring of Civil Infrastructures by Subspace System Identification Method},
	abstract = {Structural health monitoring ({SHM}) is the main contributor of the future’s smart city to deal with the need for safety, lower maintenance costs, and reliable condition assessment of structures. Among the algorithms used for {SHM} to identify the system parameters of structures, subspace system identiﬁcation ({SSI}) is a reliable method in the time-domain that takes advantages of using extended observability matrices. Considerable numbers of studies have speciﬁcally concentrated on practical applications of {SSI} in recent years. To the best of author’s knowledge, no study has been undertaken to review and investigate the application of {SSI} in the monitoring of civil engineering structures. This paper aims to review studies that have used the {SSI} algorithm for the damage identiﬁcation and modal analysis of structures. The fundamental focus is on data-driven and covariance-driven {SSI} algorithms. In this review, we consider the subspace algorithm to resolve the problem of a real-world application for {SHM}. With regard to performance, a comparison between {SSI} and other methods is provided in order to investigate its advantages and disadvantages. The applied methods of {SHM} in civil engineering structures are categorized into three classes, from simple one-dimensional (1D) to very complex structures, and the detectability of the {SSI} for di↵erent damage scenarios are reported. Finally, the available software incorporating {SSI} as their system identiﬁcation technique are investigated.},
	pages = {2786},
	number = {8},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Shokravi, Hoofar and Shokravi, Hooman and Bakhary, Norhisham and Rahimian Koloor, Seyed Saeid and Petrů, Michal},
	urldate = {2021-04-15},
	date = {2020-04-17},
	langid = {english},
	note = {9 citations (Semantic Scholar/{DOI}) [2021-04-15]},
}

@article{afrin_survey_2020,
	title = {A Survey of Road Traffic Congestion Measures towards a Sustainable and Resilient Transportation System},
	doi = {10/gjm3rd},
	abstract = {Traﬃc congestion is a perpetual problem for the sustainability of transportation development. Traﬃc congestion causes delays, inconvenience, and economic losses to drivers, as well as air pollution. Identiﬁcation and quantiﬁcation of traﬃc congestion are crucial for decision-makers to initiate mitigation strategies to improve the overall transportation system’s sustainability. In this paper, the currently available measures are detailed and compared by implementing them on a daily and weekly traﬃc historical dataset. The results showed each measure showed signiﬁcant variations in congestion states while indicating a similar congestion trend. The advantages and disadvantages of each measure are identiﬁed from the data analysis. This study summarizes the current road traﬃc congestion measures and provides a constructive insight into the development of a sustainable and resilient traﬃc management system.},
	pages = {23},
	author = {Afrin, Tanzina and Yodo, Nita},
	date = {2020},
	langid = {english},
}

@article{almaadeed_automatic_2018,
	title = {Automatic Detection and Classification of Audio Events for Road Surveillance Applications},
	abstract = {This work investigates the problem of detecting hazardous events on roads by designing an audio surveillance system that automatically detects perilous situations such as car crashes and tire skidding. In recent years, research has shown several visual surveillance systems that have been proposed for road monitoring to detect accidents with an aim to improve safety procedures in emergency cases. However, the visual information alone cannot detect certain events such as car crashes and tire skidding, especially under adverse and visually cluttered weather conditions such as snowfall, rain, and fog. Consequently, the incorporation of microphones and audio event detectors based on audio processing can signiﬁcantly enhance the detection accuracy of such surveillance systems. This paper proposes to combine time-domain, frequency-domain, and joint time-frequency features extracted from a class of quadratic time-frequency distributions ({QTFDs}) to detect events on roads through audio analysis and processing. Experiments were carried out using a publicly available dataset. The experimental results conform the effectiveness of the proposed approach for detecting hazardous events on roads as demonstrated by 7\% improvement of accuracy rate when compared against methods that use individual temporal and spectral features.},
	pages = {19},
	author = {Almaadeed, Noor and Asim, Muhammad and Al-Maadeed, Somaya and Bouridane, Ahmed and Beghdadi, Azeddine},
	date = {2018},
	langid = {english},
	keywords = {❓ Multiple {DOI}},
}

@article{wang_dynamic_2016,
	title = {Dynamic road lane management study},
	volume = {89},
	issn = {13665545},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1366554515001258},
	doi = {10/gjqr98},
	abstract = {Our Smart City contribution is transportation-oriented in that it proposes a dynamic road lane management system in order to share appropriately the space devoted to trafﬁc. After a historical view of a series of solutions from physical to {ICT} supported, we present our proposal extensively supported by up-to-date {ICT}. Following a main presentation, we describe the system architecture and its working conditions. Then, we present the proposed simulator designed to study operating and driver’s conditions with respect to the new trafﬁc signs proposed. We also describe a Mock-up technology validation and give preliminary information on in-the-ﬁeld deployment.},
	pages = {272--287},
	journaltitle = {Transportation Research Part E: Logistics and Transportation Review},
	shortjournal = {Transportation Research Part E: Logistics and Transportation Review},
	author = {Wang, Chen and David, Bertrand and Chalon, René and Yin, Chuantao},
	urldate = {2021-04-17},
	date = {2016-05},
	langid = {english},
	note = {12 citations (Semantic Scholar/{DOI}) [2021-04-17]},
}

@article{peng_jing_car--pedestrian_2017,
	title = {Car-to-Pedestrian Communication Safety System Based on the Vehicular Ad-Hoc Network Environment: A Systematic Review},
	volume = {8},
	issn = {2078-2489},
	url = {http://www.mdpi.com/2078-2489/8/4/127},
	doi = {10/gcw8pk},
	shorttitle = {Car-to-Pedestrian Communication Safety System Based on the Vehicular Ad-Hoc Network Environment},
	pages = {127},
	number = {4},
	journaltitle = {Information},
	shortjournal = {Information},
	author = {{Peng Jing} and {Wei Huang} and {Long Chen}},
	urldate = {2021-04-17},
	date = {2017-10-14},
	langid = {english},
	note = {7 citations (Semantic Scholar/{DOI}) [2021-04-17]},
}

@article{javaid_smart_2018,
	title = {Smart Traffic Management System Using Internet of Things},
	abstract = {Traffic management system is considered as one of the major dimensions of a smart city. With the rapid growth of population and urban mobility in metropolitan cities, traffic congestion is often seen on roads. To tackle various issues for managing traffic on roads and to help authorities in proper planning, a smart traffic management system using the Internet of Things ({IoT}) is proposed in this paper. A hybrid approach (combination of centralized and decentralized) is used to optimize traffic flow on roads and an algorithm is devised to manage various traffic situations efficiently. For this purpose, the system takes traffic density as input from a) cameras b) and sensors, then manages traffic signals. Another algorithm based on Artificial Intelligence is used to predict the traffic density for future to minimize the traffic congestion. Besides this, {RFIDs} are also used to prioritize the emergency vehicles such as ambulances and fire brigade vehicles during a traffic jam. In case of fire on the road, Smoke sensors are also part of this system to detect this situation. To demonstrate the effectiveness of the proposed traffic management system, a prototype is developed which not only optimizes the flow of traffic but also connects nearby rescue departments with a centralized server. Moreover, it also extracts useful information presented in graphical formats that may help the authorities in future road planning.},
	pages = {6},
	author = {Javaid, Sabeen and Sufian, Ali and Pervaiz, Saima and Tanveer, Mehak},
	date = {2018},
	langid = {english},
	keywords = {❓ Multiple {DOI}},
}

@article{tian_hierarchical_2017,
	title = {Hierarchical and Networked Vehicle Surveillance in {ITS}: A Survey},
	volume = {18},
	issn = {1524-9050, 1558-0016},
	url = {https://ieeexplore.ieee.org/document/7464298/},
	doi = {10/f9tt6z},
	shorttitle = {Hierarchical and Networked Vehicle Surveillance in {ITS}},
	abstract = {Trafﬁc surveillance has become an important topic in intelligent transportation systems ({ITSs}), which is aimed at monitoring and managing trafﬁc ﬂow. With the progress in computer vision, video-based surveillance systems have made great advances on trafﬁc surveillance in {ITSs}. However, the performance of most existing surveillance systems is susceptible to challenging complex trafﬁc scenes (e.g., object occlusion, pose variation, and cluttered background). Moreover, existing related research is mainly on a single video sensor node, which is incapable of addressing the surveillance of trafﬁc road networks. Accordingly, we present a review of the literature on the video-based vehicle surveillance systems in {ITSs}. We analyze the existing challenges in video-based surveillance systems for the vehicle and present a general architecture for video surveillance systems, i.e., the hierarchical and networked vehicle surveillance, to survey the different existing and potential techniques. Then, different methods are reviewed and discussed with respect to each module. Applications and future developments are discussed to provide future needs of {ITS} services.},
	pages = {25--48},
	number = {1},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	shortjournal = {{IEEE} Trans. Intell. Transport. Syst.},
	author = {Tian, Bin and Morris, Brendan Tran and Tang, Ming and Liu, Yuqiang and Yao, Yanjie and Gou, Chao and Shen, Dayong and Tang, Shaohu},
	urldate = {2021-04-13},
	date = {2017-01},
	langid = {english},
}

@article{yuan_survey_2021,
	title = {A Survey of Traffic Prediction: from Spatio-Temporal Data to Intelligent Transportation},
	volume = {6},
	issn = {2364-1185, 2364-1541},
	url = {http://link.springer.com/10.1007/s41019-020-00151-z},
	doi = {10/gh3684},
	shorttitle = {A Survey of Traffic Prediction},
	abstract = {Intelligent transportation (e.g., intelligent traffic light) makes our travel more convenient and efficient. With the development of mobile Internet and position technologies, it is reasonable to collect spatio-temporal data and then leverage these data to achieve the goal of intelligent transportation, and here, traffic prediction plays an important role. In this paper, we provide a comprehensive survey on traffic prediction, which is from the spatio-temporal data layer to the intelligent transportation application layer. At first, we split the whole research scope into four parts from bottom to up, where the four parts are, respectively, spatio-temporal data, preprocessing, traffic prediction and traffic application. Later, we review existing work on the four parts. First, we summarize traffic data into five types according to their difference on spatial and temporal dimensions. Second, we focus on four significant data preprocessing techniques: map-matching, data cleaning, data storage and data compression. Third, we focus on three kinds of traffic prediction problems (i.e., classification, generation and estimation/ forecasting). In particular, we summarize the challenges and discuss how existing methods address these challenges. Fourth, we list five typical traffic applications. Lastly, we provide emerging research challenges and opportunities. We believe that the survey can help the partitioners to understand existing traffic prediction problems and methods, which can further encourage them to solve their intelligent transportation applications.},
	pages = {63--85},
	number = {1},
	journaltitle = {Data Science and Engineering},
	shortjournal = {Data Sci. Eng.},
	author = {Yuan, Haitao and Li, Guoliang},
	urldate = {2021-04-12},
	date = {2021-03},
	langid = {english},
}

@article{pavlyuk_feature_2019,
	title = {Feature selection and extraction in spatiotemporal traffic forecasting: a systematic literature review},
	volume = {11},
	issn = {1867-0717, 1866-8887},
	url = {https://etrr.springeropen.com/articles/10.1186/s12544-019-0345-9},
	doi = {10/gjp33d},
	shorttitle = {Feature selection and extraction in spatiotemporal traffic forecasting},
	abstract = {A spatiotemporal approach that simultaneously utilises both spatial and temporal relationships is gaining scientific interest in the field of traffic flow forecasting. Accurate identification of the spatiotemporal structure (dependencies amongst traffic flows in space and time) plays a critical role in modern traffic forecasting methodologies, and recent developments of data-driven feature selection and extraction methods allow the identification of complex relationships. This paper systematically reviews studies that apply feature selection and extraction methods for spatiotemporal traffic forecasting. The reviewed bibliographic database includes 211 publications and covers the period from early 1984 to March 2018. A synthesis of bibliographic sources clarifies the advantages and disadvantages of different feature selection and extraction methods for learning the spatiotemporal structure and discovers trends in their applications. We conclude that there is a clear need for development of comprehensive guidelines for selecting appropriate spatiotemporal feature selection and extraction methods for urban traffic forecasting.},
	pages = {6},
	number = {1},
	journaltitle = {European Transport Research Review},
	shortjournal = {Eur. Transp. Res. Rev.},
	author = {Pavlyuk, Dmitry},
	urldate = {2021-04-12},
	date = {2019-12},
	langid = {english},
}

@article{lyu_novel_2021,
	title = {Novel Time-Delay Side-Collision Warning Model at Non-Signalized Intersections Based on Vehicle-to-Infrastructure Communication},
	volume = {18},
	issn = {1660-4601},
	url = {https://www.mdpi.com/1660-4601/18/4/1520},
	doi = {10/gjp325},
	abstract = {In complex traffic environments, collision warning systems that rely only on in-vehicle sensors are limited in accuracy and range. Vehicle-to-infrastructure (V2I) communication systems, however, offer more robust information exchange, and thus, warnings. In this study, V2I was used to analyze side-collision warning models at non-signalized intersections: A novel time-delay sidecollision warning model was developed according to the motion compensation principle. This novel time-delay model was compared with and verified against a traditional side-collision warning model. Using a V2I-oriented simulated driving platform, three vehicle-vehicle collision scenarios were designed at non-signalized intersections. Twenty participants were recruited to conduct simulated driving experiments to test and verify the performance of each collision warning model. The results showed that compared with no warning system, both side-collision warning models reduced the proportion of vehicle collisions. In terms of efficacy, the traditional model generated an effective warning in 84.2\% of cases, while the novel time-delay model generated an effective warning in 90.2\%. In terms of response time and conflict time difference, the traditional model gave a longer response time of 0.91 s (that of the time-delay model is 0.78 s), but the time-delay model reduced the driving risk with a larger conflict time difference. Based on an analysis of driver gaze change post-warning, the statistical results showed that the proportion of effective gaze changes reached 84.3\%. Based on subjective evaluations, drivers reported a higher degree of acceptance of the timedelay model. Therefore, the time-delay side-collision warning model for non-signalized intersections proposed herein can improve the applicability and efficacy of warning systems in such complex traffic environments and provide reference for safety applications in V2I systems.},
	pages = {1520},
	number = {4},
	journaltitle = {International Journal of Environmental Research and Public Health},
	shortjournal = {{IJERPH}},
	author = {Lyu, Nengchao and Wen, Jiaqiang and Wu, Chaozhong},
	urldate = {2021-04-13},
	date = {2021-02-05},
	langid = {english},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-04-14]},
}

@article{mahoor_state---art_2019,
	title = {State-Of-The-Art in Smart Streetlight Systems: A Review},
	volume = {2},
	doi = {10/gjk9dh},
	shorttitle = {State-Of-The-Art in Smart Streetlight Systems},
	journaltitle = {{IET} Smart Cities},
	shortjournal = {{IET} Smart Cities},
	author = {Mahoor, Mohsen and Hosseini, Zohreh and Khodaei, Amin and Paaso, Aleksi and Kushner, Daniel},
	date = {2019-11-11},
	note = {2 citations (Semantic Scholar/{DOI}) [2021-03-31]},
}

@article{pratico_detection_2020,
	title = {Detection and Monitoring of Bottom-Up Cracks in Road Pavement Using a Machine-Learning Approach},
	volume = {13},
	issn = {1999-4893},
	url = {https://www.mdpi.com/1999-4893/13/4/81},
	doi = {10/gjqbcz},
	abstract = {The current methods that aim at monitoring the structural health status ({SHS}) of road pavements allow detecting surface defects and failures. This notwithstanding, there is a lack of methods and systems that are able to identify concealed cracks (particularly, bottom-up cracks) and monitor their growth over time. For this reason, the objective of this study is to set up a supervised machine learning ({ML})-based method for the identiﬁcation and classiﬁcation of the {SHS} of a diﬀerently cracked road pavement based on its vibro-acoustic signature. The method aims at collecting these signatures (using acoustic-sensors, located at the roadside) and classifying the pavement’s {SHS} through {ML} models. Diﬀerent {ML} classiﬁers (i.e., multilayer perceptron, {MLP}, convolutional neural network, {CNN}, random forest classiﬁer, {RFC}, and support vector classiﬁer, {SVC}) were used and compared. Results show the possibility of associating with great accuracy (i.e., {MLP} = 91.8\%, {CNN} = 95.6\%, {RFC} = 91.0\%, and {SVC} = 99.1\%) a speciﬁc vibro-acoustic signature to a diﬀerently cracked road pavement. These results are encouraging and represent the bases for the application of the proposed method in real contexts, such as monitoring roads and bridges using wireless sensor networks, which is the target of future studies.},
	pages = {81},
	number = {4},
	journaltitle = {Algorithms},
	shortjournal = {Algorithms},
	author = {Praticò, Filippo Giammaria and Fedele, Rosario and Naumov, Vitalii and Sauer, Tomas},
	urldate = {2021-04-15},
	date = {2020-03-31},
	langid = {english},
	note = {4 citations (Semantic Scholar/{DOI}) [2021-04-15]},
}

@article{namazi_intelligent_2019,
	title = {Intelligent Intersection Management Systems Considering Autonomous Vehicles: A Systematic Literature Review},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8756239/},
	doi = {10/gf5g2r},
	shorttitle = {Intelligent Intersection Management Systems Considering Autonomous Vehicles},
	abstract = {Over the past several decades, the development of technologies and the production of autonomous vehicles have enhanced the need for intelligent intersection management systems. Subsequently, growing interest in studying the trafﬁc management of autonomous vehicles at intersections has been evident, which indicates a critical need to conduct a systematic literature review on this topic. This paper offers a systematic review of the proposed methodologies for intelligent intersection management systems and presents the remaining research gaps and possible future research approaches. We consider both pure autonomous vehicle trafﬁc and mixed trafﬁc at four-way signalized and unsignalized intersection(s). We searched for articles published from 2008 to 2019, and identiﬁed 105 primary studies. We applied the thematic analysis method to analyze the extracted data, which led to the identiﬁcation of four main classes of methodologies, namely rule-based, optimization, hybrid, and machine learning methods. We also compared how well the methods satisfy their goals, namely efﬁciency, safety, ecology, and passenger comfort. This analysis allowed us to determine the primary challenges of the presented methodologies and propose new approaches in this area.},
	pages = {91946--91965},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Namazi, Elnaz and Li, Jingyue and Lu, Chaoru},
	urldate = {2021-04-16},
	date = {2019},
	langid = {english},
	note = {14 citations (Semantic Scholar/{DOI}) [2021-04-16]},
}

@article{liu_trajectory_2019,
	title = {Trajectory planning for autonomous intersection management of connected vehicles},
	volume = {90},
	issn = {1569190X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1569190X18301497},
	doi = {10/gjqkvh},
	abstract = {This paper proposes a cooperative scheduling mechanism for autonomous vehicles passing through an intersection, called {TP}-{AIM}. The main objective of this research is to ensure safe driving while minimizing delay in an intersection without traﬃc lights. Firstly, an intersection management system, used as an info-collecting-organizing center, assigns reasonable priorities for all present vehicles and hence plans their trajectories. Secondly, a window searching algorithm is performed to ﬁnd an entering window, which can produce a collision-free trajectory with minimal delay, besides backup windows. Finally, vehicles can arrange their trajectory individually, by applying dynamic programming to compute velocity proﬁle, in order to pass through intersection. {MATLAB}/Simulink and {SUMO} based simulations are established among three types of traﬃc mechanisms with diﬀerent traﬃc ﬂows. The results show that the proposed {TP}-{AIM} mechanism signiﬁcantly reduces the average evacuation time and increases throughput by over 20\%. Moreover, the paper investigates intersection delay, which can be reduced to less than 10\% compared to classical light management systems. Both safety and eﬃciency can be guaranteed in our proposed mechanism.},
	pages = {16--30},
	journaltitle = {Simulation Modelling Practice and Theory},
	shortjournal = {Simulation Modelling Practice and Theory},
	author = {Liu, Bing and Shi, Qing and Song, Zhuoyue and El Kamel, Abdelkader},
	urldate = {2021-04-16},
	date = {2019-01},
	langid = {english},
	note = {16 citations (Semantic Scholar/{DOI}) [2021-04-16]},
}

@article{agrawal_intelligent_2020,
	title = {Intelligent traffic light design and control in smart cities: a survey on techniques and methodologies},
	volume = {5},
	issn = {1471-0242, 1741-8208},
	url = {http://www.inderscience.com/link.php?id=111456},
	doi = {10/gjqkvk},
	shorttitle = {Intelligent traffic light design and control in smart cities},
	abstract = {Increased traffic in metropolitan territories has led to significant concerns, such as road blockage, transportation delays, pollution level, fuel consumption, etc. Traffic light signals at intersections, being a part of Traffic Management System ({TMS}) play an important role in effectively controlling traffic. The conventional pre-timed controlled traffic signals are becoming a bottleneck in clearance of intense traffic especially during rush hours. Adaptive Traffic Light Control ({ATLC}) has been outlined for quick traffic clearance at junctions, which could additionally be upgraded by giving right of approach to emergency vehicles. This survey summarises {ATLC} systems designed by leveraging the existing technologies such as {WSN}, {VANET} and image processing techniques to gather real-time traffic statistics, and evaluating the accumulated data to alter traffic lights with the aid of intelligent controllers. Keeping in mind the benefits of fuzzy logic in traffic control, this survey provides in-depth review of the fuzzy controllers in context to traffic lights at isolated and multiple intersections. Popular {ATLC} systems implemented worldwide are also summarised.},
	pages = {436},
	number = {4},
	journaltitle = {International Journal of Vehicle Information and Communication Systems},
	shortjournal = {{IJVICS}},
	author = {Agrawal, Aditi and Paulus, Rajeev},
	urldate = {2021-04-16},
	date = {2020},
	langid = {english},
}

@article{tak_sectional_2020,
	title = {Sectional Information-Based Collision Warning System Using Roadside Unit Aggregated Connected-Vehicle Information for a Cooperative Intelligent Transport System},
	volume = {2020},
	issn = {0197-6729, 2042-3195},
	url = {https://www.hindawi.com/journals/jat/2020/1528028/},
	doi = {10/gjp327},
	abstract = {Vehicular collision and hazard warning is an active field of research that seeks to improve road safety by providing an earlier warning to drivers to help them avoid potential collision danger. In this study, we propose a new type of a collision warning system based on aggregated sectional information, describing vehicle movement processed by a roadside unit ({RSU}). The proposed sectional information-based collision warning system ({SCWS}) overcomes the limitations of existing collision warning systems such as the high installation costs, the need for high market penetration rates, and the lack of consideration of traffic dynamics. The proposed {SCWS} gathers vehicle operation data through on-board units ({OBUs}) and shares this aggregated information through an {RSU}. All the data for each road section are locally processed by the {RSU} using edge computing, allowing the {SCWS} to effectively estimate the information describing the vehicles surrounding the subject vehicle in each road section. The performance of the {SCWS} was evaluated through comparison with other collision warning systems such as the vehicle-to-vehicle communication-based collision warning system ({VCWS}), which solely uses in-vehicle sensors; the hybrid collision warning system ({HCWS}), which uses information from both infrastructure and in-vehicle sensors; and the infrastructure-based collision warning system ({ICWS}), which only uses data from infrastructure. In this study, the {VCWS} with a 100\% market penetration rate was considered to provide the most theoretically similar result to the actual collision risk. The comparison results show that in both aggregation and disaggregation level analyses, the proposed {SCWS} exhibits a similar collision risk trend to the {VCWS}. Furthermore, the {SCWS} shows a high potential for practical application because it provides acceptable performance even with a low market penetration rate (30\%) at the relatively low cost of {OBU} installation, compared to the {VCWS} requirement of a high market penetration rate at a high installation cost.},
	pages = {1--12},
	journaltitle = {Journal of Advanced Transportation},
	shortjournal = {Journal of Advanced Transportation},
	author = {Tak, Sehyun and Yoon, Jinsu and Woo, Soomin and Yeo, Hwasoo},
	urldate = {2021-04-13},
	date = {2020-07-21},
	langid = {english},
}

@article{jalaney_review_2019,
	title = {Review on {IoT} Based Architecture for Smart Public Transport System},
	volume = {14},
	abstract = {Smart Public Transportation ({SPT}) is a subsystem of Intelligent Transportation Systems ({ITS}). It can control public transportation systems in a highly intelligent manner to keep up their execution, and to give information on excursions and system working conditions to clients (travelers and leaders). Quick advancement in equipment, programming, and correspondence innovations has encouraged the rise of Internet-associated devices that give perceptions and information gathering from this present reality. This paper is the review method by conducting surveys on {IoT} based smart public transport and various techniques on the intelligent transport system. The results of this study show that {IoT} utilization till now tends to give priority to safety in avoiding road accidents but has not yet discussed how intelligent transportation system can be developed by integrating bus scheduling, bus presence detection, and payment efficiency of passengers by booking seat system so that limit congestion and reduce waiting time of the passengers. This research proposes breakthroughs incorporating the concept of the Internet with the integration of platforms of industrial actors involved in order to harness the power of {IoT} for various conveniences especially in the field of public transport and produce intelligence transportation system which is one of the smart city concept indicators. The objective of the paper is to examine different {ITS} design and model and survey such models to get inside and out of their architecture. It will lead to gaps in knowledge which can be further studied. The paper features the investigations of numerous frameworks and furthermore gives the future extension in the field of its to make it easier to utilize.},
	pages = {6},
	number = {2},
	author = {Jalaney, J and Ganesh, Dr R S},
	date = {2019},
	langid = {english},
}

@article{gholikhani_critical_2020,
	title = {A critical review of roadway energy harvesting technologies},
	doi = {10/gjqbfz},
	abstract = {Energy harvesting from roadways has the potential to generate electricity for a multitude of roadside data collection and communication applications. Roadside energy harvesters are broadly grouped into three categories on the basis of the energy source tapped: mechanical energy from vehicles, pavement heat, and solar radiation. In terms of harvesting technology, harvesters are grouped into electromagnetic, piezoelectric, thermoelectric, pyroelectric, photovoltaic, and solar heat collector involving liquid or air circulation. This paper provides a comprehensive state-of-the-art review of the literature on each of these energy harvesting technologies. It includes information on the harvesting principle, prototype development, implementation eﬀorts, and economic consideration for each harvesting technology. It concludes that several of these harvesting technologies are suﬃciently developed to generate self-sustainable roadside electrical power.},
	pages = {17},
	journaltitle = {Applied Energy},
	author = {Gholikhani, Mohammadreza},
	date = {2020},
	langid = {english},
}

@article{saharan_dynamic_2020,
	title = {Dynamic pricing techniques for Intelligent Transportation System in smart cities: A systematic review},
	volume = {150},
	issn = {01403664},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140366419310990},
	doi = {10/gjhbfg},
	shorttitle = {Dynamic pricing techniques for Intelligent Transportation System in smart cities},
	pages = {603--625},
	journaltitle = {Computer Communications},
	shortjournal = {Computer Communications},
	author = {Saharan, Sandeep and Bawa, Seema and Kumar, Neeraj},
	urldate = {2021-04-14},
	date = {2020-01},
	langid = {english},
	note = {20 citations (Semantic Scholar/{DOI}) [2021-04-14]},
}

@article{ang_deployment_2019,
	title = {Deployment of {IoV} for Smart Cities: Applications, Architecture, and Challenges},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8579129/},
	doi = {10/gjqr96},
	shorttitle = {Deployment of {IoV} for Smart Cities},
	pages = {6473--6492},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Ang, Li-Minn and Seng, Kah Phooi and Ijemaru, Gerald K. and Zungeru, Adamu Murtala},
	urldate = {2021-04-17},
	date = {2019},
	langid = {english},
	note = {36 citations (Semantic Scholar/{DOI}) [2021-04-17]},
}

@article{toh_advances_2020,
	title = {Advances in smart roads for future smart cities},
	volume = {476},
	issn = {1364-5021, 1471-2946},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2019.0439},
	doi = {10/gjp44n},
	abstract = {Various countries throughout the world have started their efforts in designing and implementing smart cities. China alone has over 300 smart city projects, with strong participation by industries and government offices. India too have allocated trillions in budget to build over 100 smart cities. An essential part of a smart city is transport. In this paper, we will discuss the current state, developments, and some of the emerging advances in transportation technologies and how these advances in smart roads will prepare the society towards the realization of future smart cities.},
	pages = {20190439},
	number = {2233},
	journaltitle = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	shortjournal = {Proc. R. Soc. A.},
	author = {Toh, Chai K. and Sanguesa, Julio A. and Cano, Juan C. and Martinez, Francisco J.},
	urldate = {2021-04-14},
	date = {2020-01},
	langid = {english},
	note = {10 citations (Semantic Scholar/{DOI}) [2021-04-14]},
}

@article{universitat_politecnica_de_valencia_universitat_2014,
	title = {Universitat Politècnica de València},
	volume = {18},
	issn = {1886-4996, 1134-2196},
	url = {http://polipapers.upv.es/index.php/IA/article/view/3293},
	doi = {10/gddk2w},
	abstract = {Traffic signs have come a long way since the first automobile was invented. They have long served the purpose of warning and guiding drivers and also enforcing the traffic laws governing speed, parking, turns, and stopping. In this paper, we discuss the issues and challenges facing current traffic signs, and how it will evolve into a next-generation traffic sign architecture using advance wireless communications technologies. With technological advances in the areas of wireless communications and embedded electronics and software, we foresee that, in the future, digital traffic sign posts will be capable of transmitting the traffic sign information wirelessly to road users, and this will transform our roads into intelligent roads, where signs will appear promptly and automatically on in-vehicle displays to alert the driver. There is no longer the need to watch out for traffic signs since the detection will be automatic and performed wirelessly. This transformation will lessen burden on the drivers, so that they can then focus more on the traffic ahead while driving. Also, this evolution into wireless digital sign posts will fit well with the vision of future smart cities, where smart transportation technologies will be present to transform how we drive and commute, yielding greater safety, ease, and assistance to drivers.},
	pages = {ix},
	number = {1},
	journaltitle = {Ingeniería del agua},
	shortjournal = {ing.agua},
	author = {Universitat Politècnica de València, Editorial},
	urldate = {2021-04-14},
	date = {2014-09-29},
	langid = {english},
}

@article{alsrehin_intelligent_2019,
	title = {Intelligent Transportation and Control Systems Using Data Mining and Machine Learning Techniques: A Comprehensive Study},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8681028/},
	doi = {10/gf5t9w},
	shorttitle = {Intelligent Transportation and Control Systems Using Data Mining and Machine Learning Techniques},
	abstract = {Trafﬁc congestion is becoming the issues of the entire globe. This study aims to explore and review the data mining and machine learning technologies adopted in research and industry to attempt to overcome the direct and indirect trafﬁc issues on humanity and societies. The study’s methodology is to comprehensively review around 165 studies, criticize, and categorize all these studies into a chronological and understandable category. The study is focusing on the trafﬁc management approaches that were depended on data mining and machine learning technologies to detect and predict the trafﬁc only. This study has found that there is no standard trafﬁc management approach that the community of trafﬁc management has agreed on. This study is important to the trafﬁc research communities, trafﬁc software companies, and trafﬁc government ofﬁcials. It has a direct impact on drawing a clear path for new trafﬁc management propositions. This study is one of the largest studies with respect to the size of its reviewed articles that were focused on data mining and machine learning. Additionally, this study will draw general attention to a new trafﬁc management proposition approach.},
	pages = {49830--49857},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Alsrehin, Nawaf O. and Klaib, Ahmad F. and Magableh, Aws},
	urldate = {2021-04-14},
	date = {2019},
	langid = {english},
	note = {11 citations (Semantic Scholar/{DOI}) [2021-04-15]},
}

@article{saleem_data_2019,
	title = {Data Transmission Using {IoT} in Vehicular Ad-Hoc Networks in Smart City Congestion},
	volume = {24},
	issn = {1383-469X, 1572-8153},
	url = {http://link.springer.com/10.1007/s11036-018-1205-x},
	doi = {10/gjqr97},
	abstract = {Development of Internet of Things ({IoT}) enables smart city advancement throughout the world. Increasing number of vehicles has brought focus on road safety precautions and in-vehicle communication. This is the right time to focus on the development of new applications and services for vehicular environments. The Vehicular Ad-hoc Networks ({VANETs}) are an interesting range of Mobile Ad-hoc Networks ({MANETs}) where the Vehicle to Vehicle (V2V) and vehicle roadways transmission is possible. The V2V scheme is fresh by combining Wireless Fidelity (Wi-Fi), Bluetooth and other all sorts of communication standards. An immense number of nodes working with these networks and due to their immense displacements, the analysis is prevailing regarding the possibility of routing standards. The estimation of conventional routing standards for {MANETs} illustrates that their behaviors are minimal in {VANETs}. The intention is to make use of mediators for routing with an effort to address the before described issues. The mediators are accountable for gathering data related to routing and identifying the optimal paths for forwarding information packets. The routing scheme is based on group routing standards and data cluster framework for locating the best possible routes. In this paper, we analyze smart cities vehicle communication development by implementing {IoT}. We also discuss the ways to minimize the limitations connected to {IoT} deployment and implementation in smart city environment using multi mediator scheme.},
	pages = {248--258},
	number = {1},
	journaltitle = {Mobile Networks and Applications},
	shortjournal = {Mobile Netw Appl},
	author = {Saleem, Muhammad Asim and Shijie, Zhou and Sharif, Abida},
	urldate = {2021-04-17},
	date = {2019-02},
	langid = {english},
	note = {13 citations (Semantic Scholar/{DOI}) [2021-04-17]},
}

@inproceedings{hua_effective_2019,
	location = {San Francisco East Bay, {CA}, {USA}},
	title = {Effective Vehicle Tracking Algorithm for Smart Traffic Networks},
	isbn = {978-1-72811-442-2},
	url = {https://ieeexplore.ieee.org/document/8705792/},
	doi = {10/gjpqgc},
	eventtitle = {2019 {IEEE} International Conference on Service-Oriented System Engineering ({SOSE})},
	pages = {67--6709},
	booktitle = {2019 {IEEE} International Conference on Service-Oriented System Engineering ({SOSE})},
	publisher = {{IEEE}},
	author = {Hua, Shuai and Anastasiu, David C.},
	urldate = {2021-04-12},
	date = {2019-04},
	note = {6 citations (Semantic Scholar/{DOI}) [2021-04-12]},
}

@article{beg_uav-enabled_2021,
	title = {{UAV}-enabled intelligent traffic policing and emergency response handling system for the smart city},
	volume = {25},
	issn = {1617-4909, 1617-4917},
	url = {http://link.springer.com/10.1007/s00779-019-01297-y},
	doi = {10/gjp972},
	abstract = {As modern cities expand and develop, the resultant increase in population density gives rise to the need for smart solutions to cope with the demands applied to the infrastructure of the city. In this paper, we investigate the shortcomings of traffic policing and emergency response handling systems; propose an intelligent, autonomous {UAV}-enabled solution; and describe the system in a simulated environment. Several scenarios of traffic monitoring and policing system are considered in the simulation: traffic light violations and accident detection, mobile speeding traps and automated notification, congestion detection and traffic rerouting, flagged stolen vehicles/pending arrest warrants and vehicle tracking using {UAVs}, and autonomous emergency response handling systems. Furthermore, smart city infrastructure enable intelligent handling of emergencies by providing traffic light prioritization for ground emergency response units to reduce delay for patient care, automated physical bollard on routes with congested points due to accidents or hazards, first responder support {UAV} units—medical supplies {UAV}, fire fighting {UAV} to combat or control small fires, and numerous other benefits. Lastly, we present the results of the simulated system and discuss our findings.},
	pages = {33--50},
	number = {1},
	journaltitle = {Personal and Ubiquitous Computing},
	shortjournal = {Pers Ubiquit Comput},
	author = {Beg, Abdurrahman and Qureshi, Abdul Rahman and Sheltami, Tarek and Yasar, Ansar},
	urldate = {2021-04-14},
	date = {2021-02},
	langid = {english},
	note = {4 citations (Semantic Scholar/{DOI}) [2021-04-15]},
}

@article{won_intelligent_2020,
	title = {Intelligent Traffic Monitoring Systems for Vehicle Classification: A Survey},
	volume = {8},
	issn = {2169-3536},
	url = {http://arxiv.org/abs/1910.04656},
	doi = {10/gjp974},
	shorttitle = {Intelligent Traffic Monitoring Systems for Vehicle Classification},
	abstract = {A trafﬁc monitoring system is an integral part of Intelligent Transportation Systems ({ITS}). It is one of the critical transportation infrastructures that transportation agencies invest a huge amount of money to collect and analyze the trafﬁc data to better utilize the roadway systems, improve the safety of transportation, and establish future transportation plans. With recent advances in {MEMS}, machine learning, and wireless communication technologies, numerous innovative trafﬁc monitoring systems have been developed. In this article, we present a review of state-of-the-art trafﬁc monitoring systems focusing on the major functionality–vehicle classiﬁcation. We organize various vehicle classiﬁcation systems, examine research issues and technical challenges, and discuss hardware/software design, deployment experience, and system performance of vehicle classiﬁcation systems. Finally, we discuss a number of critical open problems and future research directions in an aim to provide valuable resources to academia, industry, and government agencies for selecting appropriate technologies for their trafﬁc monitoring applications.},
	pages = {73340--73358},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Won, Myounggyu},
	urldate = {2021-04-14},
	date = {2020},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.04656},
	note = {7 citations (Semantic Scholar/{DOI}) [2021-04-15]
7 citations (Semantic Scholar/{arXiv}) [2021-04-15]},
	keywords = {Computer Science - Computers and Society, Electrical Engineering and Systems Science - Signal Processing},
}

@article{wan_mobile_2016,
	title = {Mobile Crowd Sensing for Traffic Prediction in Internet of Vehicles},
	volume = {16},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/16/1/88},
	doi = {10/f3pm3n},
	abstract = {The advances in wireless communication techniques, mobile cloud computing, automotive and intelligent terminal technology are driving the evolution of vehicle ad hoc networks into the Internet of Vehicles ({IoV}) paradigm. This leads to a change in the vehicle routing problem from a calculation based on static data towards real-time trafﬁc prediction. In this paper, we ﬁrst address the taxonomy of cloud-assisted {IoV} from the viewpoint of the service relationship between cloud computing and {IoV}. Then, we review the traditional trafﬁc prediction approached used by both Vehicle to Infrastructure (V2I) and Vehicle to Vehicle (V2V) communications. On this basis, we propose a mobile crowd sensing technology to support the creation of dynamic route choices for drivers wishing to avoid congestion. Experiments were carried out to verify the proposed approaches. Finally, we discuss the outlook of reliable trafﬁc prediction.},
	pages = {88},
	number = {1},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Wan, Jiafu and Liu, Jianqi and Shao, Zehui and Vasilakos, Athanasios and Imran, Muhammad and Zhou, Keliang},
	urldate = {2021-04-17},
	date = {2016-01-11},
	langid = {english},
	note = {153 citations (Semantic Scholar/{DOI}) [2021-04-17]},
}

@article{fernandez-ares_studying_2017,
	title = {Studying real traffic and mobility scenarios for a Smart City using a new monitoring and tracking system},
	volume = {76},
	issn = {0167739X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X16306604},
	doi = {10/gjqsbb},
	abstract = {This paper presents a novel mobility monitoring system and some of its applications to address problems that would be solved in a Smart City, such as the optimization of traﬃc ﬂows in terms of trip-time and security (Smart Traﬃc), and the improvement of security or energetic issues inside buildings. The system tracks the movement of people and vehicles monitoring the radioelectric space, catching the {WiFi} and Bluetooth signals emitted by personal (smartphones) or on-board (hands-free) devices. A study has been conducted in four diﬀerent real scenarios, i.e. with real data gathered by the system: two related with people’s mobility (a public building and a discotheque); and two focused in traﬃc tracking (urban and intercity roads). The analysis has consisted on the application of diﬀerent data mining techniques to extract useful knowledge, traﬃc forecasting methods to perform accurate predictions, and statistical analyses to model and validate the system reliability (comparting to other real data sources). The obtained results show the viability and utility of the system in all the cases, along with some of its multiple applications for solving diﬀerent issues in a city.},
	pages = {163--179},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Fernández-Ares, A. and Mora, A.M. and Arenas, M.G. and García-Sanchez, P. and Romero, G. and Rivas, V. and Castillo, P.A. and Merelo, J.J.},
	urldate = {2021-04-17},
	date = {2017-11},
	langid = {english},
	note = {32 citations (Semantic Scholar/{DOI}) [2021-04-17]},
}

@article{zheng_traffic_2020,
	title = {Traffic Flow Forecast Through Time Series Analysis Based on Deep Learning},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9079512/},
	doi = {10/gjqkvf},
	abstract = {Traffic congestion is a thorny issue to many large and medium-sized cities, posing a serious threat to sustainable urban development. Recently, intelligent traffic system ({ITS}) has emerged as an effective tool to mitigate urban congestion. The key to the {ITS} lies in the accurate forecast of traffic flow. However, the existing forecast methods of traffic flow cannot adapt to the stochasticity and sheer length of traffic flow time series. To solve the problem, this paper relies on deep learning ({DL}) to forecast traffic flow through time series analysis. The authors developed a traffic flow forecast model based on the long short-term memory ({LSTM}) network. The proposed model was compared with two classic forecast models, namely, the autoregressive integrated moving average ({ARIMA}) model and the backpropagation neural network ({BPNN}) model, through long-term traffic flow forecast experiments, using an actual traffic flow time series from {OpenITS}. The experimental results show that the proposed {LSTM} network outperformed the classic models in prediction accuracy. Our research discloses the dynamic evolution law of traffic flow, and facilitates the decision-making of traffic management.},
	pages = {82562--82570},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Zheng, Jianhu and Huang, Mingfang},
	urldate = {2021-04-16},
	date = {2020},
	langid = {english},
	note = {2 citations (Semantic Scholar/{DOI}) [2021-04-16]},
}

@article{yang_implementation_2020,
	title = {An Implementation of High Efficient Smart Street Light Management System for Smart City},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9006786/},
	doi = {10/gjqkvg},
	abstract = {Street light are among the most common infrastructure in cities. Street lights and sensors can be combined to generate an interface of data collection. The analysis of massive data serves as an integral element of a smart city. This paper proposes a highly efﬁcient system for the conﬁguration, deployment, and management of smart street lights. The features of fast deployment and high scalability of the container-based system management result in virtual deployment. Additionally, for database design, {NoSQL} and in-memory databases are integrated to realize ﬂexible data management. In terms of data transmission, this paper designs an asymmetric key and an {SSH} encrypted tunnel. Moreover, when all the services are connected, it conducts legitimacy validation via a token. Therefore, this system can help meet the demands for data throughput, lowlatency, conﬁguration, and realization of a smart city. It boasts high efﬁciency and security. Besides, it offers a ﬂexible data storage and management service to facilitate the massive data processing of a smart city. With respect to experiments, this paper designs a street lighting simulation system with edge computing devices (consisting of a micro-controller, a sensor, and an {IP} camera) and a street lighting function. The system collects real-time sensed environmental data, enables live streaming of images, and offers an {API} for historical data query. This paper utilizes container-based virtualization to deploy all edge computing devices on the server and validates the feasibility of simultaneous operation of multiple container-based services on edge computing devices. This system has high commercial value.},
	pages = {38568--38585},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Yang, Yu-Sheng and Lee, Shih-Hsiung and Chen, Guan-Sheng and Yang, Chu-Sing and Huang, Yueh-Min and Hou, Ting-Wei},
	urldate = {2021-04-16},
	date = {2020},
	langid = {english},
	note = {3 citations (Semantic Scholar/{DOI}) [2021-04-16]},
}

@article{lau_traffic-aware_2015,
	title = {A traffic-aware street lighting scheme for Smart Cities using autonomous networked sensors},
	volume = {45},
	issn = {00457906},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045790615002153},
	doi = {10/gfsp42},
	abstract = {Street lighting is a ubiquitous utility, but sustaining its operation presents a heavy financial and environmental burden. Many schemes have been proposed which selectively dim lights to improve energy efficiency, but little consideration has been given to the usefulness of the resultant street lighting system. This paper proposes a real-time adaptive lighting scheme, which detects the presence of vehicles and pedestrians and dynamically adjusts their brightness to the optimal level. This improves the energy efficiency of street lighting and its usefulness; a streetlight utility model is presented to evaluate this. The proposed scheme is simulated using an environment modelling a road network, its users, and a networked communication system - and considers a real streetlight topology from a residential area. The proposed scheme achieves similar or improved utility to existing schemes, while consuming as little as 1-2\% of the energy required by conventional and state-of-the-art techniques.},
	pages = {192--207},
	journaltitle = {Computers \& Electrical Engineering},
	shortjournal = {Computers \& Electrical Engineering},
	author = {Lau, Sei Ping and Merrett, Geoff V. and Weddell, Alex S. and White, Neil M.},
	urldate = {2021-03-31},
	date = {2015-07},
	langid = {english},
	note = {50 citations (Semantic Scholar/{DOI}) [2021-03-31]},
}

@article{impedovo_vehicular_2019,
	title = {Vehicular Traffic Congestion Classification by Visual Features and Deep Learning Approaches: A Comparison},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/23/5213},
	doi = {10/ggd7q7},
	shorttitle = {Vehicular Traffic Congestion Classification by Visual Features and Deep Learning Approaches},
	abstract = {Automatic traﬃc ﬂow classiﬁcation is useful to reveal road congestions and accidents. Nowadays, roads and highways are equipped with a huge amount of surveillance cameras, which can be used for real-time vehicle identiﬁcation, and thus providing traﬃc ﬂow estimation. This research provides a comparative analysis of state-of-the-art object detectors, visual features, and classiﬁcation models useful to implement traﬃc state estimations. More speciﬁcally, three diﬀerent object detectors are compared to identify vehicles. Four machine learning techniques are successively employed to explore ﬁve visual features for classiﬁcation aims. These classic machine learning approaches are compared with the deep learning techniques. This research demonstrates that, when methods and resources are properly implemented and tested, results are very encouraging for both methods, but the deep learning method is the most accurately performing one reaching an accuracy of 99.9\% for binary traﬃc state classiﬁcation and 98.6\% for multiclass classiﬁcation.},
	pages = {5213},
	number = {23},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Impedovo, Donato and Balducci, Fabrizio and Dentamaro, Vincenzo and Pirlo, Giuseppe},
	urldate = {2021-04-13},
	date = {2019-11-28},
	langid = {english},
}

@article{wu_novel_2018,
	title = {A novel method of vehicle-pedestrian near-crash identification with roadside {LiDAR} data},
	volume = {121},
	issn = {00014575},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0001457518305591},
	doi = {10/gfmx8p},
	abstract = {Safety evaluation based on historical crashes usually has a lot of limitations. In previous studies, near-crashes are considered as surrogate data for safety evaluation. One challenge for the use of near-crashes data is the diﬃculty of data collection. The driving simulators and naturalistic driving data may not be suitable for safety evaluation at speciﬁc sites. The observational site-based methods such as human observers and video analysis also suﬀer from some limitations such as long time data processing or reduced performance inﬂuenced by weather or light condition. The roadside Light Detection and Ranging ({LiDAR})-enhanced infrastructure provides a new solution for real-time data collection without the impact from weather or light. The high-resolution trajectories of all road users can be obtained from roadside {LiDAR} data. This paper aims to ﬁll these gaps by presenting a method for near-crash identiﬁcation based on the trajectories of road users extracted from roadside {LiDAR} data. This paper focused on vehicle-pedestrian near-crash identiﬁcation particularly considering the increased risk of vehiclepedestrian conﬂicts. Three parameters: Time Diﬀerence to the Point of Intersection ({TDPI}); Distance between Stop Position and Pedestrian ({DSPP}); Vehicle-pedestrian speed-distance proﬁle, were developed for vehiclepedestrian near-crash identiﬁcation. The authors also recommended the thresholds for risk assessment of pedestrian safety. This method was coded into an automatic procedure for near-crash identiﬁcation. This method is expected to signiﬁcantly improve the current evaluation of pedestrian safety.},
	pages = {238--249},
	journaltitle = {Accident Analysis \& Prevention},
	shortjournal = {Accident Analysis \& Prevention},
	author = {Wu, Jianqing and Xu, Hao and Zheng, Yichen and Tian, Zong},
	urldate = {2021-04-19},
	date = {2018-12},
	langid = {english},
	note = {33 citations (Semantic Scholar/{DOI}) [2021-04-19]},
}

@inproceedings{khan_deep-learning_2019,
	location = {Islamabad, Pakistan},
	title = {Deep-Learning Based Vehicle Count and Free Parking Slot Detection System},
	isbn = {978-1-72814-001-8},
	url = {https://ieeexplore.ieee.org/document/9022687/},
	doi = {10/gjp33c},
	eventtitle = {2019 22nd International Multitopic Conference ({INMIC})},
	pages = {1--7},
	booktitle = {2019 22nd International Multitopic Conference ({INMIC})},
	publisher = {{IEEE}},
	author = {Khan, Gulraiz and Farooq, Muhammad Ali and Tariq, Zeeshan and Khan, Muhammad Usman Ghani},
	urldate = {2021-04-13},
	date = {2019-11},
}

@article{chen_cooperative_2016,
	title = {Cooperative Intersection Management: A Survey},
	volume = {17},
	issn = {1524-9050, 1558-0016},
	url = {http://ieeexplore.ieee.org/document/7244203/},
	doi = {10/f3p443},
	shorttitle = {Cooperative Intersection Management},
	pages = {570--586},
	number = {2},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	shortjournal = {{IEEE} Trans. Intell. Transport. Syst.},
	author = {Chen, Lei and Englund, Cristofer},
	urldate = {2021-04-13},
	date = {2016-02},
}

@article{barriga_smart_2019,
	title = {Smart Parking: A Literature Review from the Technological Perspective},
	volume = {9},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/21/4569},
	doi = {10/gjp328},
	shorttitle = {Smart Parking},
	abstract = {The development and high growth of the Internet of Things ({IoT}) have improved quality of life and strengthened different areas in society. Many cities worldwide are looking forward to becoming smart. One of the most popular use cases in smart cities is the implementation of smart parking solutions, as they allow people to optimize time, reduce fuel consumption, and carbon dioxide emissions. Smart parking solutions have a deﬁned architecture with particular components (sensors, communication protocols, and software solutions). Although there are only three components that compose a smart parking solution, it is important to mention that each component has many types that can be used in the deployment of these solutions. This paper identiﬁes the most used types of every component and highlights usage trends in the established analysis period. It provides a complementary perspective and represents a very useful source of information. The scientiﬁc community could use this information to decide regarding the selection of types of components to implement a smart parking solution. For this purpose, herein we review several works related to smart parking solutions deployment. To achieve this goal, a semi-cyclic adaptation of the action research methodology combined with a systematic review is used to select papers related to the subject of study. The most relevant papers were reviewed to identify subcategories for each component; these classiﬁcations are presented in tables to mark the relevance of each paper accordingly. Trends of usage in terms of sensors, protocols and software solutions are analyzed and discussed in every section. In addition to the trends of usage, this paper determines a guide of complementary features from the type of components that should be considered when implementing a smart parking solution.},
	pages = {4569},
	number = {21},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Barriga, Jhonattan J. and Sulca, Juan and León, José Luis and Ulloa, Alejandro and Portero, Diego and Andrade, Roberto and Yoo, Sang Guun},
	urldate = {2021-04-13},
	date = {2019-10-28},
	langid = {english},
}

@article{wuthishuwong_safe_2015,
	title = {Safe trajectory planning for autonomous intersection management by using vehicle to infrastructure communication},
	volume = {2015},
	issn = {1687-1499},
	url = {https://jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-015-0243-3},
	doi = {10/gjqkvj},
	abstract = {The development of autonomous vehicle or self-driving car integrates with the wireless communication technology which would be a forward step for road transportation in the near future. The autonomous crossing of an intersection with an autonomous vehicle will play a crucial role in the future of intelligent transportation system ({ITS}). The fundamental objectives of this work are to manage autonomous vehicles crossing an intersection with no collisions, maintaining that a vehicle drives continuously, and to decrease the waiting time at an intersection. In this paper, a discrete model of the one-way single intersection is designed. The vehicle-to-infrastructure (V2I) communication is implemented to exchange information between a vehicle and an intersection manager which is the roadside infrastructure. The safe trajectory of autonomous vehicles for the autonomous intersection management is determined and presented by using discrete mathematics.},
	pages = {33},
	number = {1},
	journaltitle = {{EURASIP} Journal on Wireless Communications and Networking},
	shortjournal = {J Wireless Com Network},
	author = {Wuthishuwong, Chairit and Traechtler, Ansgar and Bruns, Torsten},
	urldate = {2021-04-16},
	date = {2015-12},
	langid = {english},
	note = {28 citations (Semantic Scholar/{DOI}) [2021-04-16]},
}

@article{shashirangana_automated_2021,
	title = {Automated License Plate Recognition: A Survey on Methods and Techniques},
	volume = {9},
	abstract = {With the explosive growth in the number of vehicles in use, automated license plate recognition ({ALPR}) systems are required for a wide range of tasks such as law enforcement, surveillance, and toll booth operations. The operational speciﬁcations of these systems are diverse due to the differences in the intended application. For instance, they may need to run on handheld devices or cloud servers, or operate in low light and adverse weather conditions. In order to meet these requirements, a variety of techniques have been developed for license plate recognition. Even though there has been a notable improvement in the current {ALPR} methods, there is a requirement to be ﬁlled in {ALPR} techniques for a complex environment. Thus, many approaches are sensitive to the changes in illumination and operate mostly in daylight. This study explores the methods and techniques used in {ALPR} in recent literature. We present a critical and constructive analysis of related studies in the ﬁeld of {ALPR} and identify the open challenge faced by researchers and developers. Further, we provide future research directions and recommendations to optimize the current solutions to work under extreme conditions.},
	pages = {23},
	author = {Shashirangana, Jithmi and Padmasiri, Heshan and Meedeniya, Dulani and Perera, Charith},
	date = {2021},
	langid = {english},
}

@inproceedings{lah_smart_2017,
	location = {Putrajaya},
	title = {Smart traffic monitoring and control architecture and design},
	isbn = {978-1-5386-2126-4},
	url = {http://ieeexplore.ieee.org/document/8305418/},
	doi = {10/gjpqfc},
	eventtitle = {2017 {IEEE} 15th Student Conference on Research and Development ({SCOReD})},
	pages = {72--76},
	booktitle = {2017 {IEEE} 15th Student Conference on Research and Development ({SCOReD})},
	publisher = {{IEEE}},
	author = {Lah, Airull Azizi Awang and Latiff, L. A. and Dziyauddin, Rudzidatul Akmam and Kaidi, Hazilah Mad and Ahmad, Norulhusna},
	urldate = {2021-04-12},
	date = {2017-12},
	note = {4 citations (Semantic Scholar/{DOI}) [2021-04-12]},
}

@article{branquinho_efficient_2020,
	title = {An Efficient and Secure Alert System for {VANETs} to Improve Crosswalks’ Security in Smart Cities},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/9/2473},
	doi = {10/gjp323},
	abstract = {A key characteristic of Smart Cities is the ability to reduce conﬂicts between different agents coexisting in a dynamic system, such as the interaction between vehicles and pedestrians. This paper presents a system to augment the awareness of vehicle drivers regarding the presence of pedestrians in nearby crosswalks. The proposed system interconnects Road Side Units ({RSUs}), which are informed about the state of the crosswalks, and vehicles, in order to spread to vehicles, the information about the presence of pedestrians in crosswalks. To prevent false information spreading, {RSUs} sign the alert messages they broadcast and all vehicles can validate the signatures. This poses strong security requirements, such as non-repudiation of alert messages, as well as strong real-time requirements, such as minimum message validation delays among vehicles approaching a crosswalk of interest. To manage the signed alert messages, we are proposing Nimble Asymmetric Cryptography ({NAC}), which authenticates implicit broadcast messages. {NAC} minimizes the usage of asymmetric ciphers, which are fundamental to assure non-repudiation but increase performance penalties and uses hash chaining for source authentication of implicit messages.},
	pages = {2473},
	number = {9},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Branquinho, João and Senna, Carlos and Zúquete, André},
	urldate = {2021-04-13},
	date = {2020-04-27},
	langid = {english},
	note = {3 citations (Semantic Scholar/{DOI}) [2021-04-14]},
}

@article{luo_new_2019,
	title = {A New Framework of Intelligent Public Transportation System Based on the Internet of Things},
	volume = {7},
	abstract = {As a new paradigm of information technology, the Internet of Things ({IoT}) is attracting increasing attention from various industrial ﬁelds. It is foreseeable that the applications of {IoT} will be prevalent in the public transportation system and bring changes to the system in the near future. In this paper, we analyze the impact of {IoT} environment on the public transportation system, propose a new framework of the intelligent public transportation system based on {IoT}, and present the deployment of the elements, the communication network, and the three-tier architecture of the system in detail. We also present the information ﬂow, technical scheme, optimization model, and algorithm of the main modules of dynamic optimization of the system. The innovative points of this paper lie in: (1) a new framework for public transport system based on {IoT}, which integrates the scheduling problems of subway, bus, and shared taxi, is proposed for better-coordinated transfer solutions; (2) transport ﬂow prediction methods based on periodic patterns mining is proposed for road ﬂow analysis and passenger ﬂow analysis, and; (3) mathematical model and {DSS}-based evolutionary computation algorithm are proposed for solving the dynamic bus scheduling and controlling problems. The proposed intelligent transport system based on {IoT} can assist the decision makers to increase the utilization rate of the transport resources, improve the efﬁciency of scheduling, and reduce passengers’ traveling time.},
	pages = {15},
	author = {Luo, Xing-Gang and Zhang, Hong-Bo and Zhang, Zhong-Liang and Yu, Yang and Li, Ke},
	date = {2019},
	langid = {english},
}

@article{siddiqua_icafe_2019,
	title = {{iCAFE}: Intelligent Congestion Avoidance and Fast Emergency services},
	volume = {99},
	issn = {0167739X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X19302377},
	doi = {10/ggzjk6},
	shorttitle = {{iCAFE}},
	abstract = {Content Centric Network ({CCN}) has been envisioned as a paradigm shift from client server architecture. In smart cities, transportation plays an important role where integrated services facilitate citizens through the ease of use, safety, and convenience. In this work, we propose an integrated {CCN} for intelligent Congestion Avoidance and Fast Emergency ({iCAFE}) services delivery at road side accidents. One of the significant contributions is a novel content-centric {VANET}-based protocol called {iCAFE}, an efficient traffic control algorithm and five unique packet headers for effective communications. In case of accident, emergency packets are broadcast to {RSU} wherein the forwarding information based ({FIB}) and pending interest table ({PIT}) are updated accordingly. The {RSU} broadcasts interest packets to hospital and sends a rescue message to the ambulance. The {RSU} also informs nearest {RSUs} and vehicles to evacuate the affected lane. After the rescue process is completed, the data packet is unicast from the hospital to the {RSU} and the {PIT} and {FIB} are updated. {iCAFE} achieves a high packet delivery ratio ({PDR}) with minimum rescue delay (R-Delay), high throughput, minimum network load, smaller collision probability, and minimum packet drop fraction. The {iCAFE} results are compared with the traffic accidents reduction strategy ({TARS}).},
	pages = {365--375},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Siddiqua, Ayesha and Shah, Munam Ali and Khattak, Hasan Ali and Ud Din, Ikram and Guizani, Mohsen},
	urldate = {2021-04-14},
	date = {2019-10},
	langid = {english},
	note = {12 citations (Semantic Scholar/{DOI}) [2021-04-14]},
}

@article{balieu_life_2019,
	title = {Life cycle sustainability assessment of electrified road systems},
	volume = {20},
	issn = {1468-0629, 2164-7402},
	url = {https://www.tandfonline.com/doi/full/10.1080/14680629.2019.1588771},
	doi = {10/gfxkj4},
	pages = {S19--S33},
	issue = {sup1},
	journaltitle = {Road Materials and Pavement Design},
	shortjournal = {Road Materials and Pavement Design},
	author = {Balieu, R. and Chen, F. and Kringos, N.},
	urldate = {2021-04-14},
	date = {2019-04-30},
	langid = {english},
	note = {4 citations (Semantic Scholar/{DOI}) [2021-04-14]},
}

@article{alias_review_2019,
	title = {Review of Wireless Acoustic Sensor Networks for Environmental Noise Monitoring in Smart Cities},
	volume = {2019},
	issn = {1687-725X, 1687-7268},
	url = {https://www.hindawi.com/journals/js/2019/7634860/},
	doi = {10/gjqbbh},
	abstract = {Nowadays, more than half of the world’s population lives in urban areas. Since this proportion is expected to keep rising, the sustainable development of cities is of paramount importance to guarantee the quality of life of their inhabitants. Environmental noise is one of the main concerns that has to be addressed, due to its negative impact on the health of people. Different national and international noise directives and legislations have been defined during the past decades, which local authorities must comply with involving noise mapping, action plans, policing, and public awareness, among others. To this aim, a recent change in the paradigm for environmental noise monitoring has been driven by the rise of Internet of Things technology within smart cities through the design and development of wireless acoustic sensor networks ({WASNs}). This work reviews the most relevant {WASN}-based approaches developed to date focused on environmental noise monitoring. The proposals have moved from networks composed of high-accuracy commercial devices to the those integrated by ad hoc low-cost acoustic sensors, sometimes designed as hybrid networks with low and high computational capacity nodes. After describing the main characteristics of recent {WASN}-based projects, the paper also discusses several open challenges, such as the development of acoustic signal processing techniques to identify noise events, to allow the reliable and pervasive deployment of {WASNs} in urban areas together with some potential future applications.},
	pages = {1--13},
	journaltitle = {Journal of Sensors},
	shortjournal = {Journal of Sensors},
	author = {Alías, Francesc and Alsina-Pagès, Rosa Ma.},
	urldate = {2021-04-15},
	date = {2019-05-12},
	langid = {english},
	note = {22 citations (Semantic Scholar/{DOI}) [2021-04-15]},
}

@article{durrant-whyte_sensor_1988,
	title = {Sensor Models and Multisensor Integration},
	volume = {7},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/027836498800700608},
	doi = {10/dtk8hb},
	abstract = {We maintain that the key to intelligent fusion of disparate sensory information is to provide an effective model of sensor capabilities. A sensor model is an abstraction of the actual sensing process. It describes the information a sensor is able to provide, how this information is limited by the environ ment, how it can be enhanced by information obtained from other sensors, and how it may be improved by active use of the physical sensing device. The importance of having a model of sensor performance is that capabilities can be esti mated a priori and, thus, sensor strategies developed in line with information requirements., We describe a technique for modeling sensors and the information they provide. This model treats each sensor as an individual decision maker, acting as a member of a team with common goals. Each sensor is considered as a source of uncertain geometric information, able to communicate to, and coordinate its activities with, other members of the sens ing team. We treat three components of this sensor model: the observation model, which describes a sensor's measure ment characteristics; the dependency model, which describes a sensor's dependence on information from other sources; and the state model, which describes how a sensor's observa tions are affected by its location and internal state. We show how this mechanism can be used to manipulate, communi cate, and integrate uncertain sensor observations. We show that these sensor models can deal effectively with cooperative, competitive, and complementary interactions between differ ent disparate information sources.},
	pages = {97--113},
	number = {6},
	journaltitle = {The International Journal of Robotics Research},
	shortjournal = {The International Journal of Robotics Research},
	author = {Durrant-Whyte, Hugh F.},
	urldate = {2021-04-18},
	date = {1988-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd {STM}
244 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@inproceedings{jung_geosensor_2008,
	title = {Geosensor Data Abstraction for Environmental Monitoring Application},
	volume = {5266},
	isbn = {978-3-540-87472-0},
	doi = {10/cdnvzg},
	abstract = {Environmental observation applications are designed for monitoring phenomena using heterogeneous sensor data types and for providing derived and often integrated information. To effectively handle such a large variety of different sensors, both in scale and type and data volume, we propose a geosensor abstraction for large-scale geosensor networks. Our {SGSA}(Slope Grid for Sensor Data Abstraction) represents collected data in single grid-based layers, and allows for summarizing the measured data in various integrated grid layers. Within each cell, a slope vector is used to represents the trend of the observed sensor data. This slope is used as a simplifying factor for processing queries over several sensor types. To handle dynamic sensor data, the proposed abstraction model also supports rapid data update by using a mapping table. This model can be utilized as a data representation model in various geosensor network applications.},
	pages = {168--180},
	author = {Jung, Young and Nittel, Silvia},
	date = {2008-09-23},
	note = {12 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@article{jung_design_2011,
	title = {Design of Sensor Data Processing Steps in an Air Pollution Monitoring System},
	volume = {11},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3251980/},
	doi = {10/fr7579},
	abstract = {Environmental monitoring is required to understand the effects of various kinds of phenomena such as a flood, a typhoon, or a forest fire. To detect the environmental conditions in remote places, monitoring applications employ the sensor networks to detect conditions, context models to understand phenomena, and computing technology to process the large volumes of data. In this paper, we present an air pollution monitoring system to provide alarm messages about potentially dangerous areas with sensor data analysis. We design the data analysis steps to understand the detected air pollution regions and levels. The analyzed data is used to track the pollution and to give an alarm. This implemented monitoring system is used to mitigate the damages caused by air pollution.},
	pages = {11235--11250},
	number = {12},
	journaltitle = {Sensors (Basel, Switzerland)},
	shortjournal = {Sensors (Basel)},
	author = {Jung, Young Jin and Lee, Yang Koo and Lee, Dong Gyu and Lee, Yongmi and Nittel, Silvia and Beard, Kate and Nam, Kwang Woo and Ryu, Keun Ho},
	urldate = {2021-04-18},
	date = {2011-11-28},
	pmid = {22247663},
	pmcid = {PMC3251980},
	note = {16 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@article{fayyad_deep_2020,
	title = {Deep Learning Sensor Fusion for Autonomous Vehicle Perception and Localization: A Review},
	url = {https://www.researchgate.net/publication/343310310_Deep_Learning_Sensor_Fusion_for_Autonomous_Vehicle_Perception_and_Localization_A_Review},
	doi = {10/ghf4gx},
	shorttitle = {({PDF}) Deep Learning Sensor Fusion for Autonomous Vehicle Perception and Localization},
	abstract = {{PDF} {\textbar} Autonomous vehicles ({AV}) are expected to improve, reshape, and revolutionize the future of ground transportation. It is anticipated that ordinary... {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	journaltitle = {{ResearchGate}},
	author = {Fayyad, Jamil and Jaradat, Mohammad and Gruyer, Dominique and Najjaran, Homayoun},
	urldate = {2021-04-18},
	date = {2020},
	langid = {english},
	note = {14 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@inproceedings{mohammed_unsupervised_2017,
	location = {Eindhoven, Netherlands},
	title = {Unsupervised deep representation learning to remove motion artifacts in free-mode body sensor networks},
	isbn = {978-1-5090-6244-7},
	url = {http://ieeexplore.ieee.org/document/7936037/},
	doi = {10/gjqv4f},
	abstract = {In body sensor networks, the need to brace sensing devices ﬁrmly to the body raises a fundamental barrier to usability. In this paper, we examine the eﬀects of sensing from devices that do not face this mounting limitation. With sensors integrated into common pieces of clothing, we demonstrate that signals in such free-mode body sensor networks are contaminated heavily with motion artifacts leading to mean signal-to-noise ratios ({SNRs}) as low as -12 {dB}. Further, we show that motion artifacts at these {SNR} levels reduce the F1-score of a state-of-the-art algorithm for human-activity recognition by up to 77.1\%. In order to mitigate these artifacts, we evaluate the use of statistical (Kalman Filters) and data-driven (Neural Networks) techniques. We show that well-designed methods of representing {IMU} data with deep neural networks can increase {SNRs} in free-mode body-sensor networks from -12 {dB} to +18.2 {dB} and, as a result, improve the F1-score of recognizing gestures by 14.4\% and locomotion activities by 55.3\%.},
	eventtitle = {2017 {IEEE} 14th International Conference on Wearable and Implantable Body Sensor Networks ({BSN})},
	pages = {183--188},
	booktitle = {2017 {IEEE} 14th International Conference on Wearable and Implantable Body Sensor Networks ({BSN})},
	publisher = {{IEEE}},
	author = {Mohammed, Shoaib and Tashev, Ivan},
	urldate = {2021-04-18},
	date = {2017-05},
	langid = {english},
	note = {19 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@inproceedings{amini_variational_2018,
	location = {Madrid},
	title = {Variational Autoencoder for End-to-End Control of Autonomous Driving with Novelty Detection and Training De-biasing},
	isbn = {978-1-5386-8094-0},
	url = {https://ieeexplore.ieee.org/document/8594386/},
	doi = {10/gh4h86},
	abstract = {This paper introduces a new method for end-toend training of deep neural networks ({DNNs}) and evaluates it in the context of autonomous driving. {DNN} training has been shown to result in high accuracy for perception to action learning given sufﬁcient training data. However, the trained models may fail without warning in situations with insufﬁcient or biased training data. In this paper, we propose and evaluate a novel architecture for self-supervised learning of latent variables to detect the insufﬁciently trained situations. Our method also addresses training data imbalance, by learning a set of underlying latent variables that characterize the training data and evaluate potential biases. We show how these latent distributions can be leveraged to adapt and accelerate the training pipeline by training on only a fraction of the total dataset. We evaluate our approach on a challenging dataset for driving. The data is collected from a full-scale autonomous vehicle. Our method provides qualitative explanation for the latent variables learned in the model. Finally, we show how our model can be additionally trained as an end-to-end controller, directly outputting a steering control command for an autonomous vehicle.},
	eventtitle = {2018 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages = {568--575},
	booktitle = {2018 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	publisher = {{IEEE}},
	author = {Amini, Alexander and Schwarting, Wilko and Rosman, Guy and Araki, Brandon and Karaman, Sertac and Rus, Daniela},
	urldate = {2021-04-18},
	date = {2018-10},
	langid = {english},
	note = {28 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@inproceedings{chen_end--end_2020,
	location = {Las Vegas, {NV}, {USA}},
	title = {End-to-end Autonomous Driving Perception with Sequential Latent Representation Learning},
	isbn = {978-1-72816-212-6},
	url = {https://ieeexplore.ieee.org/document/9341020/},
	doi = {10/gjqvzw},
	abstract = {Current autonomous driving systems are composed of a perception system and a decision system. Both of them are divided into multiple subsystems built up with lots of human heuristics. An end-to-end approach might clean up the system and avoid huge efforts of human engineering, as well as obtain better performance with increasing data and computation resources. Compared to the decision system, the perception system is more suitable to be designed in an endto-end framework, since it does not require online driving exploration. In this paper, we propose a novel end-to-end approach for autonomous driving perception. A latent space is introduced to capture all relevant features useful for perception, which is learned through sequential latent representation learning. The learned end-to-end perception model is able to solve the detection, tracking, localization and mapping problems altogether with only minimum human engineering efforts and without storing any maps online. The proposed method is evaluated in a realistic urban driving simulator, with both camera image and lidar point cloud as sensor inputs. The codes and videos of this work are available at our github repo† and project website‡.},
	eventtitle = {2020 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages = {1999--2006},
	booktitle = {2020 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	publisher = {{IEEE}},
	author = {Chen, Jianyu and Xu, Zhuo and Tomizuka, Masayoshi},
	urldate = {2021-04-18},
	date = {2020-10-24},
	langid = {english},
	note = {1 citations (Semantic Scholar/{DOI}) [2021-04-18]},
	keywords = {Autonomous vehicles, Ergonomics, Location awareness, Robot vision systems, Software development management, Three-dimensional displays, Videos},
}

@article{cunneen_autonomous_2019,
	title = {Autonomous Vehicles and Embedded Artificial Intelligence: The Challenges of Framing Machine Driving Decisions},
	volume = {33},
	issn = {0883-9514},
	url = {https://doi.org/10.1080/08839514.2019.1600301},
	doi = {10/gjqv4d},
	shorttitle = {Autonomous Vehicles and Embedded Artificial Intelligence},
	abstract = {With the advent of autonomous vehicles society will need to confront a new set of risks which, for the first time, includes the ability of socially embedded forms of artificial intelligence to make complex risk mitigation decisions: decisions that will ultimately engender tangible life and death consequences. Since {AI} decisionality is inherently different to human decision-making processes, questions are therefore raised regarding how {AI} weighs decisions, how we are to mediate these decisions, and what such decisions mean in relation to others. Therefore, society, policy, and end-users, need to fully understand such differences. While {AI} decisions can be contextualised to specific meanings, significant challenges remain in terms of the technology of {AI} decisionality, the conceptualisation of {AI} decisions, and the extent to which various actors understand them. This is particularly acute in terms of analysing the benefits and risks of {AI} decisions. Due to the potential safety benefits, autonomous vehicles are often presented as significant risk mitigation technologies. There is also a need to understand the potential new risks which autonomous vehicle driving decisions may present. Such new risks are framed as decisional limitations in that artificial driving intelligence will lack certain decisional capacities. This is most evident in the inability to annotate and categorise the driving environment in terms of human values and moral understanding. In both cases there is a need to scrutinise how autonomous vehicle decisional capacity is conceptually framed and how this, in turn, impacts a wider grasp of the technology in terms of risks and benefits. This paper interrogates the significant shortcomings in the current framing of the debate, both in terms of safety discussions and in consideration of {AI} as a moral actor, and offers a number of ways forward.},
	pages = {706--731},
	number = {8},
	journaltitle = {Applied Artificial Intelligence},
	author = {Cunneen, Martin and Mullins, Martin and Murphy, Finbarr},
	urldate = {2021-04-18},
	date = {2019-07-03},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08839514.2019.1600301
6 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@article{recanatesi_predictive_2019,
	title = {Predictive learning extracts latent space representations from sensory observations},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/471987v3},
	doi = {10/ggd8fv},
	abstract = {{\textless}p{\textgreater}Neural networks have achieved many recent successes in solving sequential processing and planning tasks. Their success is often ascribed to the emergence of the task’s low-dimensional latent structure in the network activity – i.e., in the learned \textit{neural representations}. Similarly, biological neural circuits and in particular the hippocampus may produce representations that organize semantically related episodes. Here, we investigate the hypothesis that representations with low-dimensional latent structure, reflecting such semantic organization, result from learning to predict observations about the world. Specifically, we ask whether and when network mechanisms for sensory prediction coincide with those for extracting the underlying latent variables. Using a recurrent neural network model trained to predict a sequence of observations in a simulated spatial navigation task, we show that network dynamics exhibit low-dimensional but nonlinearly transformed representations of sensory inputs that capture the latent structure of the sensory environment. We quantify these results using nonlinear measures of intrinsic dimensionality which highlight the importance of the \textit{predictive} aspect of neural representations, and provide mathematical arguments for when and why these representations emerge. We focus throughout on how our results can aid the analysis and interpretation of experimental data.{\textless}/p{\textgreater}},
	pages = {471987},
	journaltitle = {{bioRxiv}},
	author = {Recanatesi, Stefano and Farrell, Matthew and Lajoie, Guillaume and Deneve, Sophie and Rigotti, Mattia and Shea-Brown, Eric},
	urldate = {2021-04-18},
	date = {2019-07-13},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results
8 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@inproceedings{thuruthel_drift-free_2020,
	title = {Drift-Free Latent Space Representation for Soft Strain Sensors},
	doi = {10/gjqvzf},
	abstract = {Soft strain sensors are becoming increasingly popular for obtaining tactile information in soft robotic applications. Diverse technological solutions are being investigated to design these sensors. Simultaneously, new methods for modeling these sensor are being proposed due to their highly nonlinear, time varying properties. Among them, machine learning based approaches, particularly using dynamic recurrent neural networks look the most promising. However, these complex networks have large number of free parameters to be tuned, making it difficult to apply them for real-world applications. This paper introduces the concept of transfer learning for modelling soft strain sensors, which allows us to utilize information learned in one task to be applied to another task. We demonstrate this technique on a passive anthropomorphic finger with embedded strain sensors used for two regression tasks. We show how the transfer learning approach can drastically reduce the number of free parameters to be tuned for learning new skills. This work is an important step towards scaling of sensor networks (algorithm-wise) and for using soft sensor data for high-level control tasks.},
	eventtitle = {2020 3rd {IEEE} International Conference on Soft Robotics ({RoboSoft})},
	pages = {138--143},
	booktitle = {2020 3rd {IEEE} International Conference on Soft Robotics ({RoboSoft})},
	author = {Thuruthel, T. G. and Gilday, K. and Iida, F.},
	date = {2020-05},
	note = {2 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@inproceedings{korthals_jointly_2019,
	title = {Jointly Trained Variational Autoencoder for Multi-Modal Sensor Fusion},
	abstract = {This work presents the novel multi-modal Variational Autoencoder approach M2VAE which is derived from the complete marginal joint log-likelihood. This allows the end-to-end training of Bayesian information fusion on raw data for all subsets of a sensor setup. Furthermore, we introduce the concept of in-place fusion - applicable to distributed sensing - where latent embeddings of observations need to be fused with new data. To facilitate in-place fusion even on raw data, we introduced the concept of a re-encoding loss that stabilizes the decoding and makes visualization of latent statistics possible. We also show that the M2VAE finds a coherent latent embedding, such that a single naïve Bayes classifier performs equally well on all permutations of a bi-modal Mixture-of-Gaussians signal. Finally, we show that our approach outperforms current {VAE} approaches on a bi-modal {MNIST} \& fashion-{MNIST} data set and works sufficiently well as a preprocessing on a tri-modal simulated camera \& {LiDAR} data set from the Gazebo simulator.},
	eventtitle = {2019 22th International Conference on Information Fusion ({FUSION})},
	pages = {1--8},
	booktitle = {2019 22th International Conference on Information Fusion ({FUSION})},
	author = {Korthals, T. and Hesse, M. and Leitner, J. and Melnik, A. and Rückert, U.},
	date = {2019-07},
	keywords = {Biological system modeling, Decoding, Deep Generative Model, Encoding, Laser radar, Multi-Modal Fusion, Robot sensing systems, Sensor fusion, Training, Variational Autoencoder},
}

@article{castanedo_review_2013,
	title = {A Review of Data Fusion Techniques},
	volume = {2013},
	issn = {2356-6140},
	url = {https://www.hindawi.com/journals/tswj/2013/704504/},
	doi = {10/gb7x39},
	abstract = {The integration of data and knowledge from several sources is known as data fusion. This paper summarizes the state of the data fusion field and describes the most relevant studies. We first enumerate and explain different classification schemes for data fusion. Then, the most common algorithms are reviewed. These methods and algorithms are presented using three different categories: (i) data association, (ii) state estimation, and (iii) decision fusion.},
	pages = {e704504},
	journaltitle = {The Scientific World Journal},
	author = {Castanedo, Federico},
	urldate = {2021-04-18},
	date = {2013-10-27},
	langid = {english},
	note = {Publisher: Hindawi},
}

@collection{puente_leon_information_2008,
	location = {Aachen},
	title = {Information fusion - Overview and taxonomy},
	isbn = {978-3-8322-7270-8},
	series = {Reports on distributed measurement systems},
	pagetotal = {232},
	number = {3},
	publisher = {Shaker},
	editor = {Puente León, Fernando},
	date = {2008},
	note = {{OCLC}: 244055459},
}
@article{wald_terms_1999,
	title = {Some terms of reference in data fusion},
	volume = {37},
	issn = {01962892},
	url = {http://ieeexplore.ieee.org/document/763269/},
	doi = {10/bs85zs},
	abstract = {This paper discusses the needs for a concept and harmonized terms of reference in data fusion. Already published definitions are analyzed. A new definition of the data fusion is proposed which has been set within an European working group. Several definitions and terms of reference are given which describe the information intervening in any problem of data fusion.},
	pages = {1190--1193},
	number = {3},
	journaltitle = {{IEEE} Transactions on Geoscience and Remote Sensing},
	shortjournal = {{IEEE} Trans. Geosci. Remote Sensing},
	author = {Wald, L.},
	urldate = {2021-04-18},
	date = {1999-05},
	langid = {english},
	note = {542 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@article{univ_valenciennes_cnrs_umr_8201-_lamih_f-59313_valenciennes_france_embedded_2018,
	title = {An Embedded Multi-Sensor Data Fusion Design for Vehicle Perception Tasks},
	issn = {17962021},
	url = {http://www.jocm.us/index.php?m=content&c=index&a=show&catid=186&id=1174},
	doi = {10/gjqt94},
	abstract = {Nowadays, multi-sensor architectures are popular to provide a better understanding of environment perception for intelligent vehicles. Using multiple sensors to deal with perception tasks in a rich environment is a natural solution. Most of the research works have focused on {PC}-based implementations for perception tasks and very few concerns have been addressed for customized embedded designs. In this paper, we propose a Multi-Sensor Data Fusion ({MSDF}) embedded design for vehicle perception tasks using stereo camera and Light Detection and Ranging ({LIDAR}) sensors. A modular and scalable architecture based on Zynq-7000 {SoC} was designed.},
	pages = {8--14},
	journaltitle = {Journal of Communications},
	shortjournal = {jcm},
	author = {{Univ. Valenciennes, CNRS, UMR 8201- LAMIH, F-59313 Valenciennes, France} and Bouain, Mokhtar and Ali, Karim M. A. and Berdjag, Denis and Fakhfakh, Nizar and Atitallah, Rabie Ben},
	urldate = {2021-04-18},
	date = {2018},
	langid = {english},
	note = {5 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@article{wang_multi-sensor_2020,
	title = {Multi-Sensor Fusion in Automated Driving: A Survey},
	volume = {8},
	issn = {2169-3536},
	doi = {10/ggvkt5},
	shorttitle = {Multi-Sensor Fusion in Automated Driving},
	abstract = {With the significant development of practicability in deep learning and the ultra-high-speed information transmission rate of 5G communication technology will overcome the barrier of data transmission on the Internet of Vehicles, automated driving is becoming a pivotal technology affecting the future industry. Sensors are the key to the perception of the outside world in the automated driving system and whose cooperation performance directly determines the safety of automated driving vehicles. In this survey, we mainly discuss the different strategies of multi-sensor fusion in automated driving in recent years. The performance of conventional sensors and the necessity of multi-sensor fusion are analyzed, including radar, {LiDAR}, camera, ultrasonic, {GPS}, {IMU}, and V2X. According to the differences in the latest studies, we divide the fusion strategies into four categories and point out some shortcomings. Sensor fusion is mainly applied for multi-target tracking and environment reconstruction. We discuss the method of establishing a motion model and data association in multi-target tracking. At the end of the paper, we analyzed the deficiencies in the current studies and put forward some suggestions for further improvement in the future. Through this investigation, we hope to analyze the current situation of multi-sensor fusion in the automated driving process and provide more efficient and reliable fusion strategies.},
	pages = {2847--2868},
	journaltitle = {{IEEE} Access},
	author = {Wang, Z. and Wu, Y. and Niu, Q.},
	date = {2020},
	note = {Conference Name: {IEEE} Access
18 citations (Semantic Scholar/{DOI}) [2021-04-18]},
	keywords = {Automated driving, Cameras, Laser radar, Sensor fusion, Sensor phenomena and characterization, Sensor systems, data association, deep learning, environmental reconstruction, intent analysis, multi-sensor fusion strategy, multi-target tracking},
}

@inproceedings{banerjee_online_2018,
	title = {Online Camera {LiDAR} Fusion and Object Detection on Hybrid Data for Autonomous Driving},
	doi = {10/gjqt92},
	abstract = {Environment perception for autonomous driving traditionally uses sensor fusion to combine the object detections from various sensors mounted on the car into a single representation of the environment. Non-calibrated sensors result in artifacts and aberration in the environment model, which makes tasks like free-space detection more challenging. In this study, we improve the {LiDAR} and camera fusion approach of Levinson and Thrun. We rely on intensity discontinuities and erosion and dilation of the edge image for increased robustness against shadows and visual patterns, which is a recurring problem in point cloud related work. Furthermore, we use a gradientfree optimizer instead of an exhaustive grid search to find the extrinsic calibration. Hence, our fusion pipeline is lightweight and able to run in real-time on a computer in the car. For the detection task, we modify the Faster R-{CNN} architecture to accommodate hybrid {LiDAR}-camera data for improved object detection and classification. We test our algorithms on the {KITTI} data set and locally collected urban scenarios. We also give an outlook on how radar can be added to the fusion pipeline via velocity matching.},
	eventtitle = {2018 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {1632--1638},
	booktitle = {2018 {IEEE} Intelligent Vehicles Symposium ({IV})},
	author = {Banerjee, K. and Notz, D. and Windelen, J. and Gavarraju, S. and He, M.},
	date = {2018-06},
	note = {{ISSN}: 1931-0587
20 citations (Semantic Scholar/{DOI}) [2021-04-18]},
	keywords = {Automobiles, Calibration, Cameras, Image edge detection, Laser radar, Object detection, Three-dimensional displays},
}

@inproceedings{gigan_sensor_2007,
	title = {Sensor Abstraction Layer: A unique software interface to effectively manage sensor networks},
	isbn = {1-4244-1502-0},
	doi = {10/b4q425},
	abstract = {In this paper, we present a Sensor Abstraction Layer ({SAL}) which provides instrument middleware architectures with a consistent and uniform view of heterogenous sensor networks regardless of the technologies involved. {SAL} is designed to run on sensor gateways (also referred to as base stations) and aggregates multiple sensing technologies. The many hardware disparities and specificities related to accessing, probing and piloting heterogenous sensors are hidden and abstracted by {SAL}, which in turn offers a single, stable and hardware-independent interface to manage the entire network. The result is a single software library which aggregates multiple heterogenous sensor networks, hides their disparities, provides consistent access and control functions, and allows middleware software to be technologyindependent. © 2007 {IEEE}.},
	pages = {479--484},
	booktitle = {Proceedings of the 2007 International Conference on Intelligent Sensors, Sensor Networks and Information Processing, {ISSNIP}},
	author = {Gigan, Gilles and Atkinson, Ian},
	urldate = {2021-03-23},
	date = {2007},
	note = {28 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {\#nosource},
}

@misc{nvidia_driveworks_2018,
	title = {{DRIVEWORKS} {SDK}},
	author = {Nvidia},
	date = {2018},
}

@inproceedings{jha_ml-based_2019,
	location = {Portland, {OR}, {USA}},
	title = {{ML}-Based Fault Injection for Autonomous Vehicles: A Case for Bayesian Fault Injection},
	isbn = {978-1-72810-057-9},
	url = {https://ieeexplore.ieee.org/document/8809495/},
	doi = {10/gg2vv2},
	shorttitle = {{ML}-Based Fault Injection for Autonomous Vehicles},
	abstract = {The safety and resilience of fully autonomous vehicles ({AVs}) are of signiﬁcant concern, as exempliﬁed by several headline-making accidents. While {AV} development today involves veriﬁcation, validation, and testing, end-to-end assessment of {AV} systems under accidental faults in realistic driving scenarios has been largely unexplored. This paper presents {DriveFI}, a machine learning-based fault injection engine, which can mine situations and faults that maximally impact {AV} safety, as demonstrated on two industry-grade {AV} technology stacks (from {NVIDIA} and Baidu). For example, {DriveFI} found 561 safety-critical faults in less than 4 hours. In comparison, random injection experiments executed over several weeks could not ﬁnd any safety-critical faults.},
	eventtitle = {2019 49th Annual {IEEE}/{IFIP} International Conference on Dependable Systems and Networks ({DSN})},
	pages = {112--124},
	booktitle = {2019 49th Annual {IEEE}/{IFIP} International Conference on Dependable Systems and Networks ({DSN})},
	publisher = {{IEEE}},
	author = {Jha, Saurabh and Banerjee, Subho and Tsai, Timothy and Hari, Siva K. S. and Sullivan, Michael B. and Kalbarczyk, Zbigniew T. and Keckler, Stephen W. and Iyer, Ravishankar K.},
	urldate = {2021-04-18},
	date = {2019-06},
	langid = {english},
	note = {24 citations (Semantic Scholar/{DOI}) [2021-04-18]},
}

@article{mohammed_perception_2020,
	title = {The Perception System of Intelligent Ground Vehicles in All Weather Conditions: A Systematic Literature Review},
	volume = {20},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/20/22/6532},
	doi = {10/gjqsw5},
	shorttitle = {The Perception System of Intelligent Ground Vehicles in All Weather Conditions},
	abstract = {Perception is a vital part of driving. Every year, the loss in visibility due to snow, fog, and rain causes serious accidents worldwide. Therefore, it is important to be aware of the impact of weather conditions on perception performance while driving on highways and urban traffic in all weather conditions. The goal of this paper is to provide a survey of sensing technologies used to detect the surrounding environment and obstacles during driving maneuvers in different weather conditions. Firstly, some important historical milestones are presented. Secondly, the state-of-the-art automated driving applications (adaptive cruise control, pedestrian collision avoidance, etc.) are introduced with a focus on all-weather activity. Thirdly, the most involved sensor technologies (radar, lidar, ultrasonic, camera, and far-infrared) employed by automated driving applications are studied. Furthermore, the difference between the current and expected states of performance is determined by the use of spider charts. As a result, a fusion perspective is proposed that can fill gaps and increase the robustness of the perception system.},
	pages = {6532},
	number = {22},
	journaltitle = {Sensors},
	author = {Mohammed, Abdul Sajeed and Amamou, Ali and Ayevide, Follivi Kloutse and Kelouwani, Sousso and Agbossou, Kodjo and Zioui, Nadjet},
	urldate = {2021-04-17},
	date = {2020-01},
	langid = {english},
	note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute
1 citations (Semantic Scholar/{DOI}) [2021-04-17]},
	keywords = {advanced driver assistance systems, autonomous vehicles, infrared camera, lidar, radar, road safety, sensor, sensor fusion, ultrasonic sensor, weather conditions},
}

@inproceedings{campbell_sensor_2018,
	title = {Sensor Technology in Autonomous Vehicles : A review},
	doi = {10/ggc793},
	shorttitle = {Sensor Technology in Autonomous Vehicles},
	abstract = {This paper will review the main sensor technologies used to create an autonomous vehicle. Sensors are key components for all types of autonomous vehicles because they can provide the data required to perceive the surrounding environment and therefore aid the decision-making process. This paper explains how each of these sensors work, their advantages and disadvantages and how sensor fusion techniques can be utilised to create a more optimum and efficient system for autonomous vehicles.},
	eventtitle = {2018 29th Irish Signals and Systems Conference ({ISSC})},
	pages = {1--4},
	booktitle = {2018 29th Irish Signals and Systems Conference ({ISSC})},
	author = {Campbell, S. and O'Mahony, N. and Krpalcova, L. and Riordan, D. and Walsh, J. and Murphy, A. and Ryan, C.},
	date = {2018-06},
	note = {24 citations (Semantic Scholar/{DOI}) [2021-04-17]},
	keywords = {Autonomous vehicles, Cameras, Global Positioning System, Laser radar, Localization, Perception, Sensor Fusion, Sensor fusion, Three-dimensional displays},
}

@inproceedings{handziski_flexible_2005,
	title = {Flexible hardware abstraction for wireless sensor networks},
	doi = {10/dhgqwp},
	abstract = {We present a flexible hardware abstraction architecture ({HAA}) that balances conflicting requirements of wireless sensor networks ({WSNs}) applications and the desire for increased portability and streamlined development of applications. Our three-layer design gradually adapts the capabilities of the underlying hardware platforms to the selected platform-independent hardware interface between the operating system core and the application code. At the same time, it allows the applications to utilize a platform's full capabilities-exported at the second layer, when the performance requirements outweigh the need for cross-platform compatibility. We demonstrate the practical value of our approach by presenting how it can be applied to the most important hardware modules that are found in a typical {WSN} platform. We support our claims using concrete examples from existing hardware abstractions in {TinyOS} and our implementation of the {MSP}430 platform that follows the architecture proposed in this paper.},
	eventtitle = {Proceeedings of the Second European Workshop on Wireless Sensor Networks, 2005.},
	pages = {145--157},
	booktitle = {Proceeedings of the Second European Workshop on Wireless Sensor Networks, 2005.},
	author = {Handziski, V. and Polastre, J. and Hauer, J.- and Sharp, C. and Wolisz, A. and Culler, D.},
	date = {2005-02},
	note = {95 citations (Semantic Scholar/{DOI}) [2021-04-17]},
	keywords = {Application software, Computer architecture, Computer science, Concrete, Energy efficiency, Energy resolution, Hardware, Impedance, Operating systems, Wireless sensor networks},
}

@thesis{rawat_environment_2019,
	title = {Environment Perception for Autonomous Driving : A 1/10 Scale Implementation Of Low Level Sensor Fusion Using Occupancy Grid Mapping},
	type = {Master},
	author = {Rawat, Pallav},
	date = {2019},
}

@article{schlegl_f-anogan_2019,
	title = {f-{AnoGAN}: Fast unsupervised anomaly detection with generative adversarial networks},
	volume = {54},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841518302640},
	doi = {10/gft3ts},
	shorttitle = {f-{AnoGAN}},
	abstract = {Obtaining expert labels in clinical imaging is difficult since exhaustive annotation is time-consuming. Furthermore, not all possibly relevant markers may be known and sufficiently well described a priori to even guide annotation. While supervised learning yields good results if expert labeled training data is available, the visual variability, and thus the vocabulary of findings, we can detect and exploit, is limited to the annotated lesions. Here, we present fast {AnoGAN} (f-{AnoGAN}), a generative adversarial network ({GAN}) based unsupervised learning approach capable of identifying anomalous images and image segments, that can serve as imaging biomarker candidates. We build a generative model of healthy training data, and propose and evaluate a fast mapping technique of new data to the {GAN}’s latent space. The mapping is based on a trained encoder, and anomalies are detected via a combined anomaly score based on the building blocks of the trained model – comprising a discriminator feature residual error and an image reconstruction error. In the experiments on optical coherence tomography data, we compare the proposed method with alternative approaches, and provide comprehensive empirical evidence that f-{AnoGAN} outperforms alternative approaches and yields high anomaly detection accuracy. In addition, a visual Turing test with two retina experts showed that the generated images are indistinguishable from real normal retinal {OCT} images. The f-{AnoGAN} code is available at https://github.com/{tSchlegl}/f-{AnoGAN}.},
	pages = {30--44},
	journaltitle = {Medical Image Analysis},
	shortjournal = {Medical Image Analysis},
	author = {Schlegl, Thomas and Seeböck, Philipp and Waldstein, Sebastian M. and Langs, Georg and Schmidt-Erfurth, Ursula},
	urldate = {2021-04-16},
	date = {2019-05-01},
	langid = {english},
	note = {152 citations (Semantic Scholar/{DOI}) [2021-04-16]},
	keywords = {Anomaly detection, Optical coherence tomography, Unsupervised learning, Wasserstein generative adversarial network},
}

@article{wang_generative_2018,
	title = {Generative adversarial network based novelty detection usingminimized reconstruction error},
	volume = {19},
	issn = {2095-9230},
	url = {https://doi.org/10.1631/FITEE.1700786},
	doi = {10/ggb267},
	abstract = {Generative adversarial network ({GAN}) is the most exciting machine learning breakthrough in recent years, and it trains the learning model by finding the Nash equilibrium of a two-player zero-sum game. {GAN} is composed of a generator and a discriminator, both trained with the adversarial learning mechanism. In this paper, we introduce and investigate the use of {GAN} for novelty detection. In training, {GAN} learns from ordinary data. Then, using previously unknown data, the generator and the discriminator with the designed decision boundaries can both be used to separate novel patterns from ordinary patterns. The proposed {GAN}-based novelty detection method demonstrates a competitive performance on the {MNIST} digit database and the Tennessee Eastman ({TE}) benchmark process compared with the {PCA}-based novelty detection methods using Hotelling’s T2 and squared prediction error statistics.},
	pages = {116--125},
	number = {1},
	journaltitle = {Frontiers of Information Technology \& Electronic Engineering},
	shortjournal = {Frontiers Inf Technol Electronic Eng},
	author = {Wang, Huan-gang and Li, Xin and Zhang, Tao},
	urldate = {2021-04-16},
	date = {2018-01-01},
	langid = {english},
	note = {15 citations (Semantic Scholar/{DOI}) [2021-04-16]},
}

@inproceedings{liniger_safe_2020,
	title = {Safe Motion Planning for Autonomous Driving using an Adversarial Road Model},
	isbn = {978-0-9923747-6-1},
	url = {http://www.roboticsproceedings.org/rss16/p044.pdf},
	doi = {10/gjp379},
	abstract = {This paper presents a game-theoretic path-following formulation where the opponent is an adversary road model. This formulation allows us to compute safe sets using tools from viability theory, that can be used as terminal constraints in an optimization-based motion planner. Based on the adversary road model, we ﬁrst derive an analytical discriminating domain, which even allows guaranteeing safety in the case when steering rate constraints are considered. Second, we compute the discriminating kernel and show that the output of the gridding based algorithm can be accurately approximated by a fully connected neural network, which can again be used as a terminal constraint. Finally, we show that by using our proposed safe sets, an optimization-based motion planner can successfully drive on city and country roads with prediction horizons too short for other baselines to complete the task.},
	eventtitle = {Robotics: Science and Systems 2020},
	booktitle = {Robotics: Science and Systems {XVI}},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Liniger, Alex and Van Gool, Luc},
	urldate = {2021-04-14},
	date = {2020-07-12},
	langid = {english},
	note = {3 citations (Semantic Scholar/{DOI}) [2021-04-14]},
	keywords = {{ToDo}},
}

@article{yurtsever_survey_2020,
	title = {A Survey of Autonomous Driving: Common Practices and Emerging Technologies},
	volume = {8},
	issn = {21693536},
	doi = {10/ggqtxd},
	abstract = {Automated driving systems ({ADSs}) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with {ADSs} are on the rise. The full potential of {ADSs} cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for {ADS} development.},
	pages = {58443--58469},
	journaltitle = {{IEEE} Access},
	author = {Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1906.05113},
	note = {113 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous vehicles, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, automation, control, intelligent transportation systems, intelligent vehicles, robotics},
}

@inproceedings{kocic_sensors_2018,
	location = {Belgrade},
	title = {Sensors and Sensor Fusion in Autonomous Vehicles},
	isbn = {978-1-5386-7171-9},
	url = {https://ieeexplore.ieee.org/document/8612054/},
	doi = {10/gjppt2},
	abstract = {In this paper, we are presenting a short overview of the sensors and sensor fusion in autonomous vehicles. We focused on the sensor fusion from the key sensors in autonomous vehicles: camera, radar, and lidar. The current state-of-the-art in this area will be presented, such as 3D object detection method for leveraging both image and 3D point cloud information, moving object detection and tracking system, and occupancy grid mapping used for navigation and localization in dynamic environments. It is shown that including more sensors into sensor fusion system benefits with better performance and the robustness of the solution. Moreover, usage of camera data in localization and mapping, that is traditionally solved by radar and lidar data, improves the perceived model of the environment. Sensor fusion has a crucial role in autonomous systems overall, therefore this is one of the fastest developing areas in the autonomous vehicles domain.},
	eventtitle = {2018 26th Telecommunications Forum ({TELFOR})},
	pages = {420--425},
	booktitle = {2018 26th Telecommunications Forum ({TELFOR})},
	publisher = {{IEEE}},
	author = {Kocic, Jelena and Jovicic, Nenad and Drndarevic, Vujo},
	urldate = {2021-04-12},
	date = {2018-11},
	langid = {english},
	note = {36 citations (Semantic Scholar/{DOI}) [2021-04-12]},
}

@article{yeong_sensor_2021,
	title = {Sensor and Sensor Fusion Technology in Autonomous Vehicles: A Review},
	volume = {21},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/6/2140},
	doi = {10/gjpptz},
	shorttitle = {Sensor and Sensor Fusion Technology in Autonomous Vehicles},
	abstract = {With the signiﬁcant advancement of sensor and communication technology and the reliable application of obstacle detection techniques and algorithms, automated driving is becoming a pivotal technology that can revolutionize the future of transportation and mobility. Sensors are fundamental to the perception of vehicle surroundings in an automated driving system, and the use and performance of multiple integrated sensors can directly determine the safety and feasibility of automated driving vehicles. Sensor calibration is the foundation block of any autonomous system and its constituent sensors and must be performed correctly before sensor fusion and obstacle detection processes may be implemented. This paper evaluates the capabilities and the technical performance of sensors which are commonly employed in autonomous vehicles, primarily focusing on a large selection of vision cameras, {LiDAR} sensors, and radar sensors and the various conditions in which such sensors may operate in practice. We present an overview of the three primary categories of sensor calibration and review existing open-source calibration packages for multi-sensor calibration and their compatibility with numerous commercial sensors. We also summarize the three main approaches to sensor fusion and review current state-of-the-art multi-sensor fusion techniques and algorithms for object detection in autonomous driving applications. The current paper, therefore, provides an end-to-end review of the hardware and software methods required for sensor fusion object detection. We conclude by highlighting some of the challenges in the sensor fusion ﬁeld and propose possible future research directions for automated driving systems.},
	pages = {2140},
	number = {6},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Yeong, De Jong and Velasco-Hernandez, Gustavo and Barry, John and Walsh, Joseph},
	urldate = {2021-04-12},
	date = {2021-03-18},
	langid = {english},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-04-12]},
}

@article{marti_review_2019,
	title = {A Review of Sensor Technologies for Perception in Automated Driving},
	volume = {11},
	issn = {1939-1390, 1941-1197},
	url = {https://ieeexplore.ieee.org/document/8846569/},
	doi = {10/ggmq5q},
	abstract = {After more than 20 years of research, {ADAS} are common in modern vehicles available in the market. Automated Driving systems, still in research phase and limited in their capabilities, are starting early commercial tests in public roads. These systems rely on the information provided by on-board sensors, which allow to describe the state of the vehicle, its environment and other actors. Selection and arrangement of sensors represent a key factor in the design of the system. This survey reviews existing, novel and upcoming sensor technologies, applied to common perception tasks for {ADAS} and Automated Driving. They are put in context making a historical review of the most relevant demonstrations on Automated Driving, focused on their sensing setup. Finally, the article presents a snapshot of the future challenges for sensing technologies and perception, finishing with an overview of the commercial initiatives and manufacturers alliances that will show the intention of the market in sensors technologies for Automated Vehicles.},
	pages = {94--108},
	number = {4},
	journaltitle = {{IEEE} Intelligent Transportation Systems Magazine},
	shortjournal = {{IEEE} Intell. Transport. Syst. Mag.},
	author = {Marti, Enrique and de Miguel, Miguel Angel and Garcia, Fernando and Perez, Joshue},
	urldate = {2021-04-12},
	date = {2019},
	langid = {english},
	note = {15 citations (Semantic Scholar/{DOI}) [2021-04-12]},
}

@article{eslami_neural_2018,
	title = {Neural scene representation and rendering},
	volume = {360},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aar6170},
	doi = {10/gdm9hz},
	abstract = {Scene representation – the process of converting visual sensory data into concise descriptions –is a requirement for intelligent behaviour. Recent work has shown that neural networks excel at this task when provided large labelled datasets. However, removing the reliance on human labelling remains an important open problem. To this end, we introduce the Generative Query Network ({GQN}), a framework within which machines learn to represent scenes using only their own sensors. The {GQN} takes as input images of a scene taken from different viewpoints, constructs an internal representation, and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The {GQN} demonstrates representation learning without human labels or domain knowledge, paving the way towards machines that autonomously learn to understand the world around them.},
	pages = {1204--1210},
	number = {6394},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Eslami, S. M. Ali and Jimenez Rezende, Danilo and Besse, Frederic and Viola, Fabio and Morcos, Ari S. and Garnelo, Marta and Ruderman, Avraham and Rusu, Andrei A. and Danihelka, Ivo and Gregor, Karol and Reichert, David P. and Buesing, Lars and Weber, Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz, Neil and King, Helen and Hillier, Chloe and Botvinick, Matt and Wierstra, Daan and Kavukcuoglu, Koray and Hassabis, Demis},
	urldate = {2021-04-08},
	date = {2018-06-15},
	langid = {english},
	note = {280 citations (Semantic Scholar/{DOI}) [2021-04-08]},
	keywords = {{ToDo}},
}

@article{kuhn_introspective_2020,
	title = {Introspective Failure Prediction for Autonomous Driving Using Late Fusion of State and Camera Information},
	issn = {1558-0016},
	doi = {10/gjn25h},
	abstract = {We present an introspective failure prediction approach for autonomous vehicles. In autonomous driving, complex or unknown scenarios can cause a disengagement of the self-driving system. Disengagements can be triggered either by automatic safety measures or by human intervention. We propose to use recorded disengagement sequences from test drives as training data to learn to predict future failures. The system then learns introspectively from its own previous mistakes. In order to predict failures as early as possible, we propose a machine learning approach where sequences of sensor data are classified as either failure or success. The car itself is treated as a black box. Our method combines two sensor modalities that contain different types of information. An image-based model learns to detect generally challenging situations such as crowded intersections accurately multiple seconds in advance. A state data based model allows to detect fast changes immediately before a failure, such as sudden braking or swerving. The outcome of the individual models is fused by averaging the individual failure probabilities. We evaluate our approach on a data set provided by the {BMW} Group containing 14 hours of autonomous driving. The proposed late fusion approach allows for predicting failures at an accuracy of more than 85\% seven seconds in advance, at a false positive rate of 20\%. The proposed method outperforms state-of-the-art failure prediction by more than 15\% while being a flexible framework that allows for straightforward addition of further sensor modalities.},
	pages = {1--15},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	author = {Kuhn, C. B. and Hofbauer, M. and Petrovic, G. and Steinbach, E.},
	date = {2020},
	note = {Conference Name: {IEEE} Transactions on Intelligent Transportation Systems},
	keywords = {Accidents, Automobiles, Autonomous vehicles, Data models, Estimation, Failure prediction, Predictive models, {ToDo}, Uncertainty, autonomous driving, introspection., machine learning},
}

@thesis{mancini_towards_2020,
	title = {Towards Recognizing New Semantic Concepts in New Visual Domains},
	url = {http://arxiv.org/abs/2012.09058},
	abstract = {Deep learning models heavily rely on large scale annotated datasets for training. Unfortunately, datasets cannot capture the infinite variability of the real world, thus neural networks are inherently limited by the restricted visual and semantic information contained in their training set. In this thesis, we argue that it is crucial to design deep architectures that can operate in previously unseen visual domains and recognize novel semantic concepts. In the first part of the thesis, we describe different solutions to enable deep models to generalize to new visual domains, by transferring knowledge from a labeled source domain(s) to a domain (target) where no labeled data are available. We will show how variants of batch-normalization ({BN}) can be applied to different scenarios, from domain adaptation when source and target are mixtures of multiple latent domains, to domain generalization, continuous domain adaptation, and predictive domain adaptation, where information about the target domain is available only in the form of metadata. In the second part of the thesis, we show how to extend the knowledge of a pretrained deep model to new semantic concepts, without access to the original training set. We address the scenarios of sequential multi-task learning, using transformed task-specific binary masks, open-world recognition, with end-to-end training and enforced clustering, and incremental class learning in semantic segmentation, where we highlight and address the problem of the semantic shift of the background class. In the final part, we tackle a more challenging problem: given images of multiple domains and semantic categories (with their attributes), how to build a model that recognizes images of unseen concepts in unseen domains? We also propose an approach based on domain and semantic mixing of inputs and features, which is a first, promising step towards solving this problem.},
	type = {phdthesis},
	author = {Mancini, Massimiliano},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2012.09058},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
}

@inproceedings{kabadayi_virtual_2006,
	title = {Virtual sensors: Abstracting data from physical sensors},
	volume = {2006},
	isbn = {0-7695-2593-8},
	doi = {10/bj7wq4},
	abstract = {Sensor networks are becoming increasingly pervasive. Existing methods of aggregation in sensor networks offer mostly standard mathematical operators over homogeneous data types. In this paper, we instead focus on supporting emerging scenarios in which applications will need to extract abstracted measurements from diverse sets of sensor network nodes. This paper introduces the virtual sensors abstraction that enables an application developer to programmatically specify an application's high-level data requirements. This paper reports on our initial work with the virtual sensors and the results of our prototype implementation. © 2006 {IEEE}.},
	pages = {587--592},
	booktitle = {Proceedings - {WoWMoM} 2006: 2006 International Symposium on a World of Wireless, Mobile and Multimedia Networks},
	author = {Kabadayi, Sanem and Pridgen, Adam and Julien, Christine},
	urldate = {2021-03-23},
	date = {2006},
	note = {131 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {\#nosource},
}

@thesis{huber_probabilistic_2009,
	title = {Probabilistic Framework for Sensor Management},
	url = {http://digbib.ubka.uni-karlsruhe.de/volltexte/1000012224},
	type = {phdthesis},
	author = {Huber, Marco},
	urldate = {2021-03-29},
	date = {2009},
	langid = {english},
	note = {Publisher: {KIT} Scientific Publishing
28 citations (Semantic Scholar/{DOI}) [2021-03-29]},
}

@thesis{otto_fusion_2013,
	title = {Fusion of Data from Heterogeneous Sensors with Distributed Fields of View and Situation Evaluation for Advanced Driver Assistance Systems},
	type = {phdthesis},
	author = {Otto, Carola},
	date = {2013},
	langid = {english},
}

@article{kim_visual_2021,
	title = {Visual Question Answering based on Local-Scene-Aware Referring Expression Generation},
	url = {http://arxiv.org/abs/2101.08978},
	abstract = {Visual question answering requires a deep understanding of both images and natural language. However, most methods mainly focus on visual concept; such as the relationships between various objects. The limited use of object categories combined with their relationships or simple question embedding is insufficient for representing complex scenes and explaining decisions. To address this limitation, we propose the use of text expressions generated for images, because such expressions have few structural constraints and can provide richer descriptions of images. The generated expressions can be incorporated with visual features and question embedding to obtain the question-relevant answer. A joint-embedding multi-head attention network is also proposed to model three different information modalities with co-attention. We quantitatively and qualitatively evaluated the proposed method on the {VQA} v2 dataset and compared it with state-of-the-art methods in terms of answer prediction. The quality of the generated expressions was also evaluated on the {RefCOCO}, {RefCOCO}+, and {RefCOCOg} datasets. Experimental results demonstrate the effectiveness of the proposed method and reveal that it outperformed all of the competing methods in terms of both quantitative and qualitative results.},
	pages = {1--32},
	author = {Kim, Jung-Jun and Lee, Dong-Gyu and Wu, Jialin and Jung, Hong-Gyu and Lee, Seong-Whan},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2101.08978},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
	keywords = {Read Abstract},
}

@thesis{woillez_stochastic_2018,
	title = {Stochastic description of rare events for complex dynamics in the Solar System},
	type = {phdthesis},
	author = {Woillez, Eric},
	date = {2018},
	keywords = {⛔ No {DOI} found},
}

@thesis{kesting_microscopic_2007,
	title = {Microscopic modeling of human and automated driving: Towards traffic-adaptive cruise control},
	abstract = {Efficient transportation systems are essential to the functioning and prosperity of modern, industrialized societies. Mobility is also an integral part of our quality of life, sense of self-fulfillment and personal freedom. Our traffic demands of today are predominantly served by individual motor vehicle travel which is the primary means of transportation. However, the limited road capacity and thus traffic congestion has become a severe prob- lem in many countries. On the one hand, traffic demand can only be affected indirectly by means of policy measures. On the other hand, an extension of transport infrastructure is no longer an appropriate or desirable option in densely populated areas. Moreover, construction requires high investments and maintenance is costly in the long run. There- fore, engineers are now seeking solutions to the questions of how the capacity of the road network could be used more efficiently and how operations can be improved by way of intelligent transportation systems ({ITS}). Achieving this efficiency through automated vehicle control is the long-standing vision in transport telematics. With the recent advent of advanced driver assistance systems, at least partly automated driving is already available for basic driving tasks such as acceler- ating and braking by means of adaptive cruise control ({ACC}) systems. An {ACC} system extends earlier cruise control to situations with significant traffic in which driving at con- stant speed is not possible. The driver cannot only adjust the desired velocity but also set a certain safety time gap determining the distance to the leading car when following slower vehicles. The task of the {ACC} system is to calculate the appropriate acceleration or deceleration as a function of the input quantities and the driver’s settings. Therefore, the actual distance and speed difference to the vehicle ahead is measured by means of a long-range radar sensor. The thesis is composed of two main parts. The first part deals with a microscopic traffic flow theory. Models describing the individual acceleration, deceleration and lane-changing behavior are formulated and the emerging collective traffic dynamics are investigated by means of numerical simulations. The models and simulation tools presented provide the methodical prerequisites for the second part of the thesis in which a novel concept of a traffic-adaptive control strategy for {ACC} systems is presented. The impact of such systems on the traffic dynamics can solely be investigated and assessed by traffic simulations.},
	type = {phdthesis},
	author = {Kesting, Dipl Arne and Kesting, A},
	date = {2007},
	keywords = {⛔ No {DOI} found},
}

@thesis{zhao_accelerated_2016,
	title = {Accelerated Evaluation of Automated Vehicles},
	url = {https://deepblue.lib.umich.edu/handle/2027.42/120657},
	abstract = {Automated Vehicles ({AVs}), which monitor the driving environment and conduct some or all of the driving tasks, must be evaluated thoroughly before their release and deployment. The challenges of {AV} evaluation stem from two facts. i) Crashes are exceedingly rare events. In the U.S., one needs to drive on average 530 thousand miles to experience a police-reported crash and nearly 100 million miles for a fatal crash. The low exposure to safety-critical scenarios makes the Naturalistic-Field Operational Tests (N- {FOT}) very time-consuming and expensive to conduct, in which prototype {AVs} are driven by volunteers or test engineers on public roads. ii) {AVs} can “cheat” to pass predefined tests. Traditionally, vehicle test protocols and test conditions are pre-defined and fixed. This is not a problem when the vehicle is “dumb”, but becomes a problem when the vehicle is intelligent and can be customized to excel in the predefined tests, and performance in other test conditions receives less attention. An evaluation approach that represents the real world but not as time-consuming as the N-{FOT} is needed to address the problems mentioned above. In this research, we propose an “Accelerated Evaluation” concept to accelerate the evaluations of {AV} by several orders of magnitude. The interactions between the {AV} and the surrounding Human-controlled Vehicles ({HVs}) are modeled based on the naturalistic driving data collected by the University of Michigan Transportation Research Institute in the Safety Pilot Model Deployment Program and the Integrated Vehicle-Based Safety Systems Program. Probabilities of conflict, crash, and severe injury are used as the main metrics to assess the safety of {AV} designs. In general, Accelerated Evaluation consists of six steps. 1) Collect a large quantity of naturalistic driving data. 2) Extract events that have potential multi-vehicle conflicts. 3) Model the conflict driving scenarios using stochastic models. 4) Reduce the non-safety-critical events by skewing the probability density functions. 5) Conduct Monte Carlo simulations with the skewed (accelerated) probability density function, resulting in more intense interactions between the {AV} and {HVs}. 6) “Skew back” the simulation results to calculate the performance of {AVs} under naturalistic driving conditions. The proposed approach can be used in computer simulations, human-in-the- loop tests with driving simulators, hardware-in-the-loop tests, or vehicle tests. Four methodologies were developed in this dissertation to form the basis of the Accelerated Evaluation concept. The first method is based on the likelihood analysis of naturalistic driving. The test scenarios are built as a probabilistic model based on time series driving data. The evaluation procedure is accelerated by reducing the relatively safe events that have a high likelihood of occurring. The second method provides a mathematical basis for the “skewing back” mechanism in step 5) based on the Importance Sampling theory, such that the statistical equivalence between the accelerated tests and naturalistic driving tests can be rigorously proved. The third method, the “Adaptive Accelerated Evaluation”, provides a procedure to recursively find the best way to skew the probabilistic density functions of {HVs} to maximally reduce the evaluation duration. Finally, the Accelerated Evaluation approach to analyzing the dynamic interactions between {AVs} and {HVs} was developed based on stochastic optimization techniques. Simulation results show that the accelerated tests can reduce the evaluation time of crash, injury or conflict events by 300 to 100,000 times. In other words, driving for 1,000 miles can expose the {AV} with challenging scenarios that take 300 thousand to 100 million miles in the real-world to encounter. This technique thus has the potential to dramatically reduce the development and validation time of {AVs}.},
	type = {phdthesis},
	author = {Zhao, Ding},
	date = {2016},
	keywords = {⛔ No {DOI} found},
}

@thesis{gunnemann-gholizadeh_machine_2018,
	title = {Machine Learning Methods for Detecting Rare Events in Temporal Data},
	type = {phdthesis},
	author = {Günnemann-Gholizadeh, Nikou},
	date = {2018},
}

@thesis{patel_simulation_2020,
	title = {A Simulation Environment with Reduced Reality Gap for Testing Autonomous Vehicles A Simulation Environment with Reduced Reality Gap for Testing Autonomous Vehicles By Submitted to the Faculty of Graduate Studies},
	type = {Master},
	author = {Patel, Kaival Kamleshkumar},
	date = {2020},
	keywords = {⛔ No {DOI} found},
}

@thesis{luis_integrated_2011,
	title = {An Integrated Architecture for Autonomous Vehicles Simulation},
	type = {Master},
	author = {Luis, José and Pereira, Ferrás},
	date = {2011},
}

@thesis{chen_augmenting_2019,
	title = {Augmenting Anomaly Detection for Autonomous Vehicles With Symbolic Rules},
	type = {Master},
	author = {Chen, Tianye},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@thesis{junietz_microscopic_2019,
	title = {Microscopic and macroscopic risk metrics for the safety validation of automated driving},
	url = {http://tuprints.ulb.tu-darmstadt.de/9282/},
	abstract = {… As the last falsification step has a high effort, design guidelines are established that lead to a potent metric if followed in the development process. Which methods and metrics can be used to extrapolate {MaR} of automated driving from critical scenes in field-testing …},
	type = {phdthesis},
	author = {Junietz, P M},
	date = {2019},
	keywords = {⛔ No {DOI} found},
}

@thesis{birkenbach_using_2020,
	title = {Using Synthetic Image Data to Improve Driving Functions Through Domain Adaptation in Latent Space},
	type = {Master},
	author = {Birkenbach, Marius},
	date = {2020},
}

@thesis{hu_knowledge-based_2004,
	title = {A knowledge-based genetic algorithm for path planning of mobile robots},
	type = {Master},
	author = {Hu, Yanrong},
	date = {2004},
}

@thesis{bogdoll_entwicklung_2014,
	title = {Entwicklung, Konstruktion und Realisierung eines prototypischen Laboraufbaus zur hochgenauen wellenfrontbasierten Justage mikrooptischer Systeme},
	rights = {All rights reserved},
	institution = {{RWTH} Aachen University},
	type = {Project},
	author = {Bogdoll, Daniel and Koretz, Mario and Maqua, Arne},
	date = {2014},
}

@thesis{bogdoll_konzeption_2019,
	title = {Konzeption, Design und Implementierung einer webbasierten, multi-sided Mitfahrplattform},
	rights = {All rights reserved},
	institution = {{RWTH} Aachen University},
	type = {Master},
	author = {Bogdoll, Daniel},
	date = {2019},
}

@thesis{bogdoll_trajektorienplanung_2016,
	title = {Trajektorienplanung für eine vollautomatisierte Valet-Parkfunktion},
	rights = {All rights reserved},
	institution = {{RWTH} Aachen University},
	type = {Bachelor},
	author = {Bogdoll, Daniel},
	date = {2016},
}

@thesis{luca_bertinetto__2019,
	title = {— {LEARNING} ( {TO} {LEARN} ) {FROM} {FEW} {EXAMPLES} — visual tracking and classification with limited data},
	type = {phdthesis},
	author = {{luca bertinetto}},
	date = {2019},
}

@inproceedings{k_m_learning_2018,
	location = {New Orleans, Louisiana},
	title = {Learning beyond Datasets: Knowledge Graph Augmented Neural Networks for Natural Language Processing},
	url = {http://aclweb.org/anthology/N18-1029},
	doi = {10/gjjvfx},
	shorttitle = {Learning beyond Datasets},
	abstract = {Machine Learning has been the quintessential solution for many {AI} problems, but learning models are heavily dependent on speciﬁc training data. Some learning models can be incorporated with prior knowledge using a Bayesian setup, but these learning models do not have the ability to access any organized world knowledge on demand. In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph ({KG}) fact triples for Natural Language Processing ({NLP}) tasks. Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism. We introduce a convolutionbased model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space. We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic {NLP} task. Using this method we show signiﬁcant improvement in performance for text classiﬁcation with 20Newsgroups (News20) \& {DBPedia} datasets, and natural language inference with Stanford Natural Language Inference ({SNLI}) dataset. We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a knowledge base.},
	eventtitle = {Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 1 (Long Papers)},
	pages = {313--322},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 1 (Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {K M, Annervaz and Basu Roy Chowdhury, Somnath and Dukkipati, Ambedkar},
	urldate = {2021-03-25},
	date = {2018},
	langid = {english},
	note = {25 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{fan_chen_socially_2018,
	title = {Socially Aware Motion Planning with Deep Reinforcement Learning},
	doi = {10/ggwb5v},
	author = {Fan Chen, Yu and Everett, Michael and Liu, Miao and How, Jonathan},
	date = {2018},
	note = {246 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@report{zoox_safety_2018,
	title = {Safety Innovation At Zoox},
	abstract = {We believe that just as the combustion engine took society from the age of the horse and carriage to the automobile, autonomous mobility technology will take us to the next era in transportation. Our vision is to connect people and places in wonderful ways, while at the same time improving public safety and reducing harmful greenhouse gas emissions. Our goal is to imagine this future and build it today, for a safer and more sustainable tomorrow. This report will provide the reader with a better understanding of how autonomous vehicle technology works, and specifically, how Zoox strives to set the bar for safety in autonomous mobility. We appreciate the opportunity to share the safety foundation we are building now, as we prepare to deploy autonomous vehicles for the public. Thank you for joining us on this journey.},
	pages = {1--30},
	author = {{Zoox}},
	date = {2018},
}

@online{cameron_introducing_2020,
	title = {Introducing Voyage Telessist},
	url = {https://news.voyage.auto/introducing-voyage-telessist-a085e4c1f691},
	author = {Cameron, Oliver},
	date = {2020},
}

@online{aurora_teleassist_2019,
	title = {Teleassist: How Humans Collaborate with the Aurora Driver},
	author = {{Aurora}},
	date = {2019},
}

@video{ericsson_new_2017,
	title = {New world record speed with 5G},
	author = {Ericsson},
	date = {2017},
}

@online{grzywaczewski_training_2017,
	title = {Training {AI} for Self-Driving Vehicles: the Challenge of Scale},
	author = {Grzywaczewski, Adam},
	date = {2017},
}

@online{khastgir_curious_2020,
	title = {The Curious Case of Operational Design Domain: What it is and is not?},
	url = {https://medium.com/@siddkhastgir/the-curious-case-of-operational-design-domain-what-it-is-and-is-not-e0180b92a3ae},
	author = {Khastgir, Siddartha},
	date = {2020},
}

@online{w_5g-frequenzen_2021,
	title = {5G-Frequenzen erklärt: Unterschiede und Gemeinsamkeiten zu 4G/3G/2G},
	url = {https://stadt-bremerhaven.de/5g-einfach-erklaert-unterschiede-und-gemeinsamkeiten-zu-4g-3g-2g/},
	author = {W, Marc},
	date = {2021},
}

@video{wiegelmann_analytics_2020,
	title = {Analytics for Autonomous Driving Large scale sensor data processing, Jan Wiegelmann, Autovia {AI}},
	url = {https://www.youtube.com/watch?v=KXcaiRCTOXY},
	author = {Wiegelmann, Jan},
	date = {2020},
	keywords = {★},
}

@online{zoox_how_2020,
	title = {How Zoox Uses {TeleGuidance} to Provide Remote Assistance to its Autonomous Vehicles},
	author = {Zoox},
	date = {2020},
}

@article{cipresso_past_2018,
	title = {The past, present, and future of virtual and augmented reality research: A network and cluster analysis of the literature},
	volume = {9},
	issn = {16641078},
	doi = {10/ggdbhr},
	abstract = {The recent appearance of low cost virtual reality ({VR}) technologies - like the Oculus Rift, the {HTC} Vive and the Sony {PlayStation} {VR} - and Mixed Reality Interfaces ({MRITF}) - like the Hololens - is attracting the attention of users and researchers suggesting it may be the next largest stepping stone in technological innovation. However, the history of {VR} technology is longer than it may seem: the concept of {VR} was formulated in the 1960s and the first commercial {VR} tools appeared in the late 1980s. For this reason, during the last 20 years, 100s of researchers explored the processes, effects, and applications of this technology producing 1000s of scientific papers. What is the outcome of this significant research work? This paper wants to provide an answer to this question by exploring, using advanced scientometric techniques, the existing research corpus in the field. We collected all the existent articles about {VR} in the Web of Science Core Collection scientific database, and the resultant dataset contained 21,667 records for {VR} and 9,944 for augmented reality ({AR}). The bibliographic record contained various fields, such as author, title, abstract, country, and all the references (needed for the citation analysis). The network and cluster analysis of the literature showed a composite panorama characterized by changes and evolutions over the time. Indeed, whether until 5 years ago, the main publication media on {VR} concerned both conference proceeding and journals, more recently journals constitute the main medium of communication. Similarly, if at first computer science was the leading research field, nowadays clinical areas have increased, as well as the number of countries involved in {VR} research. The present work discusses the evolution and changes over the time of the use of {VR} in the main areas of application with an emphasis on the future expected {VR}'s capacities, increases and challenges. We conclude considering the disruptive contribution that {VR}/{AR}/{MRITF} will be able to get in scientific fields, as well in human communication and interaction, as already happened with the advent of mobile phones by increasing the use and the development of scientific applications (e.g., in clinical areas) and by modifying the social communication and interaction among people.},
	pages = {1--20},
	issue = {{NOV}},
	journaltitle = {Frontiers in Psychology},
	author = {Cipresso, Pietro and Giglioli, Irene Alice Chicchi and Raya, Mariano Alcañiz and Riva, Giuseppe},
	date = {2018},
	pmid = {30459681},
	note = {116 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Augmented reality, Computational psychometrics, Mathematical psychology, Measurement, Psychometrics, Quantitative psychology, Scientometrics, Virtual reality},
}

@article{breitenstein_systematization_2020,
	title = {Systematization of Corner Cases for Visual Perception in Automated Driving},
	doi = {10/gjjgw3},
	abstract = {One major task in automated driving is the development of robust and safe visual perception modules. It is of utmost importance that visual perception reacts adequately to so-called corner cases, which range from overexposure of the image sensor to unexpected and potentially dangerous traffic situations. Their detection thus has high significance both as an online system in the intelligent vehicle, but also in the extraction of relevant training and test data for perception modules. In this paper, we provide a systematization of corner cases for visual perception in automated driving, with the categories being structured by detection complexity. Furthermore, we discuss existing metrics and datasets which can be used for the evaluation of corner case detection methods depending on their suitability to provide beneficial information for the various categories.},
	pages = {1257--1264},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Breitenstein, Jasmin and Termohlen, Jan Aike and Lipinski, Daniel and Fingscheidt, Tim},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2102.05897},
	note = {4 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{geiger_are_2012,
	title = {Are we ready for autonomous driving? the {KITTI} vision benchmark suite},
	issn = {10636919},
	doi = {10/gf7nxj},
	abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/{SLAM} and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti © 2012 {IEEE}.},
	pages = {3354--3361},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
	date = {2012},
	note = {{ISBN}: 9781467312264
5405 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{reski_open_2020,
	title = {Open data exploration in virtual reality: a comparative study of input technology},
	volume = {24},
	issn = {14349957},
	url = {https://doi.org/10.1007/s10055-019-00378-w},
	doi = {10/gg5cjb},
	abstract = {In this article, we compare three different input technologies (gamepad, vision-based motion controls, room-scale) for an interactive virtual reality ({VR}) environment. The overall system is able to visualize (open) data from multiple online sources in a unified interface, enabling the user to browse and explore displayed information in an immersive {VR} setting. We conducted a user interaction study (n= 24 ; n= 8 per input technology, between-group design) to investigate experienced workload and perceived flow of interaction. Log files and observations allowed further insights and comparison of each condition. We have identified trends that indicate user preference of a visual (virtual) representation, but no clear trends regarding the application of physical controllers (over vision-based controls), in a scenario that encouraged exploration with no time limitations.},
	pages = {1--22},
	number = {1},
	journaltitle = {Virtual Reality},
	author = {Reski, Nico and Alissandrakis, Aris},
	date = {2020},
	note = {Publisher: Springer London
{ISBN}: 0123456789
7 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {3D gestural input, Comparative study, Gamepad, Room-scale virtual reality, Virtual reality, Vision-based motion controls},
}

@article{pek_using_2020,
	title = {Using online verification to prevent autonomous vehicles from causing accidents},
	volume = {2},
	issn = {25225839},
	url = {http://dx.doi.org/10.1038/s42256-020-0225-y},
	doi = {10/ghcb6v},
	abstract = {Ensuring that autonomous vehicles do not cause accidents remains a challenge. We present a formal verification technique for guaranteeing legal safety in arbitrary urban traffic situations. Legal safety means that autonomous vehicles never cause accidents although other traffic participants are allowed to perform any behaviour in accordance with traffic rules. Our technique serves as a safety layer for existing motion planning frameworks that provide intended trajectories for autonomous vehicles. We verify whether intended trajectories comply with legal safety and provide fallback solutions in safety-critical situations. The benefits of our verification technique are demonstrated in critical urban scenarios, which have been recorded in real traffic. The autonomous vehicle executed only safe trajectories, even when using an intended trajectory planner that was not aware of other traffic participants. Our results indicate that our online verification technique can drastically reduce the number of traffic accidents.},
	pages = {518--528},
	number = {9},
	journaltitle = {Nature Machine Intelligence},
	author = {Pek, Christian and Manzinger, Stefanie and Koschi, Markus and Althoff, Matthias},
	date = {2020},
	note = {Publisher: Springer {US}
3 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{luo_using_2020,
	title = {Using virtual reality to identify and modify risky pedestrian behaviors amongst Chinese children},
	volume = {21},
	issn = {1538957X},
	url = {https://doi.org/10.1080/15389588.2019.1694667},
	doi = {10/gjjrhh},
	abstract = {Objective: China has a high fatality rate for child pedestrians, which highlights the necessity of implementing more effective pedestrian safety training programs in elementary schools. We thus investigated the efficacy of using virtual reality ({VR}) as an instructional technology to identify and modify risky pedestrian behaviors among Chinese children. Methods: Seventy-nine children (grades 1 through 3) from three elementary schools in Hubei province participated and were categorized into urban (n = 20), migrant (n = 29), or rural (n = 30) students based on the schools’ locations. They completed a {VR} program comprising three street-crossing challenges to measure five pedestrian behaviors. The participants first attempted to complete the challenges by themselves in the first-time trial (T1) and then engaged in a personalized debriefing session before undertaking the challenges a second time (T2). Pedestrian performance for the two trials was compared by school location and grade level as between-subjects factors, and the rationale behind risky pedestrian behaviors was inductively analyzed. Results: Three risky pedestrian behaviors were observed in the program: dashing into the street, crossing on a blinking green light, and failing to check for traffic. Potential reasons for these behaviors included a lack of knowledge of road signs and traffic rules and the absence of daily adult supervision. The overall pedestrian performance increased from T1 to T2 with a moderate effect size (Ƞp2 = 0.59, p {\textless}.001). A significant main effect of the trials was found for the three pedestrian behaviors (for all values, p {\textless}.001); however, interactions of trial by location and trial by grade were nonsignificant in all univariate tests (for all values, p ≥.05). Conclusions: {VR} is an effective technology to diagnose and correct risky pedestrian behaviors among Chinese children when accompanied with individual debriefing and repetitive practices. School location and grade level had no significant influence on children’s pedestrian performance and learning outcomes, indicating the ubiquity of the pedestrian safety problem and the need for more effective instructional interventions.},
	pages = {108--113},
	number = {1},
	journaltitle = {Traffic Injury Prevention},
	author = {Luo, Heng and Yang, Tingting and Kwon, Sejung and Zuo, Mingzhang and Li, Wenhao and Choi, Ikseon},
	date = {2020},
	pmid = {31999476},
	note = {Publisher: Taylor \& Francis
4 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {China, Virtual reality, children, pedestrian behavior, safety training},
}

@article{ramos_detecting_2017,
	title = {Detecting unexpected obstacles for self-driving cars: Fusing deep learning and geometric modeling},
	doi = {10/ggf73n},
	abstract = {The detection of small road hazards, such as lost cargo, is a vital capability for self-driving cars. We tackle this challenging and rarely addressed problem with a vision system that leverages appearance, contextual as well as geometric cues. To utilize the appearance and contextual cues, we propose a new deep learning-based obstacle detection framework. Here a variant of a fully convolutional network is proposed to predict a pixel-wise semantic labeling of (i) free-space, (ii) on-road unexpected obstacles, and (iii) background. The geometric cues are exploited using a state-of-The-Art detection approach that predicts obstacles from stereo input images via model-based statistical hypothesis tests. We present a principled Bayesian framework to fuse the semantic and stereo-based detection results. The mid-level Stixel representation is used to describe obstacles in a flexible, compact and robust manner. We evaluate our new obstacle detection system on the Lost and Found dataset, which includes very challenging scenes with obstacles of only 5 cm height. Overall, we report a major improvement over the state-of-The-Art, with a performance gain of 27.4\%. In particular, we achieve a detection rate of over 90\% for distances of up to 50 m. Our system operates at 22 Hz on our self-driving platform.},
	pages = {1025--1032},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Ramos, Sebastian and Gehrig, Stefan and Pinggera, Peter and Franke, Uwe and Rother, Carsten},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1612.06573},
	note = {{ISBN}: 9781509048045
83 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{yu_bdd100k_2020,
	title = {{BDD}100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning},
	issn = {10636919},
	doi = {10/gg996m},
	abstract = {Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct {BDD}100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. {BDD}100K opens the door for future studies in this important venue.},
	pages = {2633--2642},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1805.04687},
	note = {153 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{sun_scalability_2020,
	title = {Scalability in perception for autonomous driving: Waymo open dataset},
	issn = {10636919},
	doi = {10/ghbbpn},
	abstract = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large-scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality {LiDAR} and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+{LiDAR} dataset available based on our proposed geographical coverage metric. We exhaustively annotated this data with 2D (camera image) and 3D ({LiDAR}) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.},
	pages = {2443--2451},
	journaltitle = {{arXiv}},
	author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurélien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1912.04838},
	note = {172 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{wirth_pointatme_2019,
	title = {{PointAtMe}: Efficient 3D point cloud labeling in virtual reality},
	volume = {2019-June},
	doi = {10/gjjrhf},
	abstract = {Generating annotations which can be used to train new models has become an independent field of research within machine learning. Its goal is producing highly accurate annotations as cost efficient as possible. 3D point clouds are the common sensor output when recording 3D data from a mobile platform. The latest ways of annotating 3D point clouds include their visualization on a 2D screen. This method contradicts the goal of time-efficient annotating since it is unintuitive and therefore unnecessarily time consuming. We present a novel labeling technique in Virtual Reality. Using our tool, we accelerate the process of data annotation significantly compared to existing approaches. Furthermore, we will give the machine learning community access to our tool and create a new community-labeled dataset for autonomous driving. Furthermore we plan to set up an annotation benchmark in which primarily commercial annotation companies but also researchers active in annotation can take part in. We present results from an experimental plattform based on Oculus Rift indicating a huge potential for {VR} annotations.},
	pages = {1693--1698},
	issue = {Iv},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Wirth, Florian and Quchl, Jannik and Ota, Jeffrey and Stiller, Christoph},
	date = {2019},
	note = {{ISBN}: 9781728105604
3 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Benchmark, Dataset, Label tool, Machine learning},
}

@article{van_dam_experiments_2002,
	title = {Experiments in immersive virtual reality for scientific visualization},
	volume = {26},
	issn = {00978493},
	doi = {10/bvx9d9},
	abstract = {This article provides a snapshot of immersive virtual reality ({IVR}) use for scientific visualization, in the context of the evolution of computing in general and of user interfaces in particular. The main thesis of this article is that {IVR} has great potential for dealing with the serious problem of exponentially growing scientific datasets. Our ability to produce large datasets both through numerical simulation and through data acquisition via sensors is outrunning our ability to make sense of those datasets. While our idea of "large" datasets used to be measured in hundreds of gigabytes, based at least in part on what we could easily store, manipulate, and display in real time, today's science and engineering are producing terabytes and soon even petabytes, both from observation via sensors and as output from numerical simulation. Clearly, visualization by itself will not solve the problem of understanding truly large datasets that would overwhelm both display capacity and the human visual system. We advocate a human-computer partnership that draws on the strengths of each partner, with algorithmic culling and feature-detection used to identify the small fraction of the data that should be visually examined in detail by the human. Our hope is that {IVR} will be a potent tool to let humans "see" patterns, trends, and anomalies in their data well beyond what they can do with conventional 3D desktop displays. © 2002 Published by Elsevier Science Ltd.},
	pages = {535--555},
	number = {4},
	journaltitle = {Computers and Graphics (Pergamon)},
	author = {Van Dam, Andries and Laidlaw, David H. and Simpson, Rosemary Michelle},
	date = {2002},
	note = {110 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@online{nvidia_driveworks_2020,
	title = {{DriveWorks} {SDK} Reference - Safety Force Field},
	url = {https://docs.nvidia.com/drive/driveworks-3.0/safetyforcefield_mainsection.html},
	author = {{Nvidia}},
	date = {2020},
}

@online{intel_c_2021,
	title = {C++ Library for Responsibility Sensitive Safety},
	url = {https://github.com/intel/ad-rss-lib},
	author = {{Intel}},
	date = {2021},
}

@article{zhao_safety_2020,
	title = {Safety Score: A Quantitative Approach to Guiding Safety-Aware Autonomous Vehicle Computing System Design},
	doi = {10/gjjg22},
	abstract = {High automated vehicles rely on the computing system in the car to understand the environment and make driving decisions. Therefore, computing system design is essential for ensuring the driving safety. However, to our knowledge, no clear guideline exists so far regarding how to guide the safety-aware autonomous vehicle ({AV}) computing system design. To understand the safety requirement of {AV} computing system, we performed a field study by operating industrial Level-4 {AV} fleets in multiple locations for three months. The field study indicates that traditional computing system performance metrics, such as tail latency, average latency, maximum latency, and timeout, cannot fully satisfy the safety requirement for {AV} computing system design. To address this issue, we propose the 'safety score' as a primary metric for measuring the level of safety in {AV} computing system design.},
	pages = {1479--1485},
	issue = {Iv},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Zhao, Hengyu and Zhang, Yubo and Meng, Pingfan and Shi, Hui and Li, Li Erran and Lou, Tiancheng and Zhao, Jishen},
	date = {2020},
	note = {{ISBN}: 9781728166735
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@online{edge_case_research_edge_2021,
	title = {Edge Case Research},
	author = {Edge Case Research},
	urldate = {2021-03-25},
	date = {2021},
}

@online{krempl_aufsicht_2021,
	title = {Aufsicht für Robo-Autos: "Der Mensch ist ein schlechter Überwacher"},
	author = {Krempl, Stefan},
	date = {2021},
}

@online{carritech_5g_2020,
	title = {5G Spectrum – Low, Mid and High Bands Explained},
	url = {https://www.carritech.com/news/5g-spectrum/},
	author = {{Carritech}},
	date = {2020},
}

@unpublished{chrpa_knowledge_2016,
	title = {Knowledge Engineering in Planning Representation Matters},
	author = {Chrpa, Lukáš and Vallati, Mauro},
	date = {2016},
	keywords = {⛔ No {DOI} found},
}

@article{duh_incorporating_2015,
	title = {Incorporating Knowledge Bases into Deep Learning},
	pages = {9--11},
	issue = {Figure 2},
	author = {Duh, Kevin},
	date = {2015},
}

@online{eliot_remote_2017,
	title = {Remote Piloting is a Self-Driving Car Crutch},
	url = {https://www.aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/},
	author = {Eliot, Lance},
	date = {2017},
}

@online{eliot_zoox_2019,
	title = {Zoox Cofounder Nails It: Wireless Should Not Be A Safety Case For Self-Driving Cars},
	url = {https://www.forbes.com/sites/lanceeliot/2019/10/10/zoox-cofounder-nails-it-wireless-should-not-be-a-safety-case-for-self-driving-cars/?sh=3ce92b1d548c},
	author = {Eliot, Lance},
	date = {2019},
}

@unpublished{fridman_mit_2017,
	title = {{MIT} 6.S094: Deep Reinforcement Learning for Motion Planning},
	url = {https://www.youtube.com/watch?v=QDzM8r3WgBw},
	author = {Fridman, Lex},
	date = {2017},
}

@online{garbade_understanding_2018,
	title = {Understanding few-shot learning in machine learning},
	url = {https://medium.com/quick-code/understanding-few-shot-learning-in-machine-learning-bede251a0f67},
	author = {Garbade, Michael},
	date = {2018},
	keywords = {⛔ No {DOI} found},
}

@unpublished{hornauer_studying_2017,
	title = {Studying Autonomous Driving Corner Cases},
	author = {Hornauer, Sascha and Yellapragada, Baladitya},
	date = {2017},
	keywords = {⛔ No {DOI} found},
}

@article{hoss_review_2021,
	title = {A Review of Testing Object-Based Environment Perception for Safe Automated Driving *},
	pages = {1--23},
	author = {Hoss, Michael},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.08460v1},
}

@article{houben_inspect_2020,
	title = {Inspect , Understand , Overcome : A Survey of Practical Methods for {AI} Safety},
	pages = {1--93},
	author = {Houben, Sebastian and Abrecht, Stephanie and Akila, Maram and Bär, Andreas and Brockherde, Felix},
	date = {2020},
}

@article{kyunghoon_cho_timothy_ha_gunmin_lee_and_songhwai_oh_deep_2019,
	title = {Deep Predictive {AutoDriving} Using Multi-Agent Joint Trajectory Prediction and Traffic Rules},
	url = {https://www.youtube.com/watch?v=jpxVm0zL_TM},
	author = {{Kyunghoon Cho, Timothy Ha, Gunmin Lee, and Songhwai Oh}},
	date = {2019},
}

@article{tottel_revisiting_2020,
	title = {Revisiting the Space Time Cube in the Context of Multisensorial Intelligent Infrastructure for Behavior Analysis of Interacting Traffic Participants},
	pages = {1--6},
	author = {Töttel, Lars and Zipfl, Maximilian and Ren, Marc and Rinkewitz, Felix and Schulz, Patrick and Marius, J Z},
	date = {2020},
}

@online{lewandrowski_bringt_2021,
	title = {Bringt uns Teleoperating dem autonomen Fahren näher?},
	url = {https://intellicar.de/podcast/bringt-uns-teleoperating-dem-autonomen-fahren-naeher/},
	author = {Lewandrowski, Moritz},
	date = {2021},
}

@online{safead_data_2020,
	title = {Data augmentation},
	author = {{SafeAD}},
	urldate = {2021-03-25},
	date = {2020},
}

@article{sharma_long-tailed_2020,
	title = {Long-Tailed Recognition Using Class-Balanced Experts},
	pages = {1--15},
	author = {Sharma, Saurabh and Yu, Ning and Fritz, Mario and Schiele, Bernt},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2004.03706v2},
}

@article{junietz_critical_2018,
	title = {{CRITICAL} {SCENARIOS} {FOR} {HUMAN} {DRIVERS} – {CRITICALITY} {METRIC}},
	number = {8},
	author = {Junietz, Philipp and Schneider, Jan and Winner, Hermann},
	date = {2018},
}

@article{wang_high-resolution_2018,
	title = {High-Resolution Image Synthesis and Semantic Manipulation with Conditional {GANs}},
	doi = {10/ggv75b},
	author = {Wang, Ting-chun and Zhu, Ming-yu Liu Jun-yan and Tao, Andrew and Kautz, Jan and Aug, C V},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1711.11585v2},
}

@online{boochs_knowledge-based_2019,
	title = {Knowledge-based object Detection in Image and Point cloud ({KnowDIP})},
	author = {Boochs, Frank and Ponciano, Jean-Jacques},
	date = {2019},
}

@online{ul_standard_standard_2020,
	title = {Standard for Evaluation of Autonomous Products},
	url = {https://www.shopulstandards.com/ProductDetail.aspx?productid=UL4600&ShowFreeviewModal=1},
	author = {{UL} Standard},
	date = {2020},
}

@online{berman_key_2019,
	title = {The key to autonomous vehicle safety is {ODD}},
	url = {https://www.sae.org/news/2019/11/odds-for-av-testing},
	author = {Berman, Bradley},
	date = {2019},
}

@online{pluta_vw_2021,
	title = {{VW} will autonome Autos per Satellit vernetzen},
	url = {https://www.golem.de/news/auto-vw-will-autonome-autos-per-satellit-vernetzen-2102-154431.html},
	author = {Pluta, Werner},
	date = {2021},
}

@article{deloitte_5g_2020,
	title = {5G smart cities whitepaper},
	issue = {June},
	author = {Deloitte},
	date = {2020},
}

@article{ahmad_new_2020,
	title = {New methods to define heavy-tailed distributions with applications to insurance data},
	volume = {14},
	issn = {1658-3655},
	url = {https://doi.org/10.1080/16583655.2020.1741942},
	doi = {10/gjjgwq},
	abstract = {Heavy-tailed distributions play an important role in modelling data in actuarial and financial sciences. In this article, nine new methods are suggested to define new distributions suitable for modelling data with an heavy right tail. For illustrative purposes, a special sub-model is considered in detail. Maximum likelihood estimators of the model parameters are obtained and a Monte Carlo simulation study is carried out to assess the behaviour of the estimators. Furthermore, some actuarial measures are calculated. A simulation study based on these actuarial measures is done. The usefulness of the proposed model is proved empirically by means of two real data sets. Finally, Bayesian analysis and performance of Gibbs sampling for the data sets are also carried out. {ARTICLE} {HISTORY}},
	pages = {359--382},
	number = {1},
	journaltitle = {Journal of Taibah University for Science},
	author = {Ahmad, Zubair and Mahmoudi, Eisa and Hamedani, G. G. and Kharazmi, Omid},
	date = {2020},
	note = {3 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Bayesian estimation, Monte Carlo simulation, Weibull distribution, actuarial measures, characterizations, heavy-tailed distributions, medical care insurance data, monte, vehicle insurance data, weibull distribution},
}

@article{abdelrahim_hybrid_2016,
	title = {Hybrid Machine Learning Approaches: A Method to Improve Expected Output of Semi-structured Sequential Data},
	doi = {10/gjjgwp},
	abstract = {This paper proposes an intuitive yet simple machine learning ({ML}) approach that consist of two generic algorithms augmenting one another to solve problems they are not designed to solve. Since most machine learning algorithms are designed for a particular dataset or task, combining multiple {ML} algorithms can greatly improve the overall result by either helping tune one another, generalize, or adapt to unknown tasks. In this paper, we attempt to augment the architecture of traditional Artificial Neural Network ({ANN}) with a state machine acting as a form of short term memory in addition to help divide the work amongst multiple modular {ANNs} through transitioning from state to state. The result is a larger non-stochastic network that is able to self adjust as it is fed input. We train and test the work on data that is outside either an Artificial Neural Network or a state-machine's normal capability with simplified music notation extracted from midi files. The extracted data are used to simulate inherently sequential data to test the principle. Finally, while we find many large improvements in the augmentation of the {ANN}'s architecture, but discuss further approaches to the system to improve generalization for new data.},
	pages = {342--345},
	journaltitle = {Proceedings - 2016 {IEEE} 10th International Conference on Semantic Computing, {ICSC} 2016},
	author = {Abdelrahim, Mohammed and Merlosy, Carlos and Wang, Taehyung},
	date = {2016},
	note = {Publisher: {IEEE}
{ISBN}: 9781509006618
1 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Adaption, Artificial Neural Network, Machine Learning, Sequence-based features, State-machines},
}

@article{andersson_benefits_2019,
	title = {Benefits and Costs of Autonomous Trucks and Cars},
	volume = {09},
	issn = {2160-0473},
	doi = {10/gjbjgt},
	abstract = {Autonomous vehicles are currently developed, and are expected to be introduced gradually. Society needs a basis for decisions regarding market interventions. This study identifies, quantifies and values the benefits and costs of autonomous trucks and cars considering generalized costs, external effects and social marginal cost pricing to consumers with Swedish data. The results show that the greatest benefits are saved driver costs for trucks and decreased travel time costs for car drivers. In the example calculations, capital costs may increase by 22 percent for cars and 36 percent for trucks for benefits to exceed costs in 2025. Subsidies are not needed since the producers and consumers get the major benefits and pay the costs.},
	pages = {121--145},
	number = {2},
	journaltitle = {Journal of Transportation Technologies},
	author = {Andersson, Peter and Ivehammar, Pernilla},
	date = {2019},
	note = {4 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous Vehicles, Self-Driving Vehicles, Benefi, autonomous vehicles, benefits, generalized cost, policy, self-driving vehicles, social marginal cost},
}

@article{bansal_visual_2020,
	title = {Visual Question Answering on Image Sets},
	volume = {12366 {LNCS}},
	issn = {16113349},
	doi = {10/gjjgwv},
	abstract = {We introduce the task of Image-Set Visual Question Answering ({ISVQA}), which generalizes the commonly studied single-image {VQA} problem to multi-image settings. Taking a natural language question and a set of images as input, it aims to answer the question based on the content of the images. The questions can be about objects and relationships in one or more images or about the entire scene depicted by the image set. To enable research in this new topic, we introduce two {ISVQA} datasets – indoor and outdoor scenes. They simulate the real-world scenarios of indoor image collections and multiple car-mounted cameras, respectively. The indoor-scene dataset contains 91,479 human-annotated questions for 48,138 image sets, and the outdoor-scene dataset has 49,617 questions for 12,746 image sets. We analyze the properties of the two datasets, including question-and-answer distributions, types of questions, biases in dataset, and question-image dependencies. We also build new baseline models to investigate new research challenges in {ISVQA}.},
	pages = {51--67},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Bansal, Ankan and Zhang, Yuting and Chellappa, Rama},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2008.11976},
	note = {{ISBN}: 9783030585884
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{barabas_current_2017,
	title = {Current challenges in autonomous driving},
	volume = {252},
	issn = {1757899X},
	doi = {10/gjjgww},
	abstract = {Nowadays the automotive industry makes a quantum shift to a future, where the driver will have smaller and smaller role in driving his or her vehicle ending up being totally excluded. In this paper, we have investigated the different levels of driving automatization, the prospective effects of these new technologies on the environment and traffic safety, the importance of regulations and their current state, the moral aspects of introducing these technologies and the possible scenarios of deploying the autonomous vehicles. We have found that the self-driving technologies are facing many challenges: a) They must make decisions faster in very diverse conditions which can include many moral dilemmas as well; b) They have an important potential in reducing the environmental pollution by optimizing their routes, driving styles by communicating with other vehicles, infrastructures and their environment; c) There is a considerable gap between the self-drive technology level and the current regulations; fortunately, this gap shows a continuously decreasing trend; d) In case of many types of imminent accidents management there are many concerns about the ability of making the right decision. Considering that this field has an extraordinary speed of development, our study is up to date at the submission deadline. Self-driving technologies become increasingly sophisticated and technically accessible, and in some cases, they can be deployed for commercial vehicles as well. According to the current stage of research and development, it is still unclear how the self-driving technologies will be able to handle extreme and unexpected events including their moral aspects. Since most of the traffic accidents are caused by human error or omission, it is expected that the emergence of the autonomous technologies will reduce these accidents in their number and gravity, but the very few currently available test results have not been able to scientifically underpin this issue yet. The increasing trend in automation of vehicles will radically change the composition of car industry players, as mechatronics will not only be a complementary part of the automobile industry but an indispensable part of it. There is a reasonable expectation that automated cars will perform the same or better in all respects than their conventional counterparts. However, it seems that the current regulations do not keep up with the development of technology and sometimes hinder the development and testing of autonomous technologies.},
	number = {1},
	journaltitle = {{IOP} Conference Series: Materials Science and Engineering},
	author = {Barabás, I. and Todoruţ, A. and Cordoş, N. and Molea, A.},
	date = {2017},
	note = {12 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{baek_vehicle_2020,
	title = {Vehicle trajectory prediction and collision warning via fusion of multisensors and wireless vehicular communications},
	volume = {20},
	issn = {14248220},
	doi = {10/gjjgwt},
	abstract = {Driver inattention is one of the leading causes of traffic crashes worldwide. Providing the driver with an early warning prior to a potential collision can significantly reduce the fatalities and level of injuries associated with vehicle collisions. In order to monitor the vehicle surroundings and predict collisions, on-board sensors such as radar, lidar, and cameras are often used. However, the driving environment perception based on these sensors can be adversely affected by a number of factors such as weather and solar irradiance. In addition, potential dangers cannot be detected if the target is located outside the limited field-of-view of the sensors, or if the line of sight to the target is occluded. In this paper, we propose an approach for designing a vehicle collision warning system based on fusion of multisensors and wireless vehicular communications. A high-level fusion of radar, lidar, camera, and wireless vehicular communication data was performed to predict the trajectories of remote targets and generate an appropriate warning to the driver prior to a possible collision. We implemented and evaluated the proposed vehicle collision system in virtual driving environments, which consisted of a vehicle–vehicle collision scenario and a vehicle–pedestrian collision scenario.},
	number = {1},
	journaltitle = {Sensors (Switzerland)},
	author = {Baek, Minjin and Jeong, Donggi and Choi, Dongho and Lee, Sangsun},
	date = {2020},
	pmid = {31947961},
	note = {10 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Advanced driver assistance system, Collision warning, Connected vehicles, Risk assessment, Trajectory prediction, Vehicular communications, Vulnerable road users},
}

@article{beglerovic_deep_2018,
	title = {Deep learning applied to scenario classification for lane-keep-assist systems},
	volume = {8},
	issn = {20763417},
	doi = {10/gjjgwz},
	abstract = {Test, verification, and development activities of vehicles with {ADAS} (Advanced Driver Assistance Systems) and {ADF} (Automated Driving Functions) generate large amounts of measurement data. To efficiently evaluate and use this data, a generic understanding and classification of the relevant driving scenarios is necessary. Currently, such understanding is obtained by using heuristic algorithms or even by manual inspection of sensor signals. In this paper, we apply deep learning on sensor time series data to automatically extract relevant features for classification of driving scenarios relevant for a Lane-Keep-Assist System. We compare the performance of convolutional and recurrent neural networks and propose two classification models. The first one is an online model for scenario classification during driving. The second one is an offline model for post-processing, providing higher accuracy.},
	number = {12},
	journaltitle = {Applied Sciences (Switzerland)},
	author = {Beglerovic, Halil and Schloemicher, Thomas and Metzner, Steffen and Horn, Martin},
	date = {2018},
	note = {8 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Advanced driver assistance systems ({ADAS}), Automated driving functions, Automated driving functions ({ADF}), Deep learning, Scenario classification},
}

@article{aradi_survey_2020,
	title = {Survey of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles},
	issn = {23318422},
	doi = {10/gjjgws},
	abstract = {Academic research in the field of autonomous vehicles has reached high popularity in recent years related to several topics as sensor technologies, V2X communications, safety, security, decision making, control, and even legal and standardization rules. Besides classic control design approaches, Artificial Intelligence and Machine Learning methods are present in almost all of these fields. Another part of research focuses on different layers of Motion Planning, such as strategic decisions, trajectory planning, and control. A wide range of techniques in Machine Learning itself have been developed, and this article describes one of these fields, Deep Reinforcement Learning ({DRL}). The paper provides insight into the hierarchical motion planning problem and describes the basics of {DRL}. The main elements of designing such a system are the modeling of the environment, the modeling abstractions, the description of the state and the perception models, the appropriate rewarding, and the realization of the underlying neural network. The paper describes vehicle models, simulation possibilities and computational requirements. Strategic decisions on different layers and the observation models, e.g., continuous and discrete state representations, grid-based, and camera-based solutions are presented. The paper surveys the state-of-art solutions systematized by the different tasks and levels of autonomous driving, such as car-following, lane-keeping, trajectory following, merging, or driving in dense traffic. Finally, open questions and future challenges are discussed.},
	pages = {1--14},
	journaltitle = {{arXiv}},
	author = {Aradi, Szilárd},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2001.11231},
	note = {12 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Artificial intelligence, Autonomous Vehicles, Index Terms—Machine Learning, Motion Planning, Reinforcement Learning},
}

@article{althoff_online_2014,
	title = {Online verification of automated road vehicles using reachability analysis},
	volume = {30},
	issn = {15523098},
	doi = {10/f6c8bv},
	abstract = {An approach for formally verifying the safety of automated vehicles is proposed. Due to the uniqueness of each traffic situation, we verify safety online, i.e., during the operation of the vehicle. The verification is performed by predicting the set of all possible occupancies of the automated vehicle and other traffic participants on the road. In order to capture all possible future scenarios, we apply reachability analysis to consider all possible behaviors of mathematical models considering uncertain inputs (e.g., sensor noise, disturbances) and partially unknown initial states. Safety is guaranteed with respect to the modeled uncertainties and behaviors if the occupancy of the automated vehicle does not intersect that of other traffic participants for all times. The applicability of the approach is demonstrated by test drives with an automated vehicle at the Robotics Institute at Carnegie Mellon University. © 2014 {IEEE}.},
	pages = {903--918},
	number = {4},
	journaltitle = {{IEEE} Transactions on Robotics},
	author = {Althoff, Matthias and Dolan, John M.},
	date = {2014},
	note = {211 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Automated vehicles, autonomous cars, formal verification, reachability analysis, set-based computation},
}

@article{arruda_cross-domain_2019,
	title = {Cross-Domain Car Detection Using Unsupervised Image-to-Image Translation: From Day to Night},
	volume = {2019-July},
	doi = {10/gg7th9},
	abstract = {Deep learning techniques have enabled the emergence of state-of-the-art models to address object detection tasks. However, these techniques are data-driven, delegating the accuracy to the training dataset which must resemble the images in the target task. The acquisition of a dataset involves annotating images, an arduous and expensive process, generally requiring time and manual effort. Thus, a challenging scenario arises when the target domain of application has no annotated dataset available, making tasks in such situation to lean on a training dataset of a different domain. Sharing this issue, object detection is a vital task for autonomous vehicles where the large amount of driving scenarios yields several domains of application requiring annotated data for the training process. In this work, a method for training a car detection system with annotated data from a source domain (day images) without requiring the image annotations of the target domain (night images) is presented. For that, a model based on Generative Adversarial Networks ({GANs}) is explored to enable the generation of an artificial dataset with its respective annotations. The artificial dataset (fake dataset) is created translating images from day-time domain to night-time domain. The fake dataset, which comprises annotated images of only the target domain (night images), is then used to train the car detector model. Experimental results showed that the proposed method achieved significant and consistent improvements, including the increasing by more than 10\% of the detection performance when compared to the training with only the available annotated data (i.e., day images).},
	journaltitle = {Proceedings of the International Joint Conference on Neural Networks},
	author = {Arruda, Vinicius F. and Paixao, Thiago M. and Berriel, Rodrigo F. and De Souza, Alberto F. and Badue, Claudine and Sebe, Nicu and Oliveira-Santos, Thiago},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1907.08719},
	note = {{ISBN}: 9781728119854
15 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Generative Adversarial Networks, Object Detection, Unpaired Image-to-Image Translation, Unsupervised Domain Adaptation},
}

@article{althoff_road_2013,
	title = {Road occupancy prediction of traffic participants},
	doi = {10/ggbtt7},
	abstract = {We predict the road occupancy of traffic participants for collision avoidance systems. The occupancy sets are computed for consecutive time intervals and contain all reachable positions of traffic participants in compliance with a proposed dynamic model. Those sets make it possible to check if planned emergency maneuvers are collision-free in all possible future scenarios. However, no algorithm exists for exactly computing the occupancy when the model forbids unrealistic behavior such as leaving road boundaries or largely exceeding speed limits. For this reason, we provide methods to tightly overapproximate occupancy sets to ensure safe emergency maneuvers. We demonstrate the applicability of the approach by numerical examples, which show that the occupancy computation is not only efficient, but also tight enough to trigger emergency maneuvers almost at the last possible point in time. © 2013 {IEEE}.},
	pages = {99--105},
	journaltitle = {{IEEE} Conference on Intelligent Transportation Systems, Proceedings, {ITSC}},
	author = {Althoff, Matthias and Hess, Daniel and Gambert, Florian},
	date = {2013},
	note = {{ISBN}: 9781479929146
23 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{amini_learning_2020,
	title = {Learning Robust Control Policies for End-to-End Autonomous Driving from Data-Driven Simulation},
	volume = {5},
	issn = {23773766},
	doi = {10/gjjgwr},
	abstract = {In this work, we present a data-driven simulation and training engine capable of learning end-to-end autonomous vehicle control policies using only sparse rewards. By leveraging real, human-collected trajectories through an environment, we render novel training data that allows virtual agents to drive along a continuum of new local trajectories consistent with the road appearance and semantics, each with a different view of the scene. We demonstrate the ability of policies learned within our simulator to generalize to and navigate in previously unseen real-world roads, without access to any human control labels during training. Our results validate the learned policy onboard a full-scale autonomous vehicle, including in previously un-encountered scenarios, such as new roads and novel, complex, near-crash situations. Our methods are scalable, leverage reinforcement learning, and apply broadly to situations requiring effective perception and robust operation in the physical world.},
	pages = {1143--1150},
	number = {2},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	author = {Amini, Alexander and Gilitschenski, Igor and Phillips, Jacob and Moseyko, Julia and Banerjee, Rohan and Karaman, Sertac and Rus, Daniela},
	date = {2020},
	note = {19 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Deep learning in robotics and automation, autonomous agents, data-driven simulation, real world reinforcement learning, ★},
}
@article{anoosheh_night--day_2019,
	title = {Night-to-day image translation for retrieval-based localization},
	volume = {2019-May},
	issn = {10504729},
	doi = {10/gh84dc},
	abstract = {Visual localization is a key step in many robotics pipelines, allowing the robot to (approximately) determine its position and orientation in the world. An efficient and scalable approach to visual localization is to use image retrieval techniques. These approaches identify the image most similar to a query photo in a database of geo-tagged images and approximate the query's pose via the pose of the retrieved database image. However, image retrieval across drastically different illumination conditions, e.g. day and night, is still a problem with unsatisfactory results, even in this age of powerful neural models. This is due to a lack of a suitably diverse dataset with true correspondences to perform end-to-end learning. A recent class of neural models allows for realistic translation of images among visual domains with relatively little training data and, most importantly, without ground-truth pairings.In this paper, we explore the task of accurately localizing images captured from two traversals of the same area in both day and night. We propose {ToDayGAN} - a modified image-translation model to alter nighttime driving images to a more useful daytime representation. We then compare the daytime and translated night images to obtain a pose estimate for the night image using the known 6-{DOF} position of the closest day image. Our approach improves localization performance by over 250\% compared the current state-of-the-art, in the context of standard metrics in multiple categories.},
	pages = {5958--5964},
	journaltitle = {Proceedings - {IEEE} International Conference on Robotics and Automation},
	author = {Anoosheh, Asha and Sattler, Torsten and Timofte, Radu and Pollefeys, Marc and Gool, Luc Van},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1809.09767},
	note = {{ISBN}: 9781538660263
66 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{bendale_towards_2016,
	title = {Towards open set deep networks},
	volume = {2016-Decem},
	issn = {10636919},
	doi = {10/gfx5d2},
	abstract = {Deep networks have produced significant gains for various visual recognition problems, leading to high impact academic and commercial applications. Recent work in deep networks highlighted that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images high confidence as that given class - deep network are easily fooled with images humans do not consider meaningful. The closed set nature of deep networks forces them to choose from one of the known classes leading to such artifacts. Recognition in the real world is open set, i.e. the recognition system should reject unknown/unseen classes at test time. We present a methodology to adapt deep networks for open set recognition, by introducing a new model layer, {OpenMax}, which estimates the probability of an input being from an unknown class. A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. Open-Max allows rejection of 'fooling' and unrelated open set images presented to the system, {OpenMax} greatly reduces the number of obvious errors made by a deep network. We prove that the {OpenMax} concept provides bounded open space risk, thereby formally providing an open set recognition solution. We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on {ImageNet} 2012 validation data, and thousands of fooling and open set images. The proposed {OpenMax} model significantly outperforms open set recognition accuracy of basic deep networks as well as deep networks with thresholding of {SoftMax} probabilities.},
	pages = {1563--1572},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Bendale, Abhijit and Boult, Terrance E.},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1511.06233},
	note = {{ISBN}: 9781467388504
379 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{blum_fishyscapes_2019,
	title = {Fishyscapes: A benchmark for safe semantic segmentation in autonomous driving},
	doi = {10/gjjgw2},
	abstract = {Deep learning has enabled impressive progress in the accuracy of semantic segmentation. Yet, the ability to estimate uncertainty and detect anomalies is key for safety-critical applications like autonomous driving. Existing uncertainty estimates have mostly been evaluated on simple tasks, and it is unclear whether these methods generalize to more complex scenarios. We present Fishyscapes, the first public benchmark for uncertainty estimation in the real-world task of semantic segmentation for urban driving. It evaluates pixel-wise uncertainty estimates towards the detection of anomalous objects in front of the vehicle. We adapt state-of-the-art methods to recent semantic segmentation models and compare approaches based on softmax confidence, Bayesian learning, and embedding density. Our results show that anomaly detection is far from solved even for ordinary situations, while our benchmark allows measuring advancements beyond the state-of-the-art.},
	pages = {2403--2412},
	journaltitle = {Proceedings - 2019 International Conference on Computer Vision Workshop, {ICCVW} 2019},
	author = {Blum, Hermann and Sarlin, Paul Edouard and Nieto, Juan and Siegwart, Roland and Cadena, Cesar},
	date = {2019},
	note = {{ISBN}: 9781728150239
11 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Anomaly detection, Deep learning, Deep learning for robotics, Uncertainty estimation},
}

@article{berend_cats_2020,
	title = {Cats Are Not Fish: Deep Learning Testing Calls for Out-Of-Distribution Awareness},
	doi = {10/gjjgwx},
	abstract = {As Deep Learning ({DL}) is continuously adopted in many industrial applications, its quality and reliability start to raise concerns. Similar to the traditional software development process, testing the {DL} software to uncover its defects at an early stage is an effective way to reduce risks after deployment. According to the fundamental assumption of deep learning, the {DL} software does not provide statistical guarantee and has limited capability in handling data that falls outside of its learned distribution, i.e., out-of-distribution ({OOD}) data. Although recent progress has been made in designing novel testing techniques for {DL} software, which can detect thousands of errors, the current state-of-the-art {DL} testing techniques usually do not take the distribution of generated test data into consideration. It is therefore hard to judge whether the 'identified errors' are indeed meaningful errors to the {DL} application (i.e., due to quality issues of the model) or outliers that cannot be handled by the current model (i.e., due to the lack of training data). Tofill this gap, we take thefi rst step and conduct a large scale empirical study, with a total of 451 experiment configurations, 42 deep neural networks ({DNNs}) and 1.2 million test data instances, to investigate and characterize the impact of {OOD}-awareness on {DL} testing. We further analyze the consequences when {DL} systems go into production by evaluating the effectiveness of adversarial retraining with distribution-aware errors. The results confirm that introducing data distribution awareness in both testing and enhancement phases outperforms distribution unaware retraining by up to 21.5\%.},
	pages = {1041--1052},
	journaltitle = {Proceedings - 2020 35th {IEEE}/{ACM} International Conference on Automated Software Engineering, {ASE} 2020},
	author = {Berend, David and Xie, Xiaofei and Ma, Lei and Zhou, Lingjun and Liu, Yang and Xu, Chi and Zhao, Jianjun},
	date = {2020},
	note = {{ISBN}: 9781450367684
6 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Deep learning testing, out of distribution, quality assurance},
}

@article{bertinetto_making_2020,
	title = {Making better mistakes: Leveraging class hierarchies with deep networks},
	issn = {10636919},
	doi = {10/ghbcsz},
	abstract = {Deep neural networks have improved image classification dramatically over the past decade, but have done so by focusing on performance measures that treat all classes other than the ground truth as equally wrong. This has led to a situation in which mistakes are less likely to be made than before, but are equally likely to be absurd or catastrophic when they do occur. Past works have recognised and tried to address this issue of mistake severity, often by using graph distances in class hierarchies, but this has largely been neglected since the advent of the current deep learning era in computer vision. In this paper, we aim to renew interest in this problem by reviewing past approaches and proposing two simple modifications of the cross-entropy loss which outperform the prior art under several metrics on two large datasets with complex class hierarchies: {tieredImageNet} and {iNaturalist}'19.},
	pages = {12503--12512},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Bertinetto, Luca and Mueller, Romain and Tertikas, Konstantinos and Samangooei, Sina and Lord, Nicholas A.},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1912.09393},
	note = {8 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{brown_exploration_2019,
	title = {An Exploration of Virtual Reality Use and Application Among Older Adult Populations},
	volume = {5},
	issn = {2333-7214},
	doi = {10/gjjg24},
	abstract = {Despite increased research on virtual reality ({VR}) platforms in recent years, there are very few studies that specifically examine its application within a gerontological context. This study examines the usability, preferences, and application considerations of a mobile {VR} platform by individually interviewing community-dwelling older adults both before and after trying the Samsung Gear {VR}. Participants were asked to self-select and view short {VR} videos (30 s-3 min) that were filmed within the local community (e.g., walking path and art museum). Semi-structured interview questions explored participant perceptions of using the device and was followed with two focus group sessions. Thematic analysis was employed when reviewing observational notes and transcribed audio recordings. Ten adults (aged 63-89) participated and themes identified include (a) usability, (b) video subject matter preferences, and (c) application. These themes highlighted both the challenges and opportunities of {VR} use among a wide range of older populations and provided greater insight with its exploration and application in future studies. This included potential use among those older adults who have notable functional limitations, such as those who are immobile, or reside within a care facility.},
	pages = {233372141988528},
	journaltitle = {Gerontology and Geriatric Medicine},
	author = {Brown, Julie A.},
	date = {2019},
	pmid = {31723574},
	note = {2 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {2019, accepted, august 30, final revision received, human factors, manuscript received, mobile technologies, october 3, older adults, september 26, usability, vr},
}

@article{bolte_towards_2019,
	title = {Towards corner case detection for autonomous driving},
	volume = {2019-June},
	issn = {23318422},
	doi = {10/ggs7vv},
	abstract = {The progress in autonomous driving is also due to the increased availability of vast amounts of training data for the underlying machine learning approaches. Machine learning systems are generally known to lack robustness, e.g., if the training data did rarely or not at all cover critical situations. The challenging task of corner case detection in video, which is also somehow related to unusual event or anomaly detection, aims at detecting these unusual situations, which could become critical, and to communicate this to the autonomous driving system (online use case). Such a system, however, could be also used in offline mode to screen vast amounts of data and select only the relevant situations for storing and (re)training machine learning algorithms. So far, the approaches for corner case detection have been limited to videos recorded from a fixed camera, mostly for security surveillance. In this paper, we provide a formal definition of a corner case and propose a system framework for both the online and the offline use case that can handle video signals from front cameras of a naturally moving vehicle and can output a corner case score.},
	pages = {438--445},
	issue = {Iv},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Bolte, Jan Aike and Bär, Andreas and Lipinski, Daniel and Fingscheidt, Tim and Bar, Andreas and Lipinski, Daniel and Fingscheidt, Tim},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1902.09184},
	note = {{ISBN}: 9781728105604
18 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {★},
}

@article{betella_understanding_2014,
	title = {Understanding large network datasets through embodied interaction in virtual reality},
	volume = {2014-April},
	doi = {10/gjjrhj},
	abstract = {The intricate web of information we generate nowadays is more massive than ever in the history of mankind. The sheer enormity of big data makes the task of extracting semantic associations out of complex networks more complicated. Stemming this "data deluge" calls for novel unprecedented technologies. In this work, we engineered a system that enhances a user's understanding of large datasets through embodied navigation and natural gestures. This system constitutes an immersive virtual reality environment called the "{eXperience} Induction Machine" ({XIM}). One of the applications that we tested using our system is the exploration of the human connectome: the network of nodes and connections that underlie the anatomical architecture of the human brain. As a comparative validation of our technology, we then exposed participants to a connectome dataset using both our system and a state-of-the-art software for visualization and analysis of the same network. We systematically measured participants' understanding and visual memory of the connectomic structure. Our results showed that participants retained more information about the structure of the network when using our system. Overall, our system constitutes a novel approach in the exploration and understanding of large complex networks.},
	journaltitle = {{ACM} International Conference Proceeding Series},
	author = {Betella, Alberto and Bueno, Enrique Martínez and Kongsantad, Wipawee and Zucca, Riccardo and Arsiwalla, Xerxes D. and Omedas, Pedro and Verschure, Paul F.M.J.},
	date = {2014},
	note = {{ISBN}: 9781450326261
21 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Connectome, Graphs, Immersion, Navigation, exploration, Network, Virtual reality},
}

@article{botte_overview_2019,
	title = {An overview of cooperative driving in the european union: Policies and practices},
	volume = {8},
	issn = {20799292},
	doi = {10/gf3fhs},
	abstract = {Cooperative-Intelligent Transportation Systems (C-{ITSs}) aim to connect vehicles, both with one another and with road infrastructures, so as to increase traffic safety and efficiency. This paper focuses on the European framework for supporting the development of Cooperative, Connected, and Automated Mobility, and aims to shed light on the current state of testing and deployment activities in the field at the start of 2019. This may be considered particularly timely given that the year 2019 was identified as the starting date for the deployment of mature services, and the Community legislation is currently paying great attention to the matter. In order to present a concise (but comprehensive) picture, we consulted and analysed the most diverse sources comprising more than 2000 pages.},
	pages = {1--25},
	number = {6},
	journaltitle = {Electronics (Switzerland)},
	author = {Botte, Marilisa and Pariota, Luigi and D’Acierno, Luca and Bifulco, Gennaro Nicola},
	date = {2019},
	note = {7 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {C-{ITS} services, Cooperative driving, European framework, Open-road pilot sites, Smart roads, Vehicle to everything (V2X) testbeds},
}

@article{cai_real-time_2020,
	title = {Real-time out-of-distribution detection in learning-enabled cyber-physical systems},
	doi = {10/gjjgw4},
	abstract = {Cyber-physical systems ({CPS}) greatly benefit by using machine learning components that can handle the uncertainty and variability of the real-world. Typical components such as deep neural networks, however, introduce new types of hazards that may impact system safety. The system behavior depends on data that are available only during runtime and may be different than the data used for training. Out-of-distribution data may lead to a large error and compromise safety. The paper considers the problem of efficiently detecting out-of-distribution data in {CPS} control systems. Detection must be robust and limit the number of false alarms while being computational efficient for real-time monitoring. The proposed approach leverages inductive conformal prediction and anomaly detection for developing a method that has a well-calibrated false alarm rate. We use variational autoencoders and deep support vector data description to learn models that can be used efficiently compute the nonconformity of new inputs relative to the training set and enable realtime detection of out-of-distribution high-dimensional inputs. We demonstrate the method using an advanced emergency braking system and a self-driving end-to-end controller implemented in an open source simulator for self-driving cars. The simulation results show very small number of false positives and detection delay while the execution time is comparable to the execution time of the original machine learning components.},
	pages = {174--183},
	journaltitle = {Proceedings - 2020 {ACM}/{IEEE} 11th International Conference on Cyber-Physical Systems, {ICCPS} 2020},
	author = {Cai, Feiyang and Koutsoukos, Xenofon},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2001.10494},
	note = {{ISBN}: 9781728155012
7 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Anomaly detection, Inductive conformal prediction, Out-of-distribution, Self-driving vehicles.},
}

@article{chan_anticipating_2017,
	title = {Anticipating accidents in dashcam videos},
	volume = {10114 {LNCS}},
	issn = {16113349},
	doi = {10/gjjgw6},
	abstract = {We propose a Dynamic-Spatial-Attention ({DSA}) Recurrent Neural Network ({RNN}) for anticipating accidents in dashcam videos (Fig. 1). Our {DSA}-{RNN} learns to (1) distribute soft-attention to candidate objects dynamically to gather subtle cues and (2) model the temporal dependencies of all cues to robustly anticipate an accident. Anticipating accidents is much less addressed than anticipating events such as changing a lane, making a turn, etc., since accidents are rare to be observed and can happen in many different ways mostly in a sudden. To overcome these challenges, we (1) utilize state-of-the-art object detector [3] to detect candidate objects, and (2) incorporate full-frame and object-based appearance and motion features in our model. We also harvest a diverse dataset of 678 dashcam accident videos on the web (Fig. 3). The dataset is unique, since various accidents (e.g., a motorbike hits a car, a car hits another car, etc.) occur in all videos. We manually mark the time-location of accidents and use them as supervision to train and evaluate our method. We show that our method anticipates accidents about 2 s before they occur with 80\% recall and 56.14\% precision. Most importantly, it achieves the highest mean average precision (74.35\%) outperforming other baselines without attention or {RNN}.},
	pages = {136--153},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Chan, Fu Hsiang and Chen, Yu Ting and Xiang, Yu and Sun, Min},
	date = {2017},
	note = {{ISBN}: 9783319541891
73 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{cao_5g_2016,
	title = {A 5G V2X testbed for cooperative automated driving},
	volume = {0},
	issn = {21579865},
	doi = {10/gjjgw5},
	abstract = {Ensuring low-latency and highly reliable communication between vehicles is one of the goals of 5G. We present a 5G Vehicle-to-everything (V2X) wireless testbed based on flexible and re-configurable software defined radio that is designed for cooperative automated driving. The use-cases and communication requirements for cooperative automated driving are discussed to motivate the system design and technical enablers that can achieve the most stringent link-level communication requirements of cooperative autonomous driving. The key building blocks include a re-configurable {RF} front-end, optimized base-band processing on standard Intel {CPUs} and a custom-built high-power external {RF} subsystem. The technical enablers include a new {OFDM}-like waveform based on Pulse-shaping, a flexible and self-contained frame-structure design, {GNSS}-aided hybrid synchronization and low-latency scheduled multiple-access. We finally present some experimental results from lab measurements.},
	journaltitle = {{IEEE} Vehicular Networking Conference, {VNC}},
	author = {Cao, Hanwen and Gangakhedkar, Sandip and Ali, Ali Ramadan and Gharba, Mohamed and Eichinger, Josef},
	date = {2016},
	note = {{ISBN}: 9781509051977
24 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{candido_exploiting_2010,
	title = {Exploiting domain knowledge in planning for uncertain robot systems modeled as {POMDPs}},
	issn = {10504729},
	doi = {10/fc78zc},
	abstract = {We propose a planning algorithm that allows user-supplied domain knowledge to be exploited in the synthesis of information feedback policies for systems modeled as partially observable Markov decision processes ({POMDPs}). {POMDP} models, which are increasingly popular in the robotics literature, permit a planner to consider future uncertainty in both the application of actions and sensing of observations. With our approach, domain experts can inject specialized knowledge into the planning process by providing a set of local policies that are used as primitives by the planner. If the local policies are chosen appropriately, the planner can evaluate further into the future, even for large problems, which can lead to better overall policies at decreased computational cost. We use a structured approach to encode the provided domain knowledge into the value function approximation. We demonstrate our approach on a multi-robot fire fighting problem, in which a team of robots cooperates to extinguish a spreading fire, modeled as a stochastic process. The state space for this problem is significantly larger than is typical in the {POMDP} literature, and the geometry of the problem allows for the application of an intuitive set of local policies, thus demonstrating the effectiveness of our approach. ©2010 {IEEE}.},
	pages = {3596--3603},
	journaltitle = {Proceedings - {IEEE} International Conference on Robotics and Automation},
	author = {Candido, Salvatore and Davidson, James and Hutchinson, Seth},
	date = {2010},
	note = {{ISBN}: 9781424450381
14 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{calvert_gaps_2020,
	title = {Gaps in the Control of Automated Vehicles on Roads},
	issn = {19411197},
	doi = {10/ggqmmv},
	journaltitle = {{IEEE} Intelligent Transportation Systems Magazine},
	author = {Calvert, Simeon C. and Mecacci, Giulio and van Arem, Bart and Santoni de Sio, Filippo and Heikoop, Daniel D. and Hagenzieker, Marjan},
	date = {2020},
	note = {8 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {autonomous driving, meaningful, vehicle automation, vehicle control},
}

@article{choe_ranus_2018,
	title = {{RANUS}: {RGB} and {NIR} urban scene dataset for deep scene parsing},
	volume = {3},
	issn = {23773766},
	doi = {10/gjjgxc},
	abstract = {In this letter, we present a data-driven method for scene parsing of road scenes to utilize single-channel near-infrared ({NIR}) images. To overcome the lack of data problem in non-{RGB} spectrum, we define a new color space and decompose the task of deep scene parsing into two subtasks with two separate {CNN} architectures for chromaticity channels and semantic masks. For chromaticity estimation, we build a spatially-aligned {RGB}-{NIR} image database (40k urban scenes) to infer color information from {RGB}-{NIR} spectrum learning process and leverage existing scene parsing networks trained over already available {RGB} masks. From our database, we sample key frames and manually annotate them (4k ground truth masks) to finetune the network into the proposed color space. Hence, the key contribution of this work is to replace multispectral scene parsing methods with a simple yet effective approach using single {NIR} images. The benefits of using our algorithm and dataset are confirmed in the qualitative and quantitative experiments.},
	pages = {1808--1815},
	number = {3},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	author = {Choe, Gyeongmin and Kim, Seong Heum and Im, Sunghoon and Lee, Joon Young and Narasimhan, Srinivasa G. and Kweon, In So},
	date = {2018},
	note = {13 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Deep learning in robotics and automation, semantic scene understanding},
}

@article{chen_semantically_2021,
	title = {Semantically Meaningful Class Prototype Learning for One-Shot Image Semantic Segmentation},
	issn = {19410077},
	url = {http://arxiv.org/abs/2102.10935},
	doi = {10/gjjgw9},
	abstract = {One-shot semantic image segmentation aims to segment the object regions for the novel class with only one annotated image. Recent works adopt the episodic training strategy to mimic the expected situation at testing time. However, these existing approaches simulate the test conditions too strictly during the training process, and thus cannot make full use of the given label information. Besides, these approaches mainly focus on the foreground-background target class segmentation setting. They only utilize binary mask labels for training. In this paper, we propose to leverage the multi-class label information during the episodic training. It will encourage the network to generate more semantically meaningful features for each category. After integrating the target class cues into the query features, we then propose a pyramid feature fusion module to mine the fused features for the final classifier. Furthermore, to take more advantage of the support image-mask pair, we propose a self-prototype guidance branch to support image segmentation. It can constrain the network for generating more compact features and a robust prototype for each semantic class. For inference, we propose a fused prototype guidance branch for the segmentation of the query image. Specifically, we leverage the prediction of the query image to extract the pseudo-prototype and combine it with the initial prototype. Then we utilize the fused prototype to guide the final segmentation of the query image. Extensive experiments demonstrate the superiority of our proposed approach.},
	pages = {1--12},
	author = {Chen, Tao and Xie, Guosen and Yao, Yazhou and Wang, Qiong and Shen, Fumin and Tang, Zhenmin and Zhang, Jian},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.10935},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
}

@article{chen_autonomous_2019,
	title = {Autonomous driving: cognitive construction and situation understanding},
	volume = {62},
	issn = {18691919},
	doi = {10/gjjgxb},
	abstract = {Autonomous vehicle is a kind of typical complex artificial intelligence system. In current research of autonomous driving, the most widely adopted technique is to use a basic framework of serial information processing and computations, which consists of four modules: perception, planning, decision-making, and control. However, this framework based on data-driven computing performs low computational efficiency, poor environmental understanding and self-learning ability. A neglected problem has long been how to understand and process environmental perception data from the sensors referring to the cognitive psychology level of the human driving process. The key to solving this problem is to construct a computing model with selective attention and self-learning ability for autonomous driving, which is supposed to possess the mechanism of memorizing, inferring and experiential updating, enabling it to cope with traffic scenarios with high noise, dynamic, and randomness. In addition, for the process of understanding traffic scenes, the efficiency of event-related mechanism is more significant than single-attribute scenario perception data. Therefore, an effective self-driving method should not be confined to the traditional computing framework of ‘perception, planning, decision-making, and control’. It is necessary to explore a basic computing framework that conforms to human driver’s attention, reasoning, learning, and decision-making mechanism with regard to traffic scenarios and build an autonomous system inspired by biological intelligence. In this article, we review the basic methods and main progress in current data-driven autonomous driving technologies, deeply analyze the limitations and major problems faced by related algorithms. Then, combined with authors’ research, this study discusses how to implement a basic cognitive computing framework of self-driving with selective attention and an event-driven mechanism from the basic viewpoint of cognitive science. It further describes how to use multi-sensor and graph data with semantic information (such as traffic maps and a spatial correlation of events) to realize the associative representations of objects and drivable areas, as well as the intuitive reasoning method applied to understanding the situations in different traffic scenarios. The computing framework of autonomous driving based on a selective attention mechanism and intuitive reasoning discussed in this study can adapt to a more complex, open, and dynamic traffic environment.},
	pages = {1--27},
	number = {8},
	journaltitle = {Science China Information Sciences},
	author = {Chen, Shitao and Jian, Zhiqiang and Huang, Yuhao and Chen, Yu and Zhou, Zhuoli and Zheng, Nanning},
	date = {2019},
	note = {13 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {autonomous driving, cognitive construction, event-driven mechanism, intuitive reasoning, situation understanding},
}

@article{chen_hierarchical_2021,
	title = {Hierarchical Graph Neural Networks for Few-Shot Learning},
	volume = {8215},
	issn = {1051-8215},
	doi = {10/gjjgw8},
	pages = {1--1},
	issue = {c},
	journaltitle = {{IEEE} Transactions on Circuits and Systems for Video Technology},
	author = {Chen, Cen and Li, Kenli and Wei, Wei and Zhou, Joey Tianyi and Zeng, Zeng},
	date = {2021},
}

@article{chekired_5g-slicing-enabled_2019,
	title = {5G-Slicing-Enabled Scalable {SDN} Core Network: Toward an Ultra-Low Latency of Autonomous Driving Service},
	volume = {37},
	issn = {15580008},
	doi = {10/gjjgw7},
	abstract = {5G networks are anticipated to support a plethora of innovative and promising network services. These services have heterogeneous performance requirements (e.g., high-rate traffic, low latency, and high reliability). To meet them, 5G networks are entailed to endorse flexibility that can be fulfilled through the deployment of new emerging technologies, mainly software-defined networking ({SDN}), network functions virtualization ({NFV}), and network slicing. In this paper, we focus on an interesting automotive vertical use case: autonomous vehicles. Our aim is to enhance the quality of service of autonomous driving application. To this end, we design a framework that uses the aforementioned technologies to enhance the quality of service of the autonomous driving application. The framework is made of 1) a distributed and scalable {SDN} core network architecture that deploys fog, edge and cloud computing technologies; 2) a network slicing function that maps autonomous driving functionalities into service slices; and 3) a network and service slicing system model that promotes a four-layer logical architecture to improve the transmission efficiency and satisfy the low latency constraint. In addition, we present a theoretical analysis of the propagation delay and the handling latency based on {GI}/M/1 queuing system. Simulation results show that our framework meets the low-latency requirement of the autonomous driving application as it incurs low propagation delay and handling latency for autonomous driving traffic compared to best-effort traffic.},
	pages = {1769--1782},
	number = {8},
	journaltitle = {{IEEE} Journal on Selected Areas in Communications},
	author = {Chekired, Djabir Abdeldjalil and Togou, Mohammed Amine and Khoukhi, Lyes and Ksentini, Adlen},
	date = {2019},
	note = {33 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {5G networks, autonomous driving, network functions virtualization, network slicing, software-defined networking},
}

@article{chong_abnormal_2017,
	title = {Abnormal event detection in videos using spatiotemporal autoencoder},
	volume = {10262 {LNCS}},
	issn = {16113349},
	doi = {10/gjjgxf},
	abstract = {We present an efficient method for detecting anomalies in videos. Recent applications of convolutional neural networks have shown promises of convolutional layers for object detection and recognition, especially in images. However, convolutional neural networks are supervised and require labels as learning signals. We propose a spatiotemporal architecture for anomaly detection in videos including crowded scenes. Our architecture includes two main components, one for spatial feature representation, and one for learning the temporal evolution of the spatial features. Experimental results on Avenue, Subway and {UCSD} benchmarks confirm that the detection accuracy of our method is comparable to state-of-the-art methods at a considerable speed of up to 140 fps.},
	pages = {189--196},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Chong, Yong Shean and Tay, Yong Haur},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1701.01546},
	note = {{ISBN}: 9783319590806
206 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Anomaly detection, Autoencoder, Feature learning, Regularity, ★},
}

@article{coffey_slice_2011,
	title = {Slice {WIM}: A Multi-surface, multi-touch interface for overview+detail exploration of volume datasets in virtual reality},
	volume = {D},
	doi = {10/d3zhzb},
	abstract = {We present Slice {WIM}, a method for overview+detail visualization of volume datasets that explores the potential of new interfaces made possible by a virtual reality ({VR}) environment made of two display surfaces: an interactive multi-touch table, and a stereoscopic display wall. Slice {WIM} displays a miniature version of the 3D dataset within a head-tracked stereoscopic view such that it appears to float directly above the multi-touch table. Multitouch gestures on the table are then used to navigate through the dataset and to set slices (cutting planes) through the data. Leveraging the unique table+wall hardware setup, horizontal slices through the data are projected (like a shadow) down onto the table surface, providing a useful 2D data overview to complement the 3D views as well as a data context for interpreting 2D multi-touch gestures made on the table. We demonstrate several strategies for interacting with 2D "shadow slices" on the table surface as a method for controlling the {WIM} and exploring volumetric datasets. Applications of the interface to explore two different volume datasets are presented, and design decisions and limitations are discussed along with feedback from both casual users and domain scientists. Copyright © 2011 by the Association for Computing Machinery, Inc.},
	pages = {191--198},
	number = {212},
	journaltitle = {Proceedings of the Symposium on Interactive 3D Graphics},
	author = {Coffey, Dane and Malbraaten, Nicholas and Le, Trung and Borazjani, Iman and Sotiropoulos, Fotis and Keefe, Daniel F.},
	date = {2011},
	note = {{ISBN}: 9781450305655
46 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{czarnecki_operational_2018,
	title = {Operational Design Domain for Automated Driving Systems - Taxonomy of Basic Terms {WISE} Requirements Analysis Framework for Automated Driving Systems Operational Design Domain for Automated Driving Systems Taxonomy of Basic Terms Krzysztof Czarnecki Univer},
	doi = {10/ggr6kg},
	issue = {July},
	author = {Czarnecki, Krzysztof},
	date = {2018},
}

@article{de_blaeij_risk_2003,
	title = {Risk perception of traffic participants},
	volume = {35},
	issn = {00014575},
	doi = {10/brp67j},
	abstract = {This article analyzes the risk perception of traffic participants by making use of Prospect Theory. This methodology makes a conceptual distinction between the perception of risk and the perception of the outcome of an uncertain event. In the field of transport safety such a distinction is desirable, because risks are typically very low and thus sensitive to misperception by traffic participants. Taking into account such misperception will significantly improve estimates of the valuation of transport safety. The first empirical results show that the valuation of losses is well represented by a utility function that is concave in shape. Secondly, our preliminary results show that individuals base their choice mainly on the possible outcomes and not so much on probabilities whenever there are very small probabilities (say ≤1/100) and "bad outcomes" involved. © 2002 Elsevier Science Ltd. All rights reserved.},
	pages = {167--175},
	number = {2},
	journaltitle = {Accident Analysis and Prevention},
	author = {De Blaeij, Arianne T. and Van Vuuren, Daniel J.},
	date = {2003},
	pmid = {12504137},
	note = {31 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Prospect theory, Risk perception, Traffic safety},
}

@article{das_defining_2020,
	title = {Defining Time-to-Collision Thresholds by the Type of Lead Vehicle in Non-Lane-Based Traffic Environments},
	volume = {21},
	issn = {15580016},
	doi = {10/ghqdrk},
	abstract = {While anecdotal evidence suggests the consideration of a constant time-to-collision ({TTC}) threshold in advanced driver assistance systems, yet the prospect of the warning criterion acting as a catalyst for the evaluation of conflict severities in non-lane based traffic environments needs further assessment. The current study attempts to provide a copula-based framework to capture the potential dependence between {TTC} and centerline separation between the interacting vehicles in staggered-following events according to lead vehicle type. Detailed trajectory data extracted from the video footage of urban roads were used for the purpose of this study. Considering negative association between the considered variables and checked by the performance measures, the bivariate Frank copula model is found acceptable to model the bivariate dependency structure between {TTC} and {CS}. In particular, the results of the study corroborate a pragmatic decreasing relationship of {TTC} with increasing {CS} and decrease in lead vehicle size- a finding which indicates that the risk level of riders in staggered-following scenario varies by the type of front vehicle ahead and the lateral positioning of the vehicles. Based on the results presented in this paper, our recommendation is to use different minimum {TTC} thresholds for different centerline separations and leader-follower pairs to suitably evaluate traffic safety in such non-lane based traffic environments.},
	pages = {4972--4982},
	number = {12},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	author = {Das, Sanhita and Maurya, Akhilesh Kumar},
	date = {2020},
	note = {Publisher: {IEEE}
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Time-to-collision, adaptive cruise control, centerline separation, mixed traffic},
}

@article{dai_vehicle--x_2015,
	title = {Vehicle-to-X (V2X) communication technology},
	volume = {3},
	doi = {10/gjjgxg},
	abstract = {Roads and bridges are the basic guarantee for a country’s social and economic development, and a manifestation of technological progress. In the long-term activities of transforming nature, ancient ancestors of the Chinese gave full play to their talents, overcame all difficulties, carved out paths in mountains and built bridges across waters, writing a distinctive chapter.},
	pages = {405--513},
	journaltitle = {A History of Chinese Science and Technology},
	author = {Dai, Wusan},
	date = {2015},
	note = {{ISBN}: 9783662441633},
}

@article{deng_integrating_2020,
	title = {Integrating Machine Learning with Human Knowledge},
	volume = {23},
	issn = {25890042},
	url = {https://doi.org/10.1016/j.isci.2020.101656},
	doi = {10/ghhpmv},
	abstract = {Machine learning has been heavily researched and widely used in many disciplines. However, achieving high accuracy requires a large amount of data that is sometimes difficult, expensive, or impractical to obtain. Integrating human knowledge into machine learning can significantly reduce data requirement, increase reliability and robustness of machine learning, and build explainable machine learning systems. This allows leveraging the vast amount of human knowledge and capability of machine learning to achieve functions and performance not available before and will facilitate the interaction between human beings and machine learning systems, making machine learning decisions understandable to humans. This paper gives an overview of the knowledge and its representations that can be integrated into machine learning and the methodology. We cover the fundamentals, current status, and recent progress of the methods, with a focus on popular and new topics. The perspectives on future directions are also discussed.},
	pages = {101656},
	number = {11},
	journaltitle = {{iScience}},
	author = {Deng, Changyu and Ji, Xunbi and Rainey, Colton and Zhang, Jianyu and Lu, Wei},
	date = {2020},
	note = {Publisher: Elsevier Inc.
2 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Artificial Intelligence, Computer Science, Human-Centered Computing},
}

@article{desjardins_cooperative_2011,
	title = {Cooperative adaptive cruise control: A reinforcement learning approach},
	volume = {12},
	issn = {15249050},
	doi = {10/ftw7vs},
	abstract = {Recently, improvements in sensing, communicating, and computing technologies have led to the development of driver-assistance systems ({DASs}). Such systems aim at helping drivers by either providing a warning to reduce crashes or doing some of the control tasks to relieve a driver from repetitive and boring tasks. Thus, for example, adaptive cruise control ({ACC}) aims at relieving a driver from manually adjusting his/her speed to maintain a constant speed or a safe distance from the vehicle in front of him/her. Currently, {ACC} can be improved through vehicle-to-vehicle communication, where the current speed and acceleration of a vehicle can be transmitted to the following vehicles by intervehicle communication. This way, vehicle-to-vehicle communication with {ACC} can be combined in one single system called cooperative adaptive cruise control ({CACC}). This paper investigates {CACC} by proposing a novel approach for the design of autonomous vehicle controllers based on modern machine-learning techniques. More specifically, this paper shows how a reinforcement-learning approach can be used to develop controllers for the secure longitudinal following of a front vehicle. This approach uses function approximation techniques along with gradient-descent learning algorithms as a means of directly modifying a control policy to optimize its performance. The experimental results, through simulation, show that this design approach can result in efficient behavior for {CACC}. © 2011 {IEEE}.},
	pages = {1248--1260},
	number = {4},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	author = {Desjardins, Charles and Chaib-Draa, Brahim},
	date = {2011},
	note = {182 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous vehicle control, cooperative adaptive cruise control ({CACC}), neural networks, policy-gradient algorithms, reinforcement learning ({RL})},
}

@article{dey_review_2016,
	title = {A Review of Communication, Driver Characteristics, and Controls Aspects of Cooperative Adaptive Cruise Control ({CACC})},
	volume = {17},
	issn = {15249050},
	doi = {10/f7945p},
	abstract = {Cooperative adaptive cruise control ({CACC}) systems have the potential to increase traffic throughput by allowing smaller headway between vehicles and moving vehicles safely in a platoon at a harmonized speed. {CACC} systems have been attracting significant attention from both academia and industry since connectivity between vehicles will become mandatory for new vehicles in the {USA} in the near future. In this paper, we review three basic and important aspects of {CACC} systems: communications, driver characteristics, and controls to identify the most challenging issues for their real-world deployment. Different routing protocols that support the data communication requirements between vehicles in the {CACC} platoon are reviewed. Promising and suitable protocols are identified. Driver characteristics related issues, such as how to keep drivers engaged in driving tasks during {CACC} operations, are discussed. To achieve mass acceptance, the control design needs to depict real-world traffic variability such as communication effects, driver behavior, and traffic composition. Thus, this paper also discusses the issues that existing {CACC} control modules face when considering close to ideal driving conditions.},
	pages = {491--509},
	number = {2},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	author = {Dey, Kakan C. and Yan, Li and Wang, Xujie and Wang, Yue and Shen, Haiying and Chowdhury, Mashrur and Yu, Lei and Qiu, Chenxi and Soundararaj, Vivekgautham},
	date = {2016},
	note = {186 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Cooperative adaptive cruise control ({CACC}), communication protocols, controls, driver characteristics, string stability},
}

@article{dod_potentiale_2016,
	title = {Potentiale der Car2X-Infrastruktur Random Broadcasting und die Vorteile der Vernetzung Die Zukunft liegt in der Vernetzung},
	issn = {2509-9299},
	doi = {10/gjjgxj},
	abstract = {Kurz und knapp Die Vernetzung von Fahrzeugen miteinander und mit der Infrastruktur bietet vielfältige Möglichkeiten, die Sicherheit und den Komfort im Verkehr zukünftig merklich zu erhöhen. Auf dem Weg zum vernetzten Fah-ren sind aber auch einige technische Fragestellungen zu klären. Dies betrifft zum Beispiel die Verbreitung von Informationen mittels Broadcasting. Dazu wird in diesem Whitepaper das Random Broadcasting mit Hilfe von Simulationen untersucht. Eine mögliche Anwendung für Broadcast-Übertragungen ist die Über-mittlung der Schaltzeiten von Lichtsignalanlagen. Fahrzeuge, die diese Informationen erhalten, können ihre Geschwindigkeit an die Lichtsignalphasen anpassen. Auf diese Weise kann der Verkehrsfluss erhöht werden. Mit den Daten der vernetzten Fahrzeuge können die Lichtsignalanlagen außerdem ihre Schalt-phasen dynamisch an die jeweilige Verkehrsdichte anpassen.},
	issue = {October},
	author = {Dod, Markus and Kischnick, Sara},
	date = {2016},
}

@article{diehl_graph_2019,
	title = {Graph neural networks for modelling traffic participant interaction},
	volume = {2019-June},
	issn = {23318422},
	doi = {10/gjjgxh},
	abstract = {By interpreting a traffic scene as a graph of interacting vehicles, we gain a flexible abstract representation which allows us to apply Graph Neural Network ({GNN}) models for traffic prediction. These naturally take interaction between traffic participants into account while being computationally efficient and providing large model capacity. We evaluate two state-of-the art {GNN} architectures and introduce several adaptations for our specific scenario. We show that prediction error in scenarios with much interaction decreases by 30\% compared to a model that does not take interactions into account. This suggests that interaction is important, and shows that we can model it using graphs. This makes {GNNs} a worthwhile addition to traffic prediction systems.},
	pages = {1--7},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Diehl, Frederik and Brunner, Thomas and Le, Michael Truong and Knoll, Alois},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1903.01254},
	note = {{ISBN}: 9781728105604
14 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{doshi_fast_2020,
	title = {Fast unsupervised anomaly detection in traffic videos},
	volume = {2020-June},
	issn = {21607516},
	doi = {10/ghj5zd},
	abstract = {Anomaly detection in traffic videos has been recently gaining attention due to its importance in intelligent transportation systems. Due to several factors such as weather, viewpoint, lighting conditions, etc. affecting the video quality of a real time traffic feed, it still remains a challenging problem. Even though the performance of state-of-the-art methods on the available benchmark dataset has been competitive, they demand a massive amount of external training data combined with significant computational resources. In this paper, we propose a fast unsupervised anomaly detection system comprising of three modules: preprocessing module, candidate selection module and backtracking anomaly detection module. The preprocessing module outputs stationary objects detected in a video. Then, the candidate selection module removes the misclassified stationary objects using a nearest neighbor approach and then uses K-means clustering to identify potential anomalous regions. Finally, the backtracking anomaly detection algorithm computes a similarity statistic and decides on the onset time of the anomaly. Experimental results on the Track 4 test set of the {NVIDIA} {AI} {CITY} 2020 challenge show the efficacy of the proposed framework as we achieve an F1-score of 0.5926 along with 8.2386 root mean square error ({RMSE}) and are ranked second in the competition.},
	pages = {2658--2664},
	journaltitle = {{IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	author = {Doshi, Keval and Yilmaz, Yasin},
	date = {2020},
	note = {{ISBN}: 9781728193601
5 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{diankov_randomized_2007,
	title = {Randomized statistical path planning},
	doi = {10/djw9nr},
	abstract = {This paper explores the use of statical learning methods on randomized path planning algorithms. A continuous, randomized version of A* is presented along with an empirical analysis showing planning time convergence rates in the robotic manipulation domain. The algorithm relies on several heuristics that capture a manipulator's kinematic feasibility and the local environment. A statistical framework is used to learn one of these heuristics from a large amount of training data saving the need to manually tweak parameters every time the problem changes. Using the appropriate formulation, we show that motion primitives can be automatically extracted from the training data in order to boost planning performance. Furthermore, we propose a Randomized Statistical Path Planning ({RSPP}) paradigm that outlines how a planner using heuristics should take advantage of machine learning algorithms. Planning results are shown for several manipulation problems tested in simulation. ©2007 {IEEE}.},
	pages = {1--6},
	journaltitle = {{IEEE} International Conference on Intelligent Robots and Systems},
	author = {Diankov, Rosen and Kuffner, James},
	date = {2007},
	note = {{ISBN}: 1424409128
75 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{diligenti_integrating_2017,
	title = {Integrating prior knowledge into deep learning},
	volume = {2017-Decem},
	doi = {10/gg4wk7},
	abstract = {Deep learning allows to develop feature representations and train classification models in a fully integrated way. However, learning deep networks is quite hard and it improves over shallow architectures only if a large number of training data is available. Injecting prior knowledge into the learner is a principled way to reduce the amount of required training data, as the learner does not need to induce the knowledge from the data itself. In this paper we propose a general and principled way to integrate prior knowledge when training deep networks. Semantic Based Regularization ({SBR}) is used as underlying framework to represent the prior knowledge, expressed as a collection of first-order logic clauses ({FOL}), and where each task to be learned corresponds to a predicate in the knowledge base. The knowledge base correlates the tasks to be learned and it is translated into a set of constraints which are integrated into the learning process via backpropagation. The experimental results show how the integration of the prior knowledge boosts the accuracy of a state-of-the-art deep network on an image classification task.},
	pages = {920--923},
	journaltitle = {Proceedings - 16th {IEEE} International Conference on Machine Learning and Applications, {ICMLA} 2017},
	author = {Diligenti, Michelangelo and Roychowdhury, Soumali and Gori, Marco},
	date = {2017},
	note = {{ISBN}: 9781538614174
20 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Deep Learning, Prior Knowledge, Statistical Relational Learning},
}

@article{ferreira_cooperative_2019,
	title = {Cooperative, connected and automated mobility (Ccam): Technologies and applications},
	volume = {8},
	issn = {20799292},
	doi = {10/gjjgxn},
	pages = {8--11},
	number = {12},
	journaltitle = {Electronics (Switzerland)},
	author = {Ferreira, Joaquim},
	date = {2019},
	note = {1 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{fang_object_2017,
	title = {Object detection meets knowledge graphs},
	volume = {0},
	issn = {10450823},
	doi = {10/ggsdct},
	abstract = {Object detection in images is a crucial task in computer vision, with important applications ranging from security surveillance to autonomous vehicles. Existing state-of-the-art algorithms, including deep neural networks, only focus on utilizing features within an image itself, largely neglecting the vast amount of background knowledge about the real world. In this paper, we propose a novel framework of knowledge-aware object detection, which enables the integration of external knowledge such as knowledge graphs into any object detection algorithm. The framework employs the notion of semantic consistency to quantify and generalize knowledge, which improves object detection through a re-optimization process to achieve better consistency with background knowledge. Finally, empirical evaluation on two benchmark datasets show that our approach can significantly increase recall by up to 6.3 points without compromising mean average precision, when compared to the state-of-the-art baseline.},
	pages = {1661--1667},
	journaltitle = {{IJCAI} International Joint Conference on Artificial Intelligence},
	author = {Fang, Yuan and Kuan, Kingsley and Lin, Jie and Tan, Cheston and Chandrasekhar, Vijay},
	date = {2017},
	note = {{ISBN}: 9780999241103
42 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Machine Learning: Knowledge-based Learning, Robotics and Vision: Vision and Perception},
}

@article{duan_emerging_2020,
	title = {Emerging Technologies for 5G-{IoV} Networks: Applications, Trends and Opportunities},
	volume = {34},
	issn = {1558156X},
	doi = {10/gjjgxm},
	abstract = {With the evolution of the technologies for {IoV} to be intelligent and interconnected, V2X communication technology serves as a core technology for information interaction among intelligent connected vehicles, and an important technology to realize environment sensing for future autonomous driving. In order to provide wireless communication services with ultra-low delay, ultra-high reliability and ultra-large bandwidth, this article proposes architectures of 5G-V2X communication networks by exploiting the technologies of 5G new radio ({NR}), network slicing, and deviceto- device communications. We discuss their principles and key features, and foresee the challenges, opportunities, and future research trends. The applications of related technologies of the software defined network ({SND}) and multi-access edge computing ({MEC}) are also introduced. Finally, the technologies of information security and privacy protection are identified to support the diverse services and applications in the future 5G-V2X networks.},
	pages = {283--289},
	number = {5},
	journaltitle = {{IEEE} Network},
	author = {Duan, Wei and Gu, Jinyuan and Wen, Miaowen and Zhang, Guoan and Ji, Yancheng and Mumtaz, Shahid},
	date = {2020},
	note = {6 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{driving_telecarla_2020,
	title = {{TELECARLA} : An Open Source Extension of the {CARLA} Simulator for {TELECARLA} : An Open Source Extension of the {CARLA} Simulator for Teleoperated Driving Research Using Off-the-Shelf Components},
	doi = {10/gjjgxk},
	issue = {October},
	author = {Driving, Teleoperated and Hofbauer, Markus and Kuhn, Christopher B and Petrovic, Goran and Steinbach, Eckehard},
	date = {2020},
	note = {4 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{erdogan_real-world_2019,
	title = {Real-world maneuver extraction for autonomous vehicle validation: A comparative study},
	volume = {2019-June},
	doi = {10/ghpksq},
	abstract = {Advanced Driver Assistance Systems are becoming more dominant due to their improved safety and comfort potential. Any functional development performed in this domain progresses conjointly with its validation and testing. The current trend is to use scenario-based testing approaches which are formed by the hand-driven scenario data-sets and expert inputs. However, in Autonomous Driving domain, testing all possible cases and combination of multiple parameters is not feasible due to sheer number of test-cases required, while the coverage of these expert-knowledge based sets cannot be quantitatively assessed. Extracted scenarios from realworld data can be used to strengthen the breadth of the expert knowledge driven scenario data-set. In this work, we propose three methods to extract scenarios from the time-series states of the real-world measurement data. Specifically, multiple classification algorithms are deployed for inference on i) time domain that identifies when an action occurred and ii) scenario domain that identifies what happened during this time period. Experimental evaluation over real-world collected data validates the efficacy of each of these methods even under sensor noise and other artefacts propagated to the realworld measurement values. Varying precision and recall metrics observed over the models identify the strength/weaknesses of the evaluated approach.},
	pages = {267--272},
	issue = {Iv},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Erdogan, Ahmetcan and Ugranli, Burak and Adali, Erkan and Sentas, Ali and Mungan, Eren and Kaplan, Emre and Leitner, Andrea},
	date = {2019},
	note = {Publisher: {IEEE}
{ISBN}: 9781728105604
5 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{gao_deep_2020,
	title = {Deep reinforcement learning for indoor mobile robot path planning},
	volume = {20},
	issn = {14248220},
	doi = {10/ghd6cx},
	abstract = {This paper proposes a novel incremental training mode to address the problem of Deep Reinforcement Learning ({DRL}) based path planning for a mobile robot. Firstly, we evaluate the related graphic search algorithms and Reinforcement Learning ({RL}) algorithms in a lightweight 2D environment. Then, we design the algorithm based on {DRL}, including observation states, reward function, network structure as well as parameters optimization, in a 2D environment to circumvent the time-consuming works for a 3D environment. We transfer the designed algorithm to a simple 3D environment for retraining to obtain the converged network parameters, including the weights and biases of deep neural network ({DNN}), etc. Using these parameters as initial values, we continue to train the model in a complex 3D environment. To improve the generalization of the model in different scenes, we propose to combine the {DRL} algorithm Twin Delayed Deep Deterministic policy gradients ({TD}3) with the traditional global path planning algorithm Probabilistic Roadmap ({PRM}) as a novel path planner ({PRM}+{TD}3). Experimental results show that the incremental training mode can notably improve the development efficiency. Moreover, the {PRM}+{TD}3 path planner can effectively improve the generalization of the model.},
	pages = {1--15},
	number = {19},
	journaltitle = {Sensors (Switzerland)},
	author = {Gao, Junli and Ye, Weijie and Guo, Jing and Li, Zhongjuan},
	date = {2020},
	pmid = {32992750},
	note = {3 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Deep neural network, Deep reinforcement learning, Generalization, Incremental training mode, Mobile robot, Path planning, Reward function},
}

@article{feyzabadi_knowledge_2014,
	title = {Knowledge and data representation for motion planning in dynamic environments},
	volume = {274},
	issn = {21945357},
	doi = {10/gjjgxp},
	abstract = {In this paper we describe our initial efforts to develop a knowledge base for motion planning in dynamic environments. Our eventual goal is to smooth the design and integration of multiple heterogeneous robots working in shared environments, and to enable the creation of libraries of plans that can be shared and reused by different robots. We furthermore attempt to align our work with the ongoing activities of the {IEEE} Ontologies for Robotics and Automation working group.},
	pages = {233--240},
	issue = {February},
	journaltitle = {Advances in Intelligent Systems and Computing},
	author = {Feyzabadi, Seyedshams and Carpin, Stefano},
	date = {2014},
	note = {{ISBN}: 9783319055817
2 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{fridman_mit_2017,
	title = {{MIT} advanced vehicle technology study: Large-scale naturalistic driving study of driver behavior and interaction with automation},
	issn = {2169-3536},
	doi = {10/gghppw},
	abstract = {Today, and possibly for a long time to come, the full driving task is too complex an activity to be fully formalized as a sensing-acting robotics system that can be explicitly solved through model-based and learning-based approaches in order to achieve full unconstrained vehicle autonomy. Localization, mapping, scene perception, vehicle control, trajectory optimization, and higher-level planning decisions associated with autonomous vehicle development remain full of open challenges. This is especially true for unconstrained, real-world operation where the margin of allowable error is extremely small and the number of edge-cases is extremely large. Until these problems are solved, human beings will remain an integral part of the driving task, monitoring the {AI} system as it performs anywhere from just over 0\% to just under 100\% of the driving. The governing objectives of the {MIT} Advanced Vehicle Technology ({MIT}-{AVT}) study are to (1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning based internal and external perception systems, (2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology, and (3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 23 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac {CT}6 vehicles for both long-term (over a year per driver) and medium term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include {IMU}, {GPS}, {CAN} messages, and high-definition video streams of the driver face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 122 participants, 15,610 days of participation, 511,638 miles, and 7.1 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.},
	pages = {1--16},
	journaltitle = {{arXiv}},
	author = {Fridman, Lex and Brown, Daniel E. and Glazer, Michael and Angell, William and Dodd, Spencer and Jenik, Benedikt and Terwilliger, Jack and Patsekin, Aleksandr and Kindelsberger, Julia and Ding, Li and Seaman, Sean and Mehler, Alea and Sipperley, Andrew and Pettinato, Anthony and Seppelt, Bobbie and Angell, Linda and Mehler, Bruce and Reimer, Bryan},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1711.06976},
	note = {60 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{ge_ultra-reliable_2019,
	title = {Ultra-Reliable Low-Latency Communications in Autonomous Vehicular Networks},
	volume = {68},
	issn = {19399359},
	doi = {10/gg9r5n},
	abstract = {Autonomous vehicles are expected to emerge as a main trends in vehicle development over the next decade. To support autonomous vehicles, ultra-reliable low-latency communications ({URLLC}) is required between autonomous vehicles and infrastructure networks, e.g., a fifth-generation (5G) cellular networks. Hence, reliability and latency must be jointly investigated in 5G autonomous vehicular networks. In this paper, utilizing the Euclidean norm theory, we first propose a reliability and latency joint function to evaluate the joint impact of reliability and latency in 5G autonomous vehicular networks. The interactions between reliability and latency are illustrated via Monte Carlo simulations of 5G autonomous vehicular networks. To improve both the reliability and latency performance and implement {URLLC}, a new network slicing solution that extends from resource slicing to service and function slicing is presented for 5G autonomous vehicular networks. The simulation results indicate that the proposed network slicing solution can improve both the reliability and latency performance and ensure {URLLC} in 5G autonomous vehicular networks.},
	pages = {5005--5016},
	number = {5},
	journaltitle = {{IEEE} Transactions on Vehicular Technology},
	author = {Ge, Xiaohu},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1903.01863},
	note = {Publisher: {IEEE}
47 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Reliability, autonomous vehicle, latency, network slicing, vehicular network},
}

@article{grigorescu_survey_2020,
	title = {A survey of deep learning techniques for autonomous driving},
	volume = {37},
	issn = {15564967},
	doi = {10/gg9ztb},
	abstract = {The last decade witnessed increasingly rapid progress in self-driving vehicle technology, mainly backed up by advances in the area of deep learning and artificial intelligence ({AI}). The objective of this paper is to survey the current state-of-the-art on deep learning technologies used in autonomous driving. We start by presenting {AI}-based self-driving architectures, convolutional and recurrent neural networks, as well as the deep reinforcement learning paradigm. These methodologies form a base for the surveyed driving scene perception, path planning, behavior arbitration, and motion control algorithms. We investigate both the modular perception-planning-action pipeline, where each module is built using deep learning methods, as well as End2End systems, which directly map sensory information to steering commands. Additionally, we tackle current challenges encountered in designing {AI} architectures for autonomous driving, such as their safety, training data sources, and computational hardware. The comparison presented in this survey helps gain insight into the strengths and limitations of deep learning and {AI} approaches for autonomous driving and assist with design choices.},
	pages = {362--386},
	number = {3},
	journaltitle = {Journal of Field Robotics},
	author = {Grigorescu, Sorin and Cocias, Tiberiu and Trasnea, Bogdan and Macesanu, Gigel and Cocias, Tiberiu and Macesanu, Gigel},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1910.07738},
	note = {94 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {{AI} for self-driving vehicles, artificial intelligence, autonomous driving, deep learning for autonomous driving},
}

@article{gao_vectornet_2020,
	title = {{VectorNet}: Encoding {HD} maps and agent dynamics from vectorized representation},
	issn = {10636919},
	doi = {10/ghbchs},
	abstract = {Behavior prediction in dynamic, multi-agent systems is an important problem in the context of self-driving cars, due to the complex representations and interactions of road components, including moving agents (e.g. pedestrians and vehicles) and road context information (e.g. lanes, traffic lights). This paper introduces {VectorNet}, a hierarchical graph neural network that first exploits the spatial locality of individual road components represented by vectors and then models the high-order interactions among all components. In contrast to most recent approaches, which render trajectories of moving agents and road context information as bird-eye images and encode them with convolutional neural networks ({ConvNets}), our approach operates on a vector representation. By operating on the vectorized high definition ({HD}) maps and agent trajectories, we avoid lossy rendering and computationally intensive {ConvNet} encoding steps. To further boost {VectorNet}'s capability in learning context features, we propose a novel auxiliary task to recover the randomly masked out map entities and agent trajectories based on their context. We evaluate {VectorNet} on our in-house behavior prediction benchmark and the recently released Argoverse forecasting dataset. Our method achieves on par or better performance than the competitive rendering approach on both benchmarks while saving over 70\% of the model parameters with an order of magnitude reduction in {FLOPs}. It also outperforms the state of the art on the Argoverse dataset.},
	pages = {11522--11530},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Gao, Jiyang and Sun, Chen and Zhao, Hang and Shen, Yi and Anguelov, Dragomir and Li, Congcong and Schmid, Cordelia},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2005.04259},
	note = {28 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{hallerbach_simulation-based_2018,
	title = {Simulation-based Identification of Critical Scenarios for Cooperative and Automated Vehicles},
	volume = {2018-April},
	issn = {01487191},
	doi = {10/ggh9kf},
	abstract = {One of the major challenges for the automotive industry will be the release and validation of cooperative and automated vehicles. The immense driving distance that needs to be covered for a conventional validation process requires the development of new testing procedures. Further, due to limited market penetration in the beginning, the driving behavior of other human traffic participants, regarding a mixed traffic environment, will have a significant impact on the functionality of these vehicles. In this paper, a generic simulation-based toolchain for the model-in-the-loop identification of critical scenarios will be introduced. The proposed methodology allows the identification of critical scenarios with respect to the vehicle development process. The current development status of cooperative and automated vehicle determines the availability of testable simulation models, software, and components. The identification process is realized by a coupled simulation framework. A combination of a vehicle dynamics simulation that includes a digital prototype of the cooperative and automated vehicle, a traffic simulation that provides the surrounding environment, and a cooperation simulation including cooperative features, is used to establish a suitable comprehensive simulation environment. The behavior of other traffic participants is considered in the traffic simulation environment. The criticality of the scenarios is determined by appropriate metrics. Within the context of this paper, both standard safety metrics and newly developed traffic quality metrics are used for evaluation. Furthermore, we will show how the use of these new metrics allows for investigating the impact of cooperative and automated vehicles on traffic. The identified critical scenarios are used as an input for X-in-the-Loop methods, test benches, and proving ground tests to achieve an even more precise comparison to real-world situations. As soon as the vehicle development process is in a mature state, the digital prototype becomes a "digital twin" of the cooperative and automated vehicle.},
	issue = {April},
	journaltitle = {{SAE} Technical Papers},
	author = {Hallerbach, Sven and Xia, Yiqun and Eberle, Ulrich and Koester, Frank},
	date = {2018},
	note = {38 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{guo_driving_2018,
	title = {Driving behaviour style study with a hybrid deep learning framework based on {GPS} data},
	volume = {10},
	issn = {20711050},
	doi = {10/gd77sn},
	abstract = {Innovative technologies and traffic data sources provide great potential to extend advanced strategies and methods in travel behaviour research. Considering the increasing availability of real-time vehicle trajectory data and stimulated by the advances in the modelling and analysis of big data, this paper developed a hybrid unsupervised deep learning model to study driving bahaviour and risk patterns. The approach combines Autoencoder and Self-organized Maps ({AESOM}), to extract latent features and classify driving behaviour. The specialized neural networks are applied to data from 4032 observations collected from Global Positioning System ({GPS}) sensors in Shenzhen, China. In two case studies, improper vehicle lateral position maintenance, speeding and inconsistent or excessive acceleration and deceleration have been identified. The experiments have shown that back propagation through multi-layer autoencoders is effective for non-linear and multi-modal dimensionality reduction, giving low reconstruction errors from big {GPS} datasets.},
	pages = {1--16},
	number = {7},
	journaltitle = {Sustainability (Switzerland)},
	author = {Guo, Jingqiu and Liu, Yangzexi and Zhang, Lanfang and Wang, Yibing},
	date = {2018},
	note = {14 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {{AESOM}, Deep learning, Driving behaviour analysis, {GPS} data, Risk pattern},
}

@article{hamaguchi_rare_2019,
	title = {Rare event detection using disentangled representation learning},
	volume = {2019-June},
	issn = {10636919},
	doi = {10/ghzs69},
	abstract = {This paper presents a novel method for rare event detection from an image pair with class-imbalanced datasets. A straightforward approach for event detection tasks is to train a detection network from a large-scale dataset in an end-to-end manner. However, in many applications such as building change detection on satellite images, few positive samples are available for the training. Moreover, an image pair of scenes contains many trivial events, such as in illumination changes or background motions. These many trivial events and the class imbalance problem lead to false alarms for rare event detection. In order to overcome these difficulties, we propose a novel method to learn disentangled representations from only low-cost negative samples. The proposed method disentangles the different aspects in a pair of observations: variant and invariant factors that represent trivial events and image contents, respectively. The effectiveness of the proposed approach is verified by the quantitative evaluations on four change detection datasets, and the qualitative analysis shows that the proposed method can acquire the representations that disentangle rare events from trivial ones.},
	pages = {9319--9327},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Hamaguchi, Ryuhei and Sakurada, Ken and Nakamura, Ryosuke},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1812.01285},
	note = {{ISBN}: 9781728132938
11 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Categorization, Deep Learning, Recognition: Detection, Representation Learning, Retrieval, Scene Analysis and Understanding, Vision Applications and S},
}

@article{hayatpur_datahop_2020,
	title = {{DataHop}: Spatial data exploration in virtual reality},
	doi = {10/gjjg3b},
	abstract = {Virtual reality has recently been adopted for use within the domain of visual analytics because it can provide users with an endless workspace within which they can be actively engaged and use their spatial reasoning skills for data analysis. However, virtual worlds need to utilize layouts and organizational schemes that are meaningful to the user and beneficial for data analysis. This paper presents {DataHop}, a novel visualization system that enables users to lay out their data analysis steps in a virtual environment. With a Filter, a user can specify the modification they wish to perform on one or more input data panels (i.e., containers of points), along with where output data panels should be placed in the virtual environment. Using this simple tool, highly intricate and useful visualizations may be generated and traversed by harnessing a user's spatial abilities. An exploratory study conducted with six virtual reality users evaluated the usability, affordances, and performance of {DataHop} for data analysis tasks, and found that spatially mapping one's workflow can be beneficial when exploring multidimensional datasets.},
	pages = {818--828},
	journaltitle = {{UIST} 2020 - Proceedings of the 33rd Annual {ACM} Symposium on User Interface Software and Technology},
	author = {Hayatpur, Devamardeep and Xia, Haijun and Wigdor, Daniel},
	date = {2020},
	note = {{ISBN}: 9781450375146
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Immersive data visualization, Spatial skills},
}

@article{herty_analysis_2018,
	title = {Analysis of risk levels for traffic on a multi-lane highway⁎},
	volume = {51},
	issn = {24058963},
	url = {https://doi.org/10.1016/j.ifacol.2018.07.008},
	doi = {10/gjjgxq},
	abstract = {We present an analysis of risk levels on multi-lane roads. The aim is to use the crash metrics to understand which direction of the flow mainly influences the safety in traffic flow. In fact, on multi-lane highways interactions among vehicles occur also with lane changing and we show that they strongly affect the level of potential conflicts. In particular, in this study we consider the Time-To-Collision as risk metric and we use the experimental data collected on the A3 German highway.},
	pages = {43--48},
	number = {9},
	journaltitle = {{IFAC}-{PapersOnLine}},
	author = {Herty, Michael and Visconti, Giuseppe},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1710.05752},
	note = {Publisher: Elsevier B.V.
3 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Traffic flow, multi-lane highways, risk levels, time-to-collision},
}

@article{hou_new_2014,
	title = {New algorithms for computing the time-to-collision in freeway traffic simulation models},
	volume = {2014},
	issn = {16875273},
	doi = {10/f6z5kj},
	abstract = {Ways to estimate the time-to-collision are explored. In the context of traffic simulation models, classical lane-based notions of vehicle location are relaxed and new, fast, and efficient algorithms are examined. With trajectory conflicts being the main focus, computational procedures are explored which use a two-dimensional coordinate system to track the vehicle trajectories and assess conflicts. Vector-based kinematic variables are used to support the calculations. Algorithms based on boxes, circles, and ellipses are considered. Their performance is evaluated in the context of computational complexity and solution time. Results from these analyses suggest promise for effective and efficient analyses. A combined computation process is found to be very effective.},
	journaltitle = {Computational Intelligence and Neuroscience},
	author = {Hou, Jia and List, George F. and Guo, Xiucheng},
	date = {2014},
	pmid = {25628650},
	note = {16 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{happee_take-over_2017,
	title = {Take-over performance in evasive manoeuvres},
	volume = {106},
	issn = {00014575},
	url = {https://doi.org/10.1016/j.aap.2017.04.017},
	doi = {10/gcf3ts},
	abstract = {We investigated after effects of automation in take-over scenarios in a high-end moving-base driving simulator. Drivers performed evasive manoeuvres encountering a blocked lane in highway driving. We compared the performance of drivers 1) during manual driving, 2) after automated driving with eyes on the road while performing the cognitively demanding n-back task, and 3) after automated driving with eyes off the road performing the visually demanding {SuRT} task. Both minimum time to collision ({TTC}) and minimum clearance towards the obstacle disclosed a substantial number of near miss events and are regarded as valuable surrogate safety metrics in evasive manoeuvres. {TTC} proved highly sensitive to the applied definition of colliding paths, and we prefer robust solutions using lane position while disregarding heading. The extended time to collision ({ETTC}) which takes into account acceleration was close to the more robust conventional {TTC}. In line with other publications, the initial steering or braking intervention was delayed after using automation compared to manual driving. This resulted in lower {TTC} values and stronger steering and braking actions. Using automation, effects of cognitive distraction were similar to visual distraction for the intervention time with effects on the surrogate safety metric {TTC} being larger with visual distraction. However the precision of the evasive manoeuvres was hardly affected with a similar clearance towards the obstacle, similar overshoots and similar excursions to the hard shoulder. Further research is needed to validate and complement the current simulator based results with human behaviour in real world driving conditions. Experiments with real vehicles can disclose possible systematic differences in behaviour, and naturalistic data can serve to validate surrogate safety measures like {TTC} and obstacle clearance in evasive manoeuvres.},
	pages = {211--222},
	issue = {June},
	journaltitle = {Accident Analysis and Prevention},
	author = {Happee, Riender and Gold, Christian and Radlmayr, Jonas and Hergeth, Sebastian and Bengler, Klaus},
	date = {2017},
	pmid = {28645018},
	note = {Publisher: Elsevier
27 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Automated driving, Evasive, Fallback, Surrogate safety metric, Take-over, Time to collision},
}

@article{donalek_immersive_2015,
	title = {Immersive and collaborative data visualization using virtual reality platforms},
	doi = {10/gf82hr},
	abstract = {Effective data visualization is a key part of the discovery process in the era of 'big data'. It is the bridge between the quantitative content of the data and human intuition, and thus an essential component of the scientific path from data into knowledge and understanding. Visualization is also essential in the data mining process, directing the choice of the applicable algorithms, and in helping to identify and remove bad data from the analysis. However, a high complexity or a high dimensionality of modern data sets represents a critical obstacle. How do we visualize interesting structures and patterns that may exist in hyper-dimensional data spaces? A better understanding of how we can perceive and interact with multidimensional information poses some deep questions in the field of cognition technology and human-computer interaction. To this effect, we are exploring the use of immersive virtual reality platforms for scientific data visualization, both as software and inexpensive commodity hardware. These potentially powerful and innovative tools for multi-dimensional data visualization can also provide an easy and natural path to a collaborative data visualization and exploration, where scientists can interact with their data and their colleagues in the same visual space. Immersion provides benefits beyond the traditional 'desktop' visualization tools: it leads to a demonstrably better perception of a datascape geometry, more intuitive data understanding, and a better retention of the perceived relationships in the data.},
	pages = {609--614},
	journaltitle = {Proceedings - 2014 {IEEE} International Conference on Big Data, {IEEE} Big Data 2014},
	author = {Donalek, Ciro and Djorgovski, S. G. and Cioc, Alex and Wang, Anwell and Zhang, Jerry and Lawler, Elizabeth and Yeh, Stacy and Mahabal, Ashish and Graham, Matthew and Drake, Andrew and Davidoff, Scott and Norris, Jeffrey S. and Longo, Giuseppe},
	date = {2015},
	note = {Publisher: {IEEE}
{ISBN}: 9781479956654
207 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Astroinformatics, Big data, Data analysis, Pattern recognition, Virtual reality, Visualization},
}

@article{huang_cooperative_2020,
	title = {Cooperative Adaptive Cruise Control and exhaust emission evaluation under heterogeneous connected vehicle network environment in urban city},
	volume = {256},
	issn = {10958630},
	url = {https://doi.org/10.1016/j.jenvman.2019.109975},
	doi = {10/gjjgxr},
	abstract = {With the development of information communication and artificial intelligence, the {ICV} (intelligent connected vehicle) will inevitably play an important part in future urban transport system. In this paper, we study the car following behaviour under the heterogeneous {ICV} environment. The time to receive information varies from vehicle to vehicle, since the manual vehicles and autonomous vehicles co-exist on the road. By introducing time-varying lags function, a new car following model is proposed, and the cooperative control strategy of this model is studied. Based on Lyapunov function theory and linear matrix inequality ({LMI}) approach, the sufficient condition that the existence of the feedback controller is given, which makes the closed-loop system asymptotically stable under mixed traffic flow environment. That is to say, traffic congestion phenomenon under heterogeneous traffic flow can be effectively suppressed, and the feedback controller gain matrix can be obtained via solving linear matrix inequality. Finally, by simulation the method is verified effective in alleviating traffic congestions and reducing fuel consumption and exhaust emissions. It could be a useful reference to Cooperative Vehicle Infrastructure System and Smart City.},
	pages = {109975},
	issue = {October 2019},
	journaltitle = {Journal of Environmental Management},
	author = {Huang, Ling and Zhai, Cong and Wang, Haiwei and Zhang, Ronghui and Qiu, Zhijun and Wu, Jianping},
	date = {2020},
	pmid = {31989968},
	note = {Publisher: Elsevier Ltd
3 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Cooperative adaptive cruise control, Exhaust emission evaluation, Heterogeneous, Intelligent connected vehicle, Smart city, Transportation simulation and environment},
}

@article{haeuslschmid_supporting_2017,
	title = {Supporting trust in autonomous driving},
	doi = {10/ggmrzs},
	abstract = {Autonomous cars will likely hit the market soon, but trust into such a technology is one of the big discussion points in the public debate. Drivers who have always been in complete control of their car are expected to willingly hand over control and blindly trust a technology that could kill them. We argue that trust in autonomous driving can be increased by means of a driver interface that visualizes the car's interpretation of the current situation and its corresponding actions. To verify this, we compared different visualizations in a user study, overlaid to a driving scene: (1) a chauffeur avatar, (2) a world in miniature, and (3) a display of the car's indicators as the baseline. The world in miniature visualization increased trust the most. The human-like chauffeur avatar can also increase trust, however, we did not find a significant difference between the chauffeur and the baseline.},
	pages = {319--329},
	journaltitle = {International Conference on Intelligent User Interfaces, Proceedings {IUI}},
	author = {Haeuslschmid, Renate and Von Buelow, Max and Pfleging, Bastian and Butz, Andreas},
	date = {2017},
	note = {{ISBN}: 9781450343480
46 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Anthropomorphism, Automated driving, Avatar, Trust, Trustworthiness},
}

@article{huang_integration_2001,
	title = {An integration of {GIS}, virtual reality and the internet for visualization, analysis and exploration of spatial data},
	volume = {15},
	issn = {13658816},
	doi = {10/bwrxkt},
	abstract = {This paper explores the way in which {GIS}, Virtual Reality ({VR}) and the Internet are closely integrated through the link of Virtual Reality Modelling Language ({VRML}) for spatial data visualization, analysis and exploration. Integration takes advantage of each component, and enables the dynamic 3D content to be built, visualized, interacted with and deployed all on the Web. To accomplish this, a hybrid approach that merges the conventional client-side and server-side methods is proposed, which offers the best of both worlds in terms of flexibility and capability, as well as the rational use of computing resources. Based on this approach, a Web-based prototype toolkit is designed and implemented by using an affordable desktop {GIS} through its macro language together with Java, Common Gateway Interface ({CGI}) and {HTML} programming. This toolkit comprises a 3D visualization tool, a 3D analysis tool, and a Java/{VRML} interface, which are respectively used for the creation of {VRML} models from 2D maps, surface analysis (e.g. profile creation and visibility analysis), and interaction (e.g. selecting and querying) with the output {VRML} worlds of 3D visualization and analysis. It is demonstrated that this toolkit provides an integrated environment, facilitating users to gain insights from the interaction with virtual environments that are built from existing {GIS} databases.},
	pages = {439--456},
	number = {5},
	journaltitle = {International Journal of Geographical Information Science},
	author = {Huang, B. and Jiang, B. and Li, H.},
	date = {2001},
	note = {149 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{kim_few-shot_2020,
	title = {Few-Shot Object Detection via Knowledge Transfer},
	issn = {23318422},
	doi = {10/gjjgxt},
	abstract = {Conventional methods for object detection usually require substantial amounts of training data and annotated bounding boxes. If there are only a few training data and annotations, the object detectors easily overfit and fail to generalize. It exposes the practical weakness of the object detectors. On the other hand, human can easily master new reasoning rules with only a few demonstrations using previously learned knowledge. In this paper, we introduce a few-shot object detection via knowledge transfer, which aims to detect objects from a few training examples. Central to our method is prototypical knowledge transfer with an attached meta-learner. The meta-learner takes support set images that include the few examples of the novel categories and base categories, and predicts prototypes that represent each category as a vector. Then, the prototypes reweight each {RoI} (Region-of-Interest) feature vector from a query image to remodels R-{CNN} predictor heads. To facilitate the remodeling process, we predict the prototypes under a graph structure, which propagates information of the correlated base categories to the novel categories with explicit guidance of prior knowledge that represents correlations among categories. Extensive experiments on the {PASCAL} {VOC} dataset verifies the effectiveness of the proposed method.},
	journaltitle = {{arXiv}},
	author = {Kim, Geonuk and Jung, Hong Gyu and Lee, Seong Whan},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2008.12496},
	note = {{ISBN}: 9781728185262
3 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Few-shot learning, Knowledge transfer, Object detection},
}

@article{hu_abnormal_2020,
	title = {Abnormal Driving Detection with Normalized Driving Behavior Data: A Deep Learning Approach},
	volume = {69},
	issn = {19399359},
	doi = {10/gjjgxs},
	abstract = {Abnormal driving may cause serious danger to both the driver and the public. Existing detectors of abnormal driving behavior are mainly based on shallow models, which require large quantities of labeled data. The acquisition and labelling of abnormal driving data are, however, difficult, labor-intensive and time-consuming. This situation inspires us to rethink the abnormal driving detection problem and to apply deep architecture models. In this study, we establish a novel deep-learning-based model for abnormal driving detection. A stacked sparse autoencoders model is used to learn generic driving behavior features. The model is trained in a greedy layer-wise fashion. As far as the authors know, this is the first time that a deep learning approach is applied using autoencoders as building blocks to represent driving features for abnormal driving detection. In addition, a method for denoising is added to the algorithm to increase the robustness of feature expression. The dropout technology is introduced into the entire training process to avoid overfitting. Experiments carried out on our self-created driving behavior dataset demonstrate that the proposed scheme achieves a superior performance for abnormal driving detection compared to the state-of-the-art.},
	pages = {6943--6951},
	number = {7},
	journaltitle = {{IEEE} Transactions on Vehicular Technology},
	author = {Hu, Jie and Zhang, Xiaoqin and Maybank, Stephen},
	date = {2020},
	note = {1 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Abnormal driving detection, deep learning, stacked autoencoder},
}

@article{jacobs_detecting_2019,
	title = {Detecting Out-of-Distribution Traffic Signs},
	doi = {10/gjjg28},
	abstract = {© The Author 2015. The shortfin mako shark (Isurus oxyrinchus) is a species commonly targeted by commercial and recreational anglers in many parts of the developed world. In Australia, the species is targeted by recreational anglers only, under the assumption that most of the sharks are released and populations remain minimally impacted. If released sharks do not survive, the current management strategy will need to be revised. Shortfin mako sharks are commonly subjected to lengthy angling events; however, their endothermic physiology may provide an advantage over ectothermic fishes when recovering from exercise. This study assessed the post-release survival of recreationally caught shortfin mako sharks using Survivorship Pop-up Archival Transmitting ({sPAT}) tags and examined physiological indicators of capture stress from blood samples as well as any injuries that may be caused by hook selection. Survival estimates were based on 30 shortfin mako sharks captured off the southeastern coast of Australia. Three mortalities were observed over the duration of the study, yielding an overall survival rate of 90\%. All mortalities occurred in sharks angled for {\textless} 30 min. Sharks experienced increasing plasma lactate with longer fight times and higher sea surface temperatures ({SSTs}), increased plasma glucose at higher {SSTs} and depressed expression of heat shock protein 70 and β-hydroxybutyrate at higher {SSTs}. Long fight times did not impact survival. Circle hooks significantly reduced foul hooking when compared with J hooks. Under the conditions of this study, we found that physical injury associated with hook choice is likely to have contributed to an increased likelihood of mortality, whereas the high aerobic scope associated with the species' endothermy probably enabled it to cope with long fight times and the associated physiological responses to capture.},
	journaltitle = {Computing},
	author = {Jacobs, Bart},
	date = {2019},
	note = {{ISBN}: 9783851256635},
}

@article{junietz_criticality_2018,
	title = {Criticality Metric for the Safety Validation of Automated Driving using Model Predictive Trajectory Optimization},
	volume = {2018-Novem},
	doi = {10/ggj782},
	abstract = {The safety validation of automated driving of {SAE} level 3 and higher ({AD}) is still an unsolved issue. In the validation process, criticality metrics can be used for two different purposes. First, for the identification of test scenarios from recorded data that are later tested in simulation. Secondly, for an estimation of the safety of a specific {AD} system based on the likelihood of critical situations in test drives or in other words as a safety surrogate. In the past, different metrics for those purposes have been defined that work well in specific scenarios such as longitudinal traffic. However, a metric that describes criticality in all situations and is applicable to human and {AD} traffic is currently not available. In this paper, an approach to define a criticality metric is introduced. The metric is based on the definition of criticality as the level of driving requirements in the specific situation. The computation of the proposed metric uses elements of model predictive control using an objective function that contains four elements that describe the difficulty of the driving task. Based on those demands and a simplified driving dynamics model, the solution with the minimal criticality is computed. Finally, the metric is tested in four test scenarios that are typical for highway traffic. A short parameter variation study is conducted in order to study certain effects of the algorithm and to identify room for improvement.},
	pages = {60--65},
	journaltitle = {{IEEE} Conference on Intelligent Transportation Systems, Proceedings, {ITSC}},
	author = {Junietz, Philipp and Bonakdar, Farid and Klamann, Björn and Winner, Hermann},
	date = {2018},
	note = {{ISBN}: 9781728103235
16 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Automated Driving, {MPC}, Metric, Safety Validation},
}

@article{huang_apolloscape_2018,
	title = {The apolloscape dataset for autonomous driving},
	volume = {2018-June},
	issn = {21607516},
	doi = {10/ggv97t},
	abstract = {Scene parsing aims to assign a class (semantic) label for each pixel in an image. It is a comprehensive analysis of an image. Given the rise of autonomous driving, pixel-accurate environmental perception is expected to be a key enabling technical piece. However, providing a large scale dataset for the design and evaluation of scene parsing algorithms, in particular for outdoor scenes, has been difficult. The per-pixel labelling process is prohibitively expensive, limiting the scale of existing ones. In this paper, we present a large-scale open dataset, {ApolloScape}, that consists of {RGB} videos and corresponding dense 3D point clouds. Comparing with existing datasets, our dataset has the following unique properties. The first is its scale, our initial release contains over 140K images-each with its per-pixel semantic mask, up to 1M is scheduled. The second is its complexity. Captured in various traffic conditions, the number of moving objects averages from tens to over one hundred (Figure 1). And the third is the 3D attribute, each image is tagged with high-accuracy pose information at cm accuracy and the static background point cloud has mm relative accuracy. We are able to label these many images by an interactive and efficient labelling pipeline that utilizes the high-quality 3D point cloud. Moreover, our dataset also contains different lane markings based on the lane colors and styles. We expect our new dataset can deeply benefit various autonomous driving related applications that include but not limited to 2D/3D scene understanding, localization, transfer learning, and driving simulation.},
	pages = {1067--1073},
	journaltitle = {{IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	author = {Huang, Xinyu and Cheng, Xinjing and Geng, Qichuan and Cao, Binbin and Zhou, Dingfu and Wang, Peng and Lin, Yuanqing and Yang, Ruigang},
	date = {2018},
	note = {{ISBN}: 9781538661000
226 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{kim_survey_2021,
	title = {A survey on deep learning based methods and datasets for monocular 3d object detection},
	volume = {10},
	issn = {20799292},
	doi = {10/gjhhrb},
	abstract = {Owing to recent advancements in deep learning methods and relevant databases, it is becoming increasingly easier to recognize 3D objects using only {RGB} images from single viewpoints. This study investigates the major breakthroughs and current progress in deep learning-based monocular 3D object detection. For relatively low-cost data acquisition systems without depth sensors or cameras at multiple viewpoints, we first consider existing databases with 2D {RGB} photos and their relevant attributes. Based on this simple sensor modality for practical applications, deep learning-based monocular 3D object detection methods that overcome significant research challenges are categorized and summarized. We present the key concepts and detailed descriptions of representative single-stage and multiple-stage detection solutions. In addition, we discuss the effectiveness of the detection models on their baseline benchmarks. Finally, we explore several directions for future research on monocular 3D object detection.},
	pages = {1--22},
	number = {4},
	journaltitle = {Electronics (Switzerland)},
	author = {Kim, Seong Heum and Hwang, Youngbae},
	date = {2021},
	keywords = {3D object detection, Deep learning, Monocular object detection},
}

@article{klischat_generating_2019,
	title = {Generating critical test scenarios for automated vehicles with evolutionary algorithms},
	volume = {2019-June},
	doi = {10/ggkdjz},
	abstract = {Virtual testing of automated vehicles using simulations is essential during their development. When it comes to the testing of motion planning algorithms, one is mainly interested in challenging, critical scenarios for which it is hard to find a feasible solution. However, these situations are rare under usual traffic conditions, demanding an automatic generation of critical test scenarios. We present an approach that automatically generates critical scenarios based on a minimization of the solution space of the vehicle under test. By formulating a scenario parametrization and automatic determination of relevant parameter intervals, we are able to optimize the criticality of complex scenarios. We use evolutionary algorithms to tackle the resulting highly nonlinear optimization problem. Compared to our previous approach, we are now able to handle complex situations, in particular those involving intersections. Finally, we demonstrate our approach by generating critical scenarios from initially uncritical scenarios.},
	pages = {2352--2358},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Klischat, Moritz and Althoff, Matthias},
	date = {2019},
	note = {{ISBN}: 9781728105604
19 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{kim_learning_2020,
	title = {Learning to simulate dynamic environments with gamegan},
	issn = {10636919},
	doi = {10/ghbb7x},
	abstract = {Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce {GameGAN}, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, {GameGAN} 'renders' the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, {GameGAN} is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist. We will release the code and trained model, enabling human players to play generated games and their variations with our {GameGAN}.},
	pages = {1228--1237},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Kim, Seung Wook and Zhou, Yuhao and Philion, Jonah and Torralba, Antonio and Fidler, Sanja},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2005.12126},
	note = {11 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{liu_harmonic_2021,
	title = {Harmonic Feature Activation for Few-Shot Semantic Segmentation},
	volume = {30},
	issn = {19410042},
	doi = {10/gh53zv},
	abstract = {Few-shot semantic segmentation remains an open problem because limited support (training) images are insufficient to represent the diverse semantics within target categories. Conventional methods typically model a target category solely using information from the support image(s), resulting in incomplete semantic activation. In this paper, we propose a novel few-shot segmentation approach, termed harmonic feature activation ({HFA}), with the aim to implement dense support-to-query semantic transform by incorporating the features of both query and support images. {HFA} is formulated as a bilinear model, which takes charge of the pixel-wise dense correlation (bilinear feature activation) between query and support images in a systematic way. {HFA} incorporates a low-rank decomposition procedure, which speeds up bilinear feature activation with negligible performance cost. In addition, a semantic diffusion procedure is fused with {HFA}, which further improves the global harmony and local consistency of the feature activation. Extensive experiments on commonly used datasets ({PASCAL} {VOC} and {MS} {COCO}) show that {HFA} improves the state-of-the-arts with significant margins. Code is available at https://github.com/Bibikiller/{HFA}.},
	pages = {3142--3153},
	journaltitle = {{IEEE} Transactions on Image Processing},
	author = {Liu, Binghao and Jiao, Jianbin and Ye, Qixiang},
	date = {2021},
	note = {1 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Bilinear Model, Few-shot Learning, Harmonic Activation, Semantic Diffusion, Semantic Segmentation},
}

@article{krivda_analysis_2020,
	title = {An analysis of traffic conflicts as a tool for sustainable road transport},
	volume = {12},
	issn = {20711050},
	doi = {10/gjjgxv},
	abstract = {This paper presents an approach to partially solving an issue within the scope of sustainable road transport, specifically the issue of potential accidents, i.e., traffic conflicts. First, a method is introduced for the analysis of traffic conflicts using video equipment. The attention is focused on traffic conflicts that occur at turbo-roundabouts. Given the diversity of causes of traffic conflicts, the emphasis is placed on the correct identification of the cause, i.e., whether the conflict is caused only by the negligence of the road user, or whether the conflict is more or less influenced by an inappropriately designed turbo-roundabout or one or more of its specific building elements (e.g., unsuitable corner radius). The next part of the article presents a selection of results that were obtained from analyses performed at about 100 turbo-roundabouts in nine European countries. Illustrative diagrams show the courses of the emergence of traffic conflicts, the causes of which are then described in detail. The conclusions from these analyses confirm the main hypothesis that the evaluation of traffic conflicts should be an essential part of designing roads, in order to increase traffic safety and, importantly, contribute to sustainable transport.},
	number = {17},
	journaltitle = {Sustainability (Switzerland)},
	author = {Krivda, Vladislav and Petru, Jan and Macha, David and Plocova, Kristyna and Fibich, David},
	date = {2020},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Road traffic, Road transport, Traffic conflict, Traffic safety, Turbo-roundabout},
}

@article{kutbi_zero-shot_2021,
	title = {Zero-shot Deep Domain Adaptation with Common Representation Learning},
	volume = {8828},
	doi = {10/gjjgxw},
	pages = {1--16},
	issue = {c},
	author = {Kutbi, Mohammed and Peng, Kuan-chuan and Wu, Ziyan},
	date = {2021},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{kriegel_efficient_2015,
	title = {Efficient next-best-scan planning for autonomous 3D surface reconstruction of unknown objects},
	volume = {10},
	issn = {18618200},
	doi = {10/f7zvq4},
	abstract = {This work focuses on autonomous surface reconstruction of small-scale objects with a robot and a 3D sensor. The aim is a high-quality surface model allowing for robotic applications such as grasping and manipulation. Our approach comprises the generation of next-best-scan ({NBS}) candidates and selection criteria, error minimization between scan patches and termination criteria. {NBS} candidates are iteratively determined by a boundary detection and surface trend estimation of the acquired model. To account for both a fast and high-quality model acquisition, that candidate is selected as {NBS}, which maximizes a utility function that integrates an exploration and a mesh-quality component. The modeling and scan planning methods are evaluated on an industrial robot with a high-precision laser striper system. While performing the new laser scan, data are integrated on-the-fly into both, a triangle mesh and a probabilistic voxel space. The efficiency of the system in fast acquisition of high-quality 3D surface models is proven with different cultural heritage, household and industrial objects.},
	pages = {611--631},
	number = {4},
	journaltitle = {Journal of Real-Time Image Processing},
	author = {Kriegel, Simon and Rink, Christian and Bodenmüller, Tim and Suppa, Michael},
	date = {2015},
	note = {92 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {3D modeling, Active vision, Laser scanning, Next-best-view planning},
}

@article{kwon_unified_2015,
	title = {A Unified Framework for Event Summarization and Rare Event Detection from Multiple Views},
	volume = {37},
	issn = {01628828},
	doi = {10/f7mtwh},
	abstract = {A novel approach for event summarization and rare event detection is proposed. Unlike conventional methods that deal with event summarization and rare event detection independently, our method solves them in a single framework by transforming them into a graph editing problem. In our approach, a video is represented by a graph, each node of which indicates an event obtained by segmenting the video spatially and temporally. The edges between nodes describe the relationship between events. Based on the degree of relations, edges have different weights. After learning the graph structure, our method finds subgraphs that represent event summarization and rare events in the video by editing the graph, that is, merging its subgraphs or pruning its edges. The graph is edited to minimize a predefined energy model with the Markov Chain Monte Carlo ({MCMC}) method. The energy model consists of several parameters that represent the causality, frequency, and significance of events. We design a specific energy model that uses these parameters to satisfy each objective of event summarization and rare event detection. The proposed method is extended to obtain event summarization and rare event detection results across multiple videos captured from multiple views. For this purpose, the proposed method independently learns and edits each graph of individual videos for event summarization or rare event detection. Then, the method matches the extracted multiple graphs to each other, and constructs a single composite graph that represents event summarization or rare events from multiple views. Experimental results show that the proposed approach accurately summarizes multiple videos in a fully unsupervised manner. Moreover, the experiments demonstrate that the approach is advantageous in detecting rare transition of events.},
	pages = {1737--1750},
	number = {9},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kwon, Junseok and Lee, Kyoung Mu},
	date = {2015},
	note = {Publisher: {IEEE}
{ISBN}: 9781467312288
28 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Event Summarization, Rare Event Detection, Video Structure Editing, Video Structure Learning, Video Structure Matching},
}

@article{kuutti_survey_2019,
	title = {A survey of deep learning applications to autonomous vehicle control},
	issn = {1524-9050},
	doi = {10/ggp4zb},
	abstract = {—Designing a controller for autonomous vehicles capable of providing adequate performance in all driving scenarios is challenging due to the highly complex environment and inability to test the system in the wide variety of scenarios which it may encounter after deployment. However, deep learning methods have shown great promise in not only providing excellent performance for complex and non-linear control problems, but also in generalising previously learned rules to new scenarios. For these reasons, the use of deep learning for vehicle control is becoming increasingly popular. Although important advancements have been achieved in this field, these works have not been fully summarised. This paper surveys a wide range of research works reported in the literature which aim to control a vehicle through deep learning methods. Although there exists overlap between control and perception, the focus of this paper is on vehicle control, rather than the wider perception problem which includes tasks such as semantic segmentation and object detection. The paper identifies the strengths and limitations of available deep learning methods through comparative analysis and discusses the research challenges in terms of computation, architecture selection, goal specification, generalisation, verification and validation, as well as safety. Overall, this survey brings timely and topical information to a rapidly evolving field relevant to intelligent transportation systems.},
	pages = {1--23},
	journaltitle = {{arXiv}},
	author = {Kuutti, Sampo and Bowden, Richard and Jin, Yaochu and Barber, Phil and Fallah, Saber},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1912.10773},
	note = {38 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Advanced driver assistance, Autonomous vehicles, Computer vision, Intelligent control, Machine learning, Neural networks},
}

@article{laxhammar_online_2014,
	title = {Online learning and sequential anomaly detection in trajectories},
	volume = {36},
	issn = {01628828},
	doi = {10/f23kck},
	abstract = {Detection of anomalous trajectories is an important problem in the surveillance domain. Various algorithms based on learning of normal trajectory patterns have been proposed for this problem. Yet, these algorithms typically suffer from one or more limitations: They are not designed for sequential analysis of incomplete trajectories or online learning based on an incrementally updated training set. Moreover, they typically involve tuning of many parameters, including ad-hoc anomaly thresholds, and may therefore suffer from overfitting and poorly-calibrated alarm rates. In this article, we propose and investigate the Sequential Hausdorff Nearest-Neighbor Conformal Anomaly Detector ({SHNN}-{CAD}) for online learning and sequential anomaly detection in trajectories. This is a parameter-light algorithm that offers a well-founded approach to the calibration of the anomaly threshold. The discords algorithm, originally proposed by Keogh et al., is another parameter-light anomaly detection algorithm that has previously been shown to have good classification performance on a wide range of time-series datasets, including trajectory data. We implement and investigate the performance of {SHNN}-{CAD} and the discords algorithm on four different labeled trajectory datasets. The results show that {SHNN}-{CAD} achieves competitive classification performance with minimum parameter tuning during unsupervised online learning and sequential anomaly detection in trajectories. © 2013 {IEEE}.},
	pages = {1158--1173},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Laxhammar, Rikard and Falkman, Goran},
	date = {2014},
	note = {Publisher: {IEEE}
70 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Anomaly detection, conformal prediction, online learning, trajectory data},
}

@article{li_automatic_2018,
	title = {Automatic unusual driving event identification for dependable self-driving},
	doi = {10/gjjgx3},
	abstract = {This paper introduces techniques to automatically detect driving corner cases from dashcam video and inertial sensors. Developing robust driver assistance and automated driving technologies requires an understanding of not just common highway and city traffic situations but also a plethora of corner cases that may be encountered in billions of miles of driving. Current approaches seek to collect such a catalog of corner cases by driving millions of miles with self-driving prototypes. In contrast, this paper introduces a low-cost yet scalable solution to collect such events from any dashcam-equipped vehicle to take advantage of the billions of miles that humans already drive. It detects unusual events through inertial sensing of sudden human driver reactions and rare visual events through a trained autoencoder deep neural network. We evaluate the system based on more than 120 hours real road driving data. It shows 82\% accuracy improvement versus strawman solutions for sudden reaction detection and above 71\% accuracy for rare visual views identification. The detection results proved useful for re-training and improving a self-steering algorithm on more complex situations. In terms of computational efficiency, the Android prototype achieves 17Hz frame rate (Nexus 5X).},
	pages = {15--27},
	journaltitle = {{SenSys} 2018 - Proceedings of the 16th Conference on Embedded Networked Sensor Systems},
	author = {Li, Hongyu and Wang, Hairong and Liu, Luyang and Gruteser, Marco},
	date = {2018},
	note = {{ISBN}: 9781450359528
10 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autoencoder, Inertial sensing, Self-Driving, Unusual Driving Identification},
}

@article{liu_edge_2019,
	title = {Edge Computing for Autonomous Driving: Opportunities and Challenges},
	volume = {107},
	issn = {00189219},
	doi = {10/ggj7qk},
	abstract = {Safety is the most important requirement for autonomous vehicles; hence, the ultimate challenge of designing an edge computing ecosystem for autonomous vehicles is to deliver enough computing power, redundancy, and security so as to guarantee the safety of autonomous vehicles. Specifically, autonomous driving systems are extremely complex; they tightly integrate many technologies, including sensing, localization, perception, decision making, as well as the smooth interactions with cloud platforms for high-definition ({HD}) map generation and data storage. These complexities impose numerous challenges for the design of autonomous driving edge computing systems. First, edge computing systems for autonomous driving need to process an enormous amount of data in real time, and often the incoming data from different sensors are highly heterogeneous. Since autonomous driving edge computing systems are mobile, they often have very strict energy consumption restrictions. Thus, it is imperative to deliver sufficient computing power with reasonable energy consumption, to guarantee the safety of autonomous vehicles, even at high speed. Second, in addition to the edge system design, vehicle-to-everything (V2X) provides redundancy for autonomous driving workloads and alleviates stringent performance and energy constraints on the edge side. With V2X, more research is required to define how vehicles cooperate with each other and the infrastructure. Last, safety cannot be guaranteed when security is compromised. Thus, protecting autonomous driving edge computing systems against attacks at different layers of the sensing and computing stack is of paramount concern. In this paper, we review state-of-the-art approaches in these areas as well as explore potential solutions to address these challenges.},
	pages = {1697--1716},
	number = {8},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Liu, Shaoshan and Liu, Liangkai and Tang, Jie and Yu, Bo and Wang, Yifan and Shi, Weisong},
	date = {2019},
	note = {56 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Connected and autonomous vehicles ({CAVs}), edge computing, heterogeneous computing, security, vehicle-to-everything (V2X), vehicular operating system.},
}

@article{lin_detecting_2010,
	title = {Detecting anomalies in unmanned vehicles using the Mahalanobis distance},
	issn = {10504729},
	doi = {10/fscm4z},
	abstract = {The use of unmanned autonomous vehicles is becoming more and more significant in recent years. The fact that the vehicles are unmanned (whether autonomous or not), can lead to greater difficulties in identifying failure and anomalous states, since the operator cannot rely on its own body perceptions to identify failures. Moreover, as the autonomy of unmanned vehicles increases, it becomes more difficult for operators to monitor them closely, and this further exacerbates the difficulty of identifying anomalous states, in a timely manner. Model-based diagnosis and fault-detection systems have been proposed to recognize failures. However, these rely on the capabilities of the underlying model, which necessarily abstracts away from the physical reality of the robot. In this paper we propose a novel, model-free, approach for detecting anomalies in unmanned autonomous vehicles, based on their sensor readings (internal and external). Experiments conducted on Unmanned Aerial Vehicles ({UAVs}) and Unmanned Ground Vehicles ({UGVs}) demonstrate the efficacy of the approach by detecting the vehicles deviations from nominal behavior. ©2010 {IEEE}.},
	pages = {3038--3044},
	journaltitle = {Proceedings - {IEEE} International Conference on Robotics and Automation},
	author = {Lin, Raz and Khalastchi, Eliyahu and Kaminka, Gal A.},
	date = {2010},
	note = {{ISBN}: 9781424450381
42 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{levinson_towards_2011,
	title = {Towards fully autonomous driving: Systems and algorithms},
	doi = {10/d23t96},
	abstract = {In order to achieve autonomous operation of a vehicle in urban situations with unpredictable traffic, several realtime systems must interoperate, including environment perception, localization, planning, and control. In addition, a robust vehicle platform with appropriate sensors, computational hardware, networking, and software infrastructure is essential. © 2011 {IEEE}.},
	pages = {163--168},
	issue = {Iv},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Levinson, Jesse and Askeland, Jake and Becker, Jan and Dolson, Jennifer and Held, David and Kammel, Soeren and Kolter, J. Zico and Langer, Dirk and Pink, Oliver and Pratt, Vaughan and Sokolsky, Michael and Stanek, Ganymed and Stavens, David and Teichman, Alex and Werling, Moritz and Thrun, Sebastian},
	date = {2011},
	note = {{ISBN}: 9781457708909
749 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{lei_dynamic_2018,
	title = {Dynamic Path Planning of Unknown Environment Based on Deep Reinforcement Learning},
	volume = {2018},
	issn = {16879619},
	doi = {10/gjjgxx},
	abstract = {Dynamic path planning of unknown environment has always been a challenge for mobile robots. In this paper, we apply double Q-network ({DDQN}) deep reinforcement learning proposed by {DeepMind} in 2016 to dynamic path planning of unknown environment. The reward and punishment function and the training method are designed for the instability of the training stage and the sparsity of the environment state space. In different training stages, we dynamically adjust the starting position and target position. With the updating of neural network and the increase of greedy rule probability, the local space searched by agent is expanded. Pygame module in {PYTHON} is used to establish dynamic environments. Considering lidar signal and local target position as the inputs, convolutional neural networks ({CNNs}) are used to generalize the environmental state. Q-learning algorithm enhances the ability of the dynamic obstacle avoidance and local planning of the agents in environment. The results show that, after training in different dynamic environments and testing in a new environment, the agent is able to reach the local target position successfully in unknown dynamic environment.},
	journaltitle = {Journal of Robotics},
	author = {Lei, Xiaoyun and Zhang, Zhian and Dong, Peifang},
	date = {2018},
	note = {27 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{li_photo-realistic_2017,
	title = {Photo-Realistic Simulation of Road Scene for Data-Driven Methods in Bad Weather},
	volume = {2018-Janua},
	doi = {10/gjjgx2},
	abstract = {Modern data-driven computer vision algorithms require a large volume, varied data for validation or evaluation. We utilize computer graphics techniques to generate a large volume foggy image dataset of road scenes with different levels of fog. We compare with other popular synthesized datasets, including data collected both from the virtual world and the real world. In addition, we benchmark recent popular dehazing methods and evaluate their performance on different datasets, which provides us an objectively comparison of their limitations and strengths. To our knowledge, this is the first foggy and hazy dataset with large volume data which can be helpful for computer vision research in the autonomous driving.},
	pages = {491--500},
	journaltitle = {Proceedings - 2017 {IEEE} International Conference on Computer Vision Workshops, {ICCVW} 2017},
	author = {Li, Kunming and Li, Yu and You, Shaodi and Barnes, Nick},
	date = {2017},
	note = {{ISBN}: 9781538610343
10 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{li_automated_2017,
	title = {Automated Driving Systems - A vision for Safety},
	volume = {37},
	issn = {02503301},
	doi = {10/gjjgxz},
	pages = {573--579},
	number = {2},
	journaltitle = {U.S. Department of Transportation},
	author = {Li, Da Ming and Zhang, Tong Qing and Tang, Sheng Kai and Duan, Cui Lan and Yang, Jun Hu and Mu, Huan and Liu, Xiao Wei},
	date = {2017},
	pmid = {27363146},
	keywords = {Cyanobacterial bloom, Lake Hongze, Lake eutrophication, Microcystis, Quantitative real-time {PCR}},
}

@article{liu_v2x-based_2016,
	title = {V2X-Based Decentralized Cooperative Adaptive Cruise Control in the Vicinity of Intersections},
	volume = {17},
	issn = {15249050},
	doi = {10/f8ds42},
	abstract = {Cooperative driving with V2X communication is widely researched due to its considerable potential to improve the safety and efficiency of road transportation systems. In this paper, a decentralized cooperative adaptive cruise control algorithm using V2X for vehicles in the vicinity of intersections ({CACC}-{VI}) is proposed. This algorithm is designed to improve the throughput of intersection by reorganizing the vehicle platoons around it, in consideration of safety, fuel consumption, speed limit, heterogeneous features of vehicles, and passenger comfort. Within a platoon, vehicles try to find the optimal control input by a distributed particle swarm optimization ({PSO}) algorithm, in order to reduce tracking errors, while respecting different constraints. A concept of opportunity space is proposed to facilitate platoon reorganization, in which a subplatoon or an individual vehicle can choose to accelerate to join in the preceding platoon or to decelerate to depart from the current one. The main idea is to make full use of the road capacity and to distribute it to most vehicles that are capable to find an accelerating trajectory to get through the intersection within a limited period. The originality of our algorithm is the introduction of a novel application of V2X communication to make the traffic more intelligent, in terms of safety, time saving, and environment friendly.},
	pages = {644--658},
	number = {3},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	author = {Liu, Bing and El Kamel, Abdelkader},
	date = {2016},
	note = {46 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {{PSO} with constraints, V2X communication, cooperative adaptive cruise control ({CACC}), eco-driving},
}

@article{liu_large-scale_2019,
	title = {Large-scale long-tailed recognition in an open world},
	volume = {2019-June},
	issn = {10636919},
	doi = {10/ghhdjg},
	abstract = {Real world data often have a long-tailed and open-ended distribution. A practical recognition system must classify among majority and minority classes, generalize from a few known instances, and acknowledge novelty upon a never seen instance. We define Open Long-Tailed Recognition ({OLTR}) as learning from such naturally distributed data and optimizing the classification accuracy over a balanced test set which include head, tail, and open classes. {OLTR} must handle imbalanced classification, few-shot learning, and open-set recognition in one integrated algorithm, whereas existing classification approaches focus only on one aspect and deliver poorly over the entire class spectrum. The key challenges are how to share visual knowledge between head and tail classes and how to reduce confusion between tail and open classes. We develop an integrated {OLTR} algorithm that maps an image to a feature space such that visual concepts can easily relate to each other based on a learned metric that respects the closed-world classification while acknowledging the novelty of the open world. Our so-called dynamic meta-embedding combines a direct image feature and an associated memory feature, with the feature norm indicating the familiarity to known classes. On three large-scale {OLTR} datasets we curate from object-centric {ImageNet}, scene-centric Places, and face-centric {MS}1M data, our method consistently outperforms the state-of-the-art. Our code, datasets, and models enable future {OLTR} research and are publicly available at url\{https://liuziwei7.github.io/projects/{LongTail}.html.},
	pages = {2532--2541},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Liu, Ziwei and Miao, Zhongqi and Zhan, Xiaohang and Wang, Jiayun and Gong, Boqing and Yu, Stella X.},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1904.05160},
	note = {{ISBN}: 9781728132938
117 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Categorization, Recognition: Detection, Representation Learning, Retrieval, ★},
}

@article{loke_cooperative_2019,
	title = {Cooperative Automated Vehicles: A Review of Opportunities and Challenges in Socially Intelligent Vehicles beyond Networking},
	volume = {4},
	issn = {23798858},
	doi = {10/gjjgx9},
	abstract = {The connected automated vehicle has been often touted as a technology that will become pervasive in society in the near future. One can view an automated vehicle as having Artificial Intelligence ({AI}) capabilities, being able to self-drive, sense its surroundings, recognise objects in its vicinity, and perform reasoning and decision-making. Rather than being stand alone, we examine the need for automated vehicles to cooperate and interact within their socio-cyber-physical environments, including the problems cooperation will solve, but also the issues and challenges. We review current work in cooperation for automated vehicles, based on selected examples from the literature. We conclude noting the need for the ability to behave cooperatively as a form of social-{AI} capability for automated vehicles, beyond sensing the immediate environment and beyond the underlying networking technology.},
	pages = {509--518},
	number = {4},
	journaltitle = {{IEEE} Transactions on Intelligent Vehicles},
	author = {Loke, Seng W.},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1710.00461},
	note = {4 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Automated vehicles, connected vehicles, cooperative vehicles, multiagent systems},
}

@article{liu_safety_2020,
	title = {Safety analysis of a modified cooperative adaptive cruise control algorithm accounting for communication delay},
	volume = {12},
	issn = {20711050},
	doi = {10/gjjgx4},
	abstract = {Cooperative adaptive cruise control ({CACC}) is a promising technology to improve traffic efficiency and enhance road safety. In this paper, a modified {CACC} control model considering the communication time delay is proposed, which is used to investigate the longitudinal safety impacts of the communication time delay to the {CACC} platoon. Then, the communication time delay model is integrated into the {CACC} model to simulate the realistic information transfer process in the {CACC} platoon. Then a microscopic {CACC} platoon simulation is designed and conducted to verify the feasibility and reliability of the modified {CACC} control algorithm. The obtained results reveal that the modified {CACC} control algorithm can not only reduce about 96.6\% of inter-vehicle spacing error, but also enhance the vehicles' ability to sense the upstream traffic changes. Furthermore, to quantitatively analyze the longitudinal safety influence of the time delay caused by representative communication systems, sensitivity analysis experiments of headway time were designed and conducted. In the sensitivity analysis, the time exposed time-to-collision ({TET}) and the time-integrated time-to-collision ({TIT}) were introduced as the key performance indicators ({KPIs}) to quantify the rear-end collision risks. Sensitivity analysis results demonstrate that the performance of the {CACC} platoon is strictly related to the applied wireless communication style. Furthermore, the {CACC} system supported by the 5th generation (5G) communication system shows great advantages in narrowing the minimal headway time gap and reducing the rear-end collision risks.},
	number = {18},
	journaltitle = {Sustainability (Switzerland)},
	author = {Liu, Yi and Wang, Wei and Hua, Xuedong and Wang, Shunchao},
	date = {2020},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {5G, Communication latency, Cooperative adaptive cruise control, Rear-end collision risks},
}

@article{lopez_categorization_2017,
	title = {Categorization of anomalies in smart manufacturing systems to support the selection of detection mechanisms},
	volume = {2},
	issn = {23773766},
	doi = {10/gfx4rj},
	abstract = {An important issue in anomaly detection in smart manufacturing systems is the lack of consistency in the formal definitions of anomalies, faults, and attacks. The term anomaly is used to cover a wide range of situations that are addressed by different types of solutions. In this letter, we categorize anomalies in machines, controllers, and networks along with their detection mechanisms, and unify them under a common framework to aid in the identification of potential solutions. The main contribution of the proposed categorization is that it allows the identification of gaps in anomaly detection in smart manufacturing systems.},
	pages = {1885--1892},
	number = {4},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	author = {Lopez, Felipe and Saez, Miguel and Shao, Yuru and Balta, Efe C. and Moyne, James and Mao, Z. Morley and Barton, Kira and Tilbury, Dawn},
	date = {2017},
	note = {22 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Factory automation, Intelligent and flexible manufacturing},
}

@article{liu_od-gcn_2020,
	title = {{OD}-{GCN}: Object detection boosted by knowledge {GCN}},
	doi = {10/gjjgx5},
	abstract = {Classical {CNN} based object detection methods only extract the objects' image features, but do not consider the high-level relationship among objects in context. In this article, the graph convolutional networks ({GCN}) is integrated into the object detection framework to exploit the benefit of category relationship among objects, which is able to provide extra confidence for any pre-trained object detection model in our framework. In experiments, we test several popular base detection models on {COCO} dataset. The results show promising improvement on {mAP} by 1{\textasciitilde}5pp. In addition, visualized analysis reveals the benchmark improvement is quite reasonable in human's opinion.},
	pages = {1--6},
	journaltitle = {2020 {IEEE} International Conference on Multimedia and Expo Workshops, {ICMEW} 2020},
	author = {Liu, Zheng and Jiang, Zidong and Feng, Wei and Feng, Hui},
	date = {2020},
	note = {{ISBN}: 9781728114859
2 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Graph convolutional network, Knowledge graph, Object detection},
}

@article{mahajan_prediction_2020,
	title = {Prediction of Lane-Changing Maneuvers with Automatic Labeling and Deep Learning},
	volume = {2674},
	issn = {21694052},
	doi = {10/gjjgx6},
	abstract = {Highway safety has attracted significant research interest in recent years, especially as innovative technologies such as connected and autonomous vehicles ({CAVs}) are fast becoming a reality. Identification and prediction of driving intention are fundamental for avoiding collisions as it can provide useful information to drivers and vehicles in their vicinity. However, the state-of-the-art in maneuver prediction requires the utilization of large labeled datasets, which demand a significant amount of processing and might hinder real-time applications. In this paper, an end-to-end machine learning model for predicting lane-change maneuvers from unlabeled data using a limited number of features is developed and presented. The model is built on a novel comprehensive dataset (i.e., {highD}) obtained from German highways with camera-equipped drones. Density-based clustering is used to identify lane-changing and lane-keeping maneuvers and a support vector machine ({SVM}) model is then trained to learn the boundaries of the clustered labels and automatically label the new raw data. The labeled data are then input to a long short-term memory ({LSTM}) model which is used to predict maneuver class. The classification results show that lane changes can efficiently be predicted in real-time, with an average detection time of at least 3 s with a small percentage of false alarms. The utilization of unlabeled data and vehicle characteristics as features increases the prospects of transferability of the approach and its practical application for highway safety.},
	pages = {336--347},
	number = {7},
	journaltitle = {Transportation Research Record},
	author = {Mahajan, Vishal and Katrakazas, Christos and Antoniou, Constantinos},
	date = {2020},
	note = {2 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{makantasis_deep_2020,
	title = {Deep reinforcement-learning-based driving policy for autonomous road vehicles},
	volume = {14},
	issn = {1751956X},
	doi = {10/gjjgzd},
	abstract = {In this work, the problem of path planning for an autonomous vehicle that moves on a freeway is considered. The most common approaches that are used to address this problem are based on optimal control methods, which make assumptions about the model of the environment and the system dynamics. On the contrary, this work proposes the development of a driving policy based on reinforcement learning. In this way, the proposed driving policy makes minimal or no assumptions about the environment, since a priori knowledge about the system dynamics is not required. Driving scenarios where the road is occupied both by autonomous and manual driving vehicles are considered. To the best of the authors' knowledge, this is one of the first approaches that propose a reinforcement learning driving policy for mixed driving environments. The derived reinforcement learning policy, firstly, is compared against an optimal policy derived via dynamic programming, and, secondly, its efficiency is evaluated under realistic scenarios generated by the established {SUMO} microscopic traffic flow simulator. Finally, some initial results regarding the effect of autonomous vehicles' behaviour on the overall traffic flow are presented.},
	pages = {13--24},
	number = {1},
	journaltitle = {{IET} Intelligent Transport Systems},
	author = {Makantasis, Konstantinos and Kontorinaki, Maria and Nikolos, Ioannis},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1907.05246},
	note = {4 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{m_g_considering_2016,
	title = {Considering knowledge gaps for automated driving in conventional traffic},
	doi = {10/gjjgx7},
	abstract = {With increasing numbers of vehicles using low level automation and higher level automation expected in the future, significant effects are expected on traffic flow. Despite much simulation and driving simulator research on {SAE} level 1 vehicles, there remain many questions in regard to the effects of the systems on traffic flow. The effects of higher levels of automation are even more difficult to estimate, as these vehicles are not even present on roads at this time, let alone in sufficient numbers to analyze. In this research, we propose a methodology for a-priori analysis of potential conflict situations: Method for Explorative {TRaffic} scenario Observation and Analysis ({METRO}-A). It is applied to the case of automated driving in conventional traffic to analyze potential difficulties that {SAE} level 3 and 4 and higher vehicles may encounter in mixed traffic conditions, for a weaving section case-study. Furthermore, a set of important research questions are constructed that are relevant for the automotive industry, and road agencies and authorities.},
	pages = {102--111},
	author = {M G, {AROEN} and {VAN}, {BART} and R, {ISABEL} and C, {SIMEON}},
	date = {2016},
	note = {5 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {automated},
}

@article{masotti_deep_2018,
	title = {Deep Learning for Automatic Image Captioning in Poor Training Conditions},
	volume = {4},
	issn = {2499-4553},
	doi = {10/gjjgzc},
	pages = {43--55},
	number = {1},
	journaltitle = {Italian Journal of Computational Linguistics},
	author = {Masotti, Caterina and Croce, Danilo and Basili, Roberto},
	date = {2018},
}

@article{martinez-gonzalez_unrealrox_2020,
	title = {{UnrealROX}: an extremely photorealistic virtual reality environment for robotics simulations and synthetic data generation},
	volume = {24},
	issn = {14349957},
	url = {https://doi.org/10.1007/s10055-019-00399-5},
	doi = {10/gjjgx8},
	abstract = {Data-driven algorithms have surpassed traditional techniques in almost every aspect in robotic vision problems. Such algorithms need vast amounts of quality data to be able to work properly after their training process. Gathering and annotating that sheer amount of data in the real world is a time-consuming and error-prone task. These problems limit scale and quality. Synthetic data generation has become increasingly popular since it is faster to generate and automatic to annotate. However, most of the current datasets and environments lack realism, interactions, and details from the real world. {UnrealROX} is an environment built over Unreal Engine 4 which aims to reduce that reality gap by leveraging hyperrealistic indoor scenes that are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline to generate raw data and ground truth annotations. This virtual reality environment enables robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation.},
	pages = {271--288},
	number = {2},
	journaltitle = {Virtual Reality},
	author = {Martinez-Gonzalez, Pablo and Oprea, Sergiu and Garcia-Garcia, Alberto and Jover-Alvarez, Alvaro and Orts-Escolano, Sergio and Garcia-Rodriguez, Jose},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1810.06936},
	note = {Publisher: Springer London
{ISBN}: 0123456789
20 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Grasping, Robotics, Synthetic data},
}

@article{mallozzi_autonomous_2019,
	title = {Autonomous Vehicles: State of the Art, Future Trends, and Challenges},
	doi = {10/ghx95q},
	abstract = {Autonomous vehicles are considered to be the next big thing. Several companies are racing to put self-driving vehicles on the road by 2020. Regulations and standards are not ready for such a change. New technologies, such as the intensive use of machine learning, are bringing new solutions but also opening new challenges. This chapter reports the state of the art, future trends, and challenges of autonomous vehicles, with a special focus on software. One of the major challenges we further elaborate on is using machine learning techniques in order to deal with uncertainties that characterize the environments in which autonomous vehicles will need to operate while guaranteeing safety properties.},
	pages = {347--367},
	journaltitle = {Automotive Systems and Software Engineering},
	author = {Mallozzi, Piergiuseppe and Pelliccione, Patrizio and Knauss, Alessia and Berger, Christian and Mohammadiha, Nassar},
	date = {2019},
	note = {{ISBN}: 9783030121570
8 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{melchior_particle_2007,
	title = {Particle {RRT} for path planning with uncertainty},
	issn = {10504729},
	doi = {10/b7cvzj},
	abstract = {This paper describes a new extension to the Rapidly-exploring Random Tree ({RRT}) path planning algorithm. The Particle {RRT} algorithm explicitly considers uncertainty in its domain, similar to the operation of a particle filter. Each extension to the search tree is treated as a stochastic process and is simulated multiple times. The behavior of the robot can be characterized based on the specified uncertainty in the environment, and guarantees can be made as to the performance under this uncertainty. Extensions to the search tree, and therefore entire paths, may be chosen based on the expected probability of successful execution. The benefit of this algorithm is demonstrated in the simulation of a rover operating in rough terrain with unknown coefficients of friction. © 2007 {IEEE}.},
	pages = {1617--1624},
	journaltitle = {Proceedings - {IEEE} International Conference on Robotics and Automation},
	author = {Melchior, Nik A. and Simmons, Reid},
	date = {2007},
	note = {{ISBN}: 1424406021
215 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{mavromatis_multi-radio_2018,
	title = {Multi-radio 5g architecture for connected and autonomous vehicles: Application and design insights},
	issn = {2410-0218},
	doi = {10/gjjgzb},
	abstract = {Connected and Autonomous Vehicles ({CAVs}) will play a crucial role in next-generation Cooperative Intelligent Transportation Systems (C-{ITSs}). Not only is the information exchange fundamental to improve road safety and efficiency, but it also paves the way to a wide spectrum of advanced {ITS} applications enhancing efficiency, mobility and accessibility. Highly dynamic network topologies and unpredictable wireless channel conditions entail numerous design challenges and open questions. In this paper, we address the beneficial interactions between {CAVs} and an {ITS} and propose a novel architecture design paradigm. Our solution can accommodate multi-layer applications over multiple Radio Access Technologies ({RATs}) and provide a smart configuration interface for enhancing the performance of each {RAT}.},
	pages = {1--10},
	journaltitle = {{arXiv}},
	author = {Mavromatis, Ioannis and Tassi, Andrea and Rigazzi, Giovanni and Piechocki, Robert J. and Nix, Andrew},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1801.09510},
	note = {19 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {C-{ITS}, {CAV}, Connected and Autonomous Vehicles, {DSRC}, Fog Computing, {LTE}-A, Layered Services, {MmWave}, V2X},
}

@article{millais_exploring_2018,
	title = {Exploring data in virtual reality: Comparisons with 2d data visualizations},
	volume = {2018-April},
	doi = {10/ggfwpw},
	abstract = {Virtual Reality ({VR}) has often been discussed as a promising medium for immersive data visualization and exploration. However, few studies have evaluated users’ open-ended exploration of multi-dimensional datasets using {VR} and compared the results with that of traditional (2D) visualizations. Using a workload- and insight-based evaluation methodology, we conducted a user study to perform such a comparison. We find that there is no overall task-workload dierence between traditional visualizations and visualizations in {VR}, but there are dierences in the accuracy and depth of insights that users gain. Our results also suggest that users feel more satisfied and successful when using {VR} data exploration tools, thus demonstrating the potential of {VR} as an engaging medium for visual data analytics.},
	pages = {5--10},
	journaltitle = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Millais, Patrick and Jones, Simon L. and Kelly, Ryan},
	date = {2018},
	note = {{ISBN}: 9781450356206
20 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Data Dashboards, Data Visualization, Virtual Reality},
}

@article{mejbaul_islam_unsupervised_2020,
	title = {Unsupervised abnormality detection using heterogenous autonomous system},
	volume = {2020-Novem},
	issn = {21593450},
	doi = {10/gjjgzg},
	abstract = {Due to the rise of autonomous vehicles like drones and cars anomaly detection for better and robust surveillance becomes prominent for real-time recognition of normal and abnormal states. But the whole system fails if the unmanned device is unable to detect its own device's anomaly in real-time. Considering the scenario, we can make use of various data of autonomous vehicles like images, video streams, and other digital or analog sensor data to detect device anomaly. In this paper, we have demonstrated a heterogeneous system that estimates the degree of an anomaly in unmanned surveillance drone by inspecting {IMU} (Inertial Measurement Unit) sensor data and real-time image in an unsupervised approach. We've used {AngleNet} for detecting images taken in an abnormal state. On top of that, an autoencoder fed by the {IMU} data has been ensembled with {AngleNet} for evaluating the final degree of the anomaly. This proposed method is based on the result of the {IEEE} {SP} Cup 2020 which achieved 97.3 percent accuracy on the provided dataset. Besides, this approach has been evaluated on an in-house setup for substantiating its robustness.},
	pages = {761--766},
	issue = {July},
	journaltitle = {{IEEE} Region 10 Annual International Conference, Proceedings/{TENCON}},
	author = {Mejbaul Islam, Kazi and Noor, Rouhan and Shafayet Chowdhury, Sayeed and Tahiat Ohi, Tafannum and Redwan Islam, Mohammad and Kumer Roy, Chinmoy and Sakib, Nazmus},
	date = {2020},
	note = {{ISBN}: 9781728184555
2 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {{AngleNet}, Auto-encoder, Drone image, {IMU}, Unsupervised anomaly detection},
}

@article{mcallister_concrete_2017,
	title = {Concrete problems for autonomous vehicle safety: Advantages of Bayesian deep learning},
	volume = {0},
	issn = {10450823},
	doi = {10/ggb47m},
	abstract = {Autonomous vehicle ({AV}) software is typically composed of a pipeline of individual components, linking sensor inputs to motor outputs. Erroneous component outputs propagate downstream, hence safe {AV} software must consider the ultimate effect of each component's errors. Further, improving safety alone is not sufficient. Passengers must also feel safe to trust and use {AV} systems. To address such concerns, we investigate three under-explored themes for {AV} research: safety, interpretability, and compliance. Safety can be improved by quantifying the uncertainties of component outputs and propagating them forward through the pipeline. Interpretability is concerned with explaining what the {AV} observes and why it makes the decisions it does, building reassurance with the passenger. Compliance refers to maintaining some control for the passenger. We discuss open challenges for research within these themes. We highlight the need for concrete evaluation metrics, propose example problems, and highlight possible solutions.},
	pages = {4745--4753},
	journaltitle = {{IJCAI} International Joint Conference on Artificial Intelligence},
	author = {{McAllister}, Rowan and Gal, Yarin and Kendall, Alex and Van Der Wilk, Mark and Shah, Amar and Cipolla, Roberto and Weller, Adrian},
	date = {2017},
	note = {{ISBN}: 9780999241103
114 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Social: Challenges, Social: Human-machine Interaction, Technical: Models, Technical: Techniques},
}

@article{mccomas_effectiveness_2002,
	title = {Effectiveness of virtual reality for teaching pedestrian safety},
	volume = {5},
	issn = {10949313},
	doi = {10/bzhshn},
	abstract = {Sixty percent to 70\% of pedestrian injuries in children under the age of 10 years are the result of the child either improperly crossing intersections or dashing out in the street between intersections. The purpose of this injury prevention research study was to evaluate a desktop virtual reality ({VR}) program that was designed to educate and train children to safely cross intersections. Specifically, the objectives were to determine whether children can learn pedestrian safety skills while working in a virtual environment and whether pedestrian safety learning in {VR} transfers to real world behavior. Following focus groups with a number of key experts, a virtual city with eight interactive intersections was developed. Ninety-five children participated in a community trial from two schools (urban and suburban). Approximately half were assigned to a control group who received an unrelated {VR} program, and half received the pedestrian safety {VR} intervention. Children were identified by group and grade by colored tags on their backpacks, and actual street crossing behavior of all children was observed 1 week before and 1 week after the interventions. There was a significant change in performance after three trials with the {VR} intervention. Children learned safe street crossing within the virtual environment. Learning, identified as improved street-crossing behavior, transferred to real world behavior in the suburban school children but not in the urban school. The results are discussed in relation to possibilities for future {VR} interventions for injury prevention.},
	pages = {185--190},
	number = {3},
	journaltitle = {Cyberpsychology and Behavior},
	author = {{McComas}, Joan and {MacKay}, Morag and Pivik, Jayne},
	date = {2002},
	pmid = {12123238},
	note = {150 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{minoda_viode_2021,
	title = {{VIODE}: A Simulated Dataset to Address the Challenges of Visual-Inertial Odometry in Dynamic Environments},
	issn = {23773766},
	doi = {10/gjjgzf},
	abstract = {Dynamic environments such as urban areas are still challenging for popular visual-inertial odometry ({VIO}) algorithms. Existing datasets typically fail to capture the dynamic nature of these environments, therefore making it difficult to quantitatively evaluate the robustness of existing {VIO} methods. To address this issue, we propose three contributions: firstly, we provide the {VIODE} benchmark, a novel dataset recorded from a simulated {UAV} that navigates in challenging dynamic environments. The unique feature of the {VIODE} dataset is the systematic introduction of moving objects into the scenes. It includes three environments, each of which is available in four dynamic levels that progressively add moving objects. The dataset contains synchronized stereo images and {IMU} data, as well as ground-truth trajectories and instance segmentation masks. Secondly, we compare state-of-the-art {VIO} algorithms on the {VIODE} dataset and show that they display substantial performance degradation in highly dynamic scenes. Thirdly, we propose a simple extension for visual localization algorithms that relies on semantic information. Our results show that scene semantics are an effective way to mitigate the adverse effects of dynamic objects on {VIO} algorithms. Finally, we make the {VIODE} dataset publicly available at https://github.com/kminoda/{VIODE}.},
	pages = {1--1},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	author = {Minoda, Koji and Schilling, Fabian and Wueest, Valentin and Floreano, Dario and Yairi, Takehisa},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.05965},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{mora_mind_2020,
	title = {Mind the gap: Developments in autonomous driving research and the sustainability challenge},
	volume = {275},
	issn = {09596526},
	url = {https://doi.org/10.1016/j.jclepro.2020.124087},
	doi = {10/gjjgzh},
	abstract = {Scientific knowledge on autonomous-driving technology is expanding at a faster-than-ever pace. As a result, the likelihood of incurring information overload is particularly notable for researchers, who can struggle to overcome the gap between information processing requirements and information processing capacity. We address this issue by adopting a multi-granulation approach to latent knowledge discovery and synthesis in large-scale research domains. The proposed methodology combines citation-based community detection methods and topic modelling techniques to give a concise but comprehensive overview of how the autonomous vehicle ({AV}) research field is conceptually structured. Thirteen core thematic areas are extracted and presented by mining the large data-rich environments resulting from 50 years of {AV} research. The analysis demonstrates that this research field is strongly oriented towards examining the technological developments needed to enable the widespread rollout of {AVs}, whereas it largely overlooks the wide-ranging sustainability implications of this sociotechnical transition. On account of these findings, we call for a broader engagement of {AV} researchers with the sustainability concept and we invite them to increase their commitment to conducting systematic investigations into the sustainability of {AV} deployment. Sustainability research is urgently required to produce an evidence-based understanding of what new sociotechnical arrangements are needed to ensure that the systemic technological change introduced by {AV}-based transport systems can fulfill societal functions while meeting the urgent need for more sustainable transport solutions.},
	pages = {124087},
	issue = {January},
	journaltitle = {Journal of Cleaner Production},
	author = {Mora, Luca and Wu, Xinyi and Panori, Anastasia},
	date = {2020},
	note = {Publisher: Elsevier Ltd
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous vehicle, Knowledge gap, Research developments, Sustainability, Text mining, Topic modelling},
}

@article{philion_fastdraw_2019,
	title = {{FastDraw}: Addressing the long tail of lane detection by adapting a sequential prediction network},
	volume = {2019-June},
	issn = {23318422},
	doi = {10/gjjgzr},
	abstract = {The search for predictive models that generalize to the long tail of sensor inputs is the central difficulty when developing data-driven models for autonomous vehicles. In this paper, we use lane detection to study modeling and training techniques that yield better performance on real world test drives. On the modeling side, we introduce a novel fully convolutional model of lane detection that learns to decode lane structures instead of delegating structure inference to post-processing. In contrast to previous works, our convolutional decoder is able to represent an arbitrary number of lanes per image, preserves the polyline representation of lanes without reducing lanes to polynomials, and draws lanes iteratively without requiring the computational and temporal complexity of recurrent neural networks. Because our model includes an estimate of the joint distribution of neighboring pixels belonging to the same lane, our formulation includes a natural and computationally cheap definition of uncertainty. On the training side, we demonstrate a simple yet effective approach to adapt the model to new environments using unsupervised style transfer. By training {FastDraw} to make predictions of lane structure that are invariant to low-level stylistic differences between images, we achieve strong performance at test time in weather and lighting conditions that deviate substantially from those of the annotated datasets that are publicly available. We quantitatively evaluate our approach on the {CVPR} 2017 Tusimple lane marking challenge, difficult {CULane} datasets [29], and a small labeled dataset of our own and achieve competitive accuracy while running at 90 {FPS}.},
	pages = {11574--11583},
	journaltitle = {{arXiv}},
	author = {Philion, Jonah},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1905.04354},
	note = {{ISBN}: 9781728132938
17 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Categorization, Deep Learning, Recognition: Detection, Retrieval},
}

@article{molina-masegosa_lte-v_2017,
	title = {{LTE}-V for Sidelink 5G V2X Vehicular Communications: A New 5G Technology for Short-Range Vehicle-to-Everything Communications},
	volume = {12},
	issn = {15566072},
	doi = {10/gdgv6m},
	abstract = {This article provides an overview of the long-term evolution-vehicle ({LTE}-V) standard supporting sidelink or vehicle-to-vehicle (V2V) communications using {LTE}'s direct interface named {PC}5 in {LTE}. We review the physical layer changes introduced under Release 14 for {LTE}-V, its communication modes 3 and 4, and the {LTE}-V evolutions under discussion in Release 15 to support fifth-generation (5G) vehicle-to-everything (V2X) communications and autonomous vehicles' applications. Modes 3 and 4 support direct V2V communications but differ on how they allocate the radio resources. Resources are allocated by the cellular network under mode 3. Mode 4 does not require cellular coverage, and vehicles autonomously select their radio resources using a distributed scheduling scheme supported by congestion control mechanisms. Mode 4 is considered the baseline mode and represents an alternative to 802.11p or dedicated shortrange communications ({DSRC}). In this context, this article also presents a detailed analysis of the performance of {LTE}-V sidelink mode 4, and proposes a modification to its distributed scheduling.},
	pages = {30--39},
	number = {4},
	journaltitle = {{IEEE} Vehicular Technology Magazine},
	author = {Molina-Masegosa, Rafael and Gozalvez, Javier},
	date = {2017},
	note = {308 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{muralidhar_incorporating_2019,
	title = {Incorporating Prior Domain Knowledge into Deep Neural Networks},
	volume = {1},
	doi = {10/ghcd58},
	abstract = {In recent years, the large amount of labeled data available has also helped tend research toward using minimal domain knowledge, e.g., in deep neural network research. However, in many situations, data is limited and of poor quality. Can domain knowledge be useful in such a setting? In this paper, we propose domain adapted neural networks ({DANN}) to explore how domain knowledge can be integrated into model training for deep networks. In particular, we incorporate loss terms for knowledge available as monotonicity constraints and approximation constraints. We evaluate our model on both synthetic data generated using the popular Bohachevsky function and a real-world dataset for predicting oxygen solubility in water. In both situations, we find that our {DANN} model outperforms its domain-agnostic counterpart yielding an overall mean performance improvement of 19.5\% with a worst- and best-case performance improvement of 4\% and 42.7\%, respectively.},
	pages = {36--45},
	number = {1},
	journaltitle = {Proceedings - 2018 {IEEE} International Conference on Big Data, Big Data 2018},
	author = {Muralidhar, Nikhil and Islam, Mohammad Raihanul and Marwah, Manish and Karpatne, Anuj and Ramakrishnan, Naren},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {2103.00180},
	note = {{ISBN}: 9781538650356
26 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {-noisy data, Deep Learning, Domain Knowledge, Limited Training Data, Neural Networks, Noisy Data, adapted neural networks, advantages of hybrid models, as opposed to using, dann, deep learning, domain knowledge, figure 1, like domain, limited training data, neural networks},
}
@article{muller_sim4cv_2018,
	title = {Sim4CV: A Photo-Realistic Simulator for Computer Vision Applications},
	volume = {126},
	issn = {15731405},
	url = {https://doi.org/10.1007/s11263-018-1073-7},
	doi = {10/gd4h33},
	abstract = {We present a photo-realistic training and evaluation simulator (Sim4CV) (http://www.sim4cv.org) with extensive applications across various fields of computer vision. Built on top of the Unreal Engine, the simulator integrates full featured physics based cars, unmanned aerial vehicles ({UAVs}), and animated human actors in diverse urban and suburban 3D environments. We demonstrate the versatility of the simulator with two case studies: autonomous {UAV}-based tracking of moving objects and autonomous driving using supervised learning. The simulator fully integrates both several state-of-the-art tracking algorithms with a benchmark evaluation tool and a deep neural network architecture for training vehicles to drive autonomously. It generates synthetic photo-realistic datasets with automatic ground truth annotations to easily extend existing real-world datasets and provides extensive synthetic data variety through its ability to reconfigure synthetic worlds on the fly using an automatic world generation tool.},
	pages = {902--919},
	number = {9},
	journaltitle = {International Journal of Computer Vision},
	author = {Müller, Matthias and Casser, Vincent and Lahoud, Jean and Smith, Neil and Ghanem, Bernard},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1708.05869},
	note = {Publisher: Springer {US}
73 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous driving, Deep learning, Imitation learning, Object tracking, Simulator, Unreal Engine 4},
}

@article{mozaffari_deep_2019,
	title = {Deep learning-based vehicle behaviour prediction for autonomous driving applications: a review},
	issn = {23318422},
	doi = {10/gh6zwq},
	abstract = {—Behaviour prediction function of an autonomous vehicle predicts the future states of the nearby vehicles based on the current and past observations of the surrounding environment. This helps enhance their awareness of the imminent hazards. However, conventional behaviour prediction solutions are applicable in simple driving scenarios that require short prediction horizons. Most recently, deep learning-based approaches have become popular due to their superior performance in more complex environments compared to the conventional approaches. Motivated by this increased popularity, we provide a comprehensive review of the state-of-the-art of deep learning-based approaches for vehicle behaviour prediction in this paper. We firstly give an overview of the generic problem of vehicle behaviour prediction and discuss its challenges, followed by classification and review of the most recent deep learning-based solutions based on three criteria: input representation, output type, and prediction method. The paper also discusses the performance of several well-known solutions, identifies the research gaps in the literature and outlines potential new research directions.},
	pages = {1--15},
	journaltitle = {{arXiv}},
	author = {Mozaffari, Sajjad and Al-Jarrah, Omar Y. and Dianati, Mehrdad and Jennings, Paul and Mouzakitis, Alexandros},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1912.11676},
	note = {19 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous vehicles, Deep learning, Intelligent vehicles, Machine learning, Trajectory prediction, Vehicle behaviour prediction},
}

@article{schlechtriemen_lane_2014,
	title = {A lane change detection approach using feature ranking with maximized predictive power},
	doi = {10/gjjgz3},
	abstract = {Risk estimation for the current traffic situation is crucial for safe autonomous driving systems. One part of the uncertainty in risk estimation is the behavior of the surrounding traffic participants. In this paper we focus on highway scenarios, where possible behaviors consist of a change in acceleration and lane change maneuvers. We present a novel approach for the recognition of lane change intentions of traffic participants. Our novel approach is an extension of the Naïve Bayesian approach and results in a generative model. It builds on the relations to the directly surrounding vehicles and to the static traffic environment. We obtain the conditional probabilities of all relevant features using Gaussian mixtures with a flexible number of components. We systematically reduce the number of features by selecting the most powerful ones. Furthermore we investigate the predictive power of each feature with respect to the time before a lane change event. In a large scale experiment on real world data with over 160.781 samples collected on a test drive of 1100km we trained and validated our intention prediction model and achieved a significant improvement in the recognition performance of lane change intentions compared to current state of the art methods. © 2014 {IEEE}.},
	pages = {108--114},
	issue = {Iv},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Schlechtriemen, Julian and Wedel, Andreas and Hillenbrand, Joerg and Breuel, Gabi and Kuhnert, Klaus Dieter},
	date = {2014},
	note = {{ISBN}: 9781479936380
87 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{munir_deepant_2019,
	title = {{DeepAnT}: A Deep Learning Approach for Unsupervised Anomaly Detection in Time Series},
	volume = {7},
	issn = {21693536},
	doi = {10/ggxr4m},
	abstract = {Traditional distance and density-based anomaly detection techniques are unable to detect periodic and seasonality related point anomalies which occur commonly in streaming data, leaving a big gap in time series anomaly detection in the current era of the {IoT}. To address this problem, we present a novel deep learning-based anomaly detection approach ({DeepAnT}) for time series data, which is equally applicable to the non-streaming cases. {DeepAnT} is capable of detecting a wide range of anomalies, i.e., point anomalies, contextual anomalies, and discords in time series data. In contrast to the anomaly detection methods where anomalies are learned, {DeepAnT} uses unlabeled data to capture and learn the data distribution that is used to forecast the normal behavior of a time series. {DeepAnT} consists of two modules: time series predictor and anomaly detector. The time series predictor module uses deep convolutional neural network ({CNN}) to predict the next time stamp on the defined horizon. This module takes a window of time series (used as a context) and attempts to predict the next time stamp. The predicted value is then passed to the anomaly detector module, which is responsible for tagging the corresponding time stamp as normal or abnormal. {DeepAnT} can be trained even without removing the anomalies from the given data set. Generally, in deep learning-based approaches, a lot of data are required to train a model. Whereas in {DeepAnT}, a model can be trained on relatively small data set while achieving good generalization capabilities due to the effective parameter sharing of the {CNN}. As the anomaly detection in {DeepAnT} is unsupervised, it does not rely on anomaly labels at the time of model generation. Therefore, this approach can be directly applied to real-life scenarios where it is practically impossible to label a big stream of data coming from heterogeneous sensors comprising of both normal as well as anomalous points. We have performed a detailed evaluation of 15 algorithms on 10 anomaly detection benchmarks, which contain a total of 433 real and synthetic time series. Experiments show that {DeepAnT} outperforms the state-of-the-art anomaly detection methods in most of the cases, while performing on par with others.},
	pages = {1991--2005},
	journaltitle = {{IEEE} Access},
	author = {Munir, Mohsin and Siddiqui, Shoaib Ahmed and Dengel, Andreas and Ahmed, Sheraz},
	date = {2019},
	note = {Publisher: {IEEE}
61 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Anomaly detection, artificial intelligence, convolutional neural network, deep neural networks, recurrent neural networks, time series analysis},
}

@article{murray-bruce_estimating_2015,
	title = {Estimating localized sources of diffusion fields using spatiotemporal sensor measurements},
	volume = {63},
	issn = {1053587X},
	doi = {10/gjjgzj},
	abstract = {We consider diffusion fields induced by a finite number of spatially localized sources and address the problem of estimating these sources using spatiotemporal samples of the field obtained with a sensor network. Within this framework, we consider two different time evolutions: the case where the sources are instantaneous, as well as, the case where the sources decay exponentially in time after activation. We first derive novel exact inversion formulas, for both source distributions, through the use of Green's second theorem and a family of sensing functions to compute generalized field samples. These generalized samples can then be inverted using variations of existing algebraic methods such as Prony's method. Next, we develop a novel and robust reconstruction method for diffusion fields by properly extending these formulas to operate on the spatiotemporal samples of the field. Finally, we present numerical results using both synthetic and real data to verify the algorithms proposed herein.},
	pages = {3018--3031},
	number = {12},
	journaltitle = {{IEEE} Transactions on Signal Processing},
	author = {Murray-Bruce, John and Dragotti, Pier Luigi},
	date = {2015},
	note = {Publisher: {IEEE}
32 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Prony's method, Spatiotemporal sampling, diffusion fields, finite rate of innovation ({FRI}), sensor networks},
}

@article{ponciano_knowledge-based_2017,
	title = {Knowledge-based object recognition in point clouds and image data sets Wissensgestützte Objekterkennung in Punktwolken und Bilddatensätzen},
	doi = {10/gjjgzs},
	issue = {September},
	author = {Ponciano, Ph D Student Jean-jacques and Boochs, Prof Frank and Trémeau, Prof Alain},
	date = {2017},
}

@article{philion_learning_2020,
	title = {Learning to Evaluate Perception Models Using Planner-Centric Metrics},
	issn = {10636919},
	doi = {10/ghbbvp},
	abstract = {Variants of accuracy and precision are the gold-standard by which the computer vision community measures progress of perception algorithms. One reason for the ubiquity of these metrics is that they are largely task-agnostic; we in general seek to detect zero false negatives or positives. The downside of these metrics is that, at worst, they penalize all incorrect detections equally without conditioning on the task or scene, and at best, heuristics need to be chosen to ensure that different mistakes count differently. In this paper, we propose a principled metric for 3D object detection specifically for the task of self-driving. The core idea behind our metric is to isolate the task of object detection and measure the impact the produced detections would induce on the downstream task of driving. Without hand-designing it to, we find that our metric penalizes many of the mistakes that other metrics penalize by design. In addition, our metric downweighs detections based on additional factors such as distance from a detection to the ego car and the speed of the detection in intuitive ways that other detection metrics do not. For human evaluation, we generate scenes in which standard metrics and our metric disagree and find that humans side with our metric 79\% of the time. Our project page including an evaluation server can be found at https://nv-tlabs.github.io/detection-relevance.},
	pages = {14052--14061},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Philion, Jonah and Kar, Amlan and Fidler, Sanja},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2004.08745},
	note = {6 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{piciarelli_trajectory-based_2008,
	title = {Trajectory-based anomalous event detection},
	volume = {18},
	issn = {10518215},
	doi = {10/fbfvxj},
	abstract = {During the last years, the task of automatic event analysis in video sequences has gained an increasing attention among the research community. The application domains are disparate, ranging from video surveillance to automatic video annotation for sport videos or {TV} shots. Whatever the application field, most of the works in event analysis are based on two main approaches: the former based on explicit event recognition, focused on finding high-level, semantic Interpretations of video sequences, and the latter based on anomaly detection. This paper deals with the second approach, where the final goal is not the explicit labeling of recognized events, but the detection of anomalous events differing from typical patterns. In particular, the proposed work addresses anomaly detection by means of trajectory analysis, an approach with several application fields, most notably video surveillance and traffic monitoring. The proposed approach is based on single-class support vector machine ({SVM}) clustering, where the novelty detection {SVM} capabilities are used for the identification of anomalous trajectories. Particular attention is given to trajectory classification in absence of a priori information on the distribution of outliers. Experimental results prove the validity of the proposed approach. © 2008 {IEEE}.},
	pages = {1544--1554},
	number = {11},
	journaltitle = {{IEEE} Transactions on Circuits and Systems for Video Technology},
	author = {Piciarelli, Claudio and Micheloni, Christian and Foresti, Gian Luca},
	date = {2008},
	note = {398 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Anomaly detection, event analysis, support vector machines ({SVMs}), trajectory clustering.},
}

@article{pinggera_lost_2016,
	title = {Lost and found: Detecting small road hazards for self-driving vehicles},
	volume = {2016-Novem},
	issn = {21530866},
	doi = {10/ggwdc3},
	abstract = {Detecting small obstacles on the road ahead is a critical part of the driving task which has to be mastered by fully autonomous cars. In this paper, we present a method based on stereo vision to reliably detect such obstacles from a moving vehicle. The proposed algorithm performs statistical hypothesis tests in disparity space directly on stereo image data, assessing freespace and obstacle hypotheses on independent local patches. This detection approach does not depend on a global road model and handles both static and moving obstacles. For evaluation, we employ a novel lost-cargo image sequence dataset comprising more than two thousand frames with pixelwise annotations of obstacle and free-space and provide a thorough comparison to several stereo-based baseline methods. The dataset will be made available to the community to foster further research on this important topic4. The proposed approach outperforms all considered baselines in our evaluations on both pixel and object level and runs at frame rates of up to 20 Hz on 2 mega-pixel stereo imagery. Small obstacles down to the height of 5 cm can successfully be detected at 20 m distance at low false positive rates.},
	pages = {1099--1106},
	journaltitle = {{IEEE} International Conference on Intelligent Robots and Systems},
	author = {Pinggera, Peter and Ramos, Sebastian and Gehrig, Stefan and Franke, Uwe and Rother, Carsten and Mester, Rudolf},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1609.04653},
	note = {{ISBN}: 9781509037629
37 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{schmidhuber_deep_2015,
	title = {Deep Learning in neural networks: An overview},
	volume = {61},
	issn = {18792782},
	doi = {10/f6v78n},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	pages = {85--117},
	journaltitle = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	date = {2015},
	pmid = {25462637},
	eprinttype = {arxiv},
	eprint = {1404.7828},
	note = {8740 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Deep learning, Evolutionary computation, Reinforcement learning, Supervised learning, Unsupervised learning},
}

@article{sarkar_domain_2020,
	title = {Domain knowledge based genetic algorithms for mobile robot path planning having single and multiple targets},
	issn = {22131248},
	url = {https://doi.org/10.1016/j.jksuci.2020.10.010},
	doi = {10/gjjgz2},
	abstract = {In path planning problem of a mobile robot, the objective is to find an optimal collision free path from a source to single or multiple targets. Domain knowledge based genetic algorithms have been proposed to address the path planning problem having single as well as multiple independent targets. Four new domain knowledge based operators namely “circuit removal operator”, “insertion-deletion operator”, “refinement Operator”, and “target alignment operator” have been introduced in this work. Among these four operators, first three have been used for the path planning problem having single target, whereas all these four operators have been used for path planning problem having multiple independent targets. Proposed methods have been deployed on several simulated environments of varying sizes. From the experimental results, it has been established that the domain knowledge based operators enhance the capability of conventional genetic algorithm. Our proposed method for Mobile Robot's Path Planning Problem having single target has outperformed some previously proposed evolutionary algorithm based methods.},
	issue = {xxxx},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	author = {Sarkar, Ritam and Barman, Debaditya and Chowdhury, Nirmalya},
	date = {2020},
	note = {Publisher: The Authors
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Domain knowledge based operator, Genetic algorithms, Multiple targets path planning, Robot path planning, Single target path planning},
}

@inproceedings{sequeira_role_2020,
	title = {The role of machine learning for trajectory prediction in cooperative driving},
	isbn = {978-1-4503-8015-7},
	doi = {10/gjjgz6},
	abstract = {In this paper, we study the role that machine learning can play in cooperative driving. Given the increasing rate of connectivity in modern vehicles, and road infrastructure, cooperative driving is a promising first step in automated driving. The example scenario we explored in this paper, is coordinated lane merge, with data collection, test and evaluation all conducted in an automotive test track. The assumption is that vehicles are a mix of those equipped with communication units on board, i.e. connected vehicles, and those that are not connected. However, roadside cameras are connected and can capture all vehicles including those without connectivity. We develop a Traffic Orchestrator that suggests trajectories based on these two sources of information, i.e. connected vehicles, and connected roadside cameras. Recommended trajectories are built, which are then communicated back to the connected vehicles. We explore the use of different machine learning techniques in accurately and timely prediction of trajectories.},
	booktitle = {Proceedings of the International Symposium on Mobile Ad Hoc Networking and Computing ({MobiHoc})},
	author = {Sequeira, Luis and Mahmoodi, Toktam},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2010.11743},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {5G, {MEC}, V2X communications, cooperative driving, intelligent transport system, lane merge, machine learning, mobile edge, reinforcement learning},
}

@article{sarkar_behavior_2019,
	title = {A behavior driven approach for sampling rare event situations for autonomous vehicles},
	issn = {21530866},
	doi = {10/gjjgzz},
	abstract = {Performance evaluation of urban autonomous vehicles ({AVs}) requires a realistic model of the behavior of other road users in the environment. Learning such models from data involves collecting naturalistic data of real-world human behavior. In many cases, acquisition of this data can be prohibitively expensive or intrusive. Additionally, the available data often contain only typical behaviors and exclude behaviors that are classified as rare events. To evaluate the performance of {AVs} in such situations, we develop a model of traffic behavior based on the theory of bounded rationality. Based on the experiments performed on a large naturalistic driving data, we show that the developed model can be applied to estimate probability of rare events, as well as to generate new traffic situations for testing.},
	pages = {6407--6414},
	journaltitle = {{IEEE} International Conference on Intelligent Robots and Systems},
	author = {Sarkar, Atrisha and Czamecki, Krzysztof},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1903.01539},
	note = {{ISBN}: 9781728140049
6 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{schreiner_bayesian_2016,
	title = {Bayesian Environment Representation, Prediction, and Criticality Assessment for Driver Assistance Systems},
	doi = {10/gjjgz4},
	issue = {September},
	journaltitle = {Bayesian Environment Representation, Prediction, and Criticality Assessment for Driver Assistance Systems},
	author = {Schreiner, Matthias},
	date = {2016},
}

@article{salakhutdinov_integrating_2019,
	title = {Integrating Domain-Knowledge into Deep Learning},
	doi = {10/gf7m9t},
	abstract = {Machine comprehension ({MC}), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to {MC}. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow ({BIDAF}) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset ({SQuAD}) and {CNN}/{DailyMail} cloze test.},
	pages = {3176--3176},
	author = {Salakhutdinov, Ruslan},
	date = {2019},
	note = {{ISBN}: 9781450362016
1 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{schutera_night--day_2020,
	title = {Night-to-Day: Online Image-to-Image Translation for Object Detection Within Autonomous Driving by Night},
	volume = {8858},
	issn = {23798858},
	doi = {10/gjjgz5},
	abstract = {Object detectors are central to autonomous driving and widely used in driver assistance systems. Object detectors are trained on a finite amount of data, within a specific domain. This hampers detection performance when applying object detectors to samples from other domains during inference, an effect known as domain gap. Domain gap is a concern for datadriven applications, evoking repetitive retraining of networks when the applications unfold into other domains. With object detectors that have been trained on day images only, domain gap can be clearly observed in object detection by night. Training object detectors on night images is critical due to the enormous effort required to generate an adequate amount of diversely labeled data, and existing data sets often tend to overfit specific domain characteristics. This work proposes, for the first time, adapting domains by online image-to-image translation to expand an object detector\&\#x0027;s domain of operation. The domain gap is decreased without additional labeling effort, and without having to retrain the object detector, while unfolding into the target domain. The approach follows the concept of domain adaptation, shifting the samples of the target domain into the domain known to the object detector (source domain).},
	pages = {1--10},
	issue = {c},
	journaltitle = {{IEEE} Transactions on Intelligent Vehicles},
	author = {Schutera, Mark and Hussein, Mostafa and Abhau, Jochen and Mikut, Ralf and Reischl, Markus},
	date = {2020},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {{BDD}100K, Domain adaptation, {UNIT} architecture, image-to-image translation, object detection},
}

@article{stocco_misbehaviour_2020,
	title = {Misbehaviour prediction for autonomous driving systems},
	issn = {02705257},
	doi = {10/gjjg2b},
	abstract = {Deep Neural Networks ({DNNs}) are the core component of modern autonomous driving systems. To date, it is still unrealistic that a {DNN} will generalize correctly to all driving conditions. Current testing techniques consist of offline solutions that identify adversarial or corner cases for improving the training phase. In this paper, we address the problem of estimating the confidence of {DNNs} in response to unexpected execution contexts with the purpose of predicting potential safety-critical misbehaviours and enabling online healing of {DNN}-based vehicles. Our approach {SelfOracle} is based on a novel concept of self-assessment oracle, which monitors the {DNN} confidence at runtime, to predict unsupported driving scenarios in advance. {SelfOracle} uses autoencoderand time series-based anomaly detection to reconstruct the driving scenarios seen by the car, and to determine the confidence boundary between normal and unsupported conditions. In our empirical assessment, we evaluated the effectiveness of different variants of {SelfOracle} at predicting injected anomalous driving contexts, using {DNN} models and simulation environment from Udacity. Results show that, overall, {SelfOracle} can predict 77\% misbehaviours, up to six seconds in advance, outperforming the online input validation approach of {DeepRoad}.},
	pages = {359--371},
	journaltitle = {Proceedings - International Conference on Software Engineering},
	author = {Stocco, Andrea and Weiss, Michael and Calzana, Marco and Tonella, Paolo},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1910.04443},
	note = {{ISBN}: 9781450371216
14 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {2020, Anomaly detection, Deep learning, Misbehaviour prediction, Testing, acm reference format, and paolo tonella, andrea stocco, anomaly detection, deep learning, marco calzana, michael weiss, misbehaviour prediction, testing},
}

@article{theofilatos_predicting_2016,
	title = {Predicting Road Accidents: A Rare-events Modeling Approach},
	volume = {14},
	issn = {23521465},
	url = {http://dx.doi.org/10.1016/j.trpro.2016.05.293},
	doi = {10/gjjg2d},
	abstract = {Modeling road accident occurrence has gained increasing attention over the years. So far, considerable efforts have been made from researchers and policy makers in order to explain road accidents and improve road safety performance of highways. In reality, road accidents are rare events. In such cases, the binary dependent variable is characterized by dozens to thousands of times fewer events (accidents) than non-events (non-accidents). Instead of using traditional logistic regression methods, this paper considers accidents as rare events and proposes a series of rare-events logit models which are applied in order to model road accident occurrence by utilizing real-time traffic data. This statistical procedure was initially proposed by King and Zeng (2001) when scholars study rare events such as wars, massive economic crises and so on. Rare-events logit models basically estimate the same models as traditional logistic regression, but the estimates as well as the probabilities are corrected for the bias that occurs when the sample is small or the observed events are very rare. Consequently, the basic problem of underestimating the event probabilities is avoided as stated by King and Zeng (2001). To the best of our knowledge, this is the first time that this approach is followed when road accident data are analyzed. Instead of applying a traditional case-control study, the complete dataset of hourly aggregated traffic data such as flow, occupancy, mean time speed and percentage of trucks, were collected from three random loop detectors in the Attica Tollway ("Attiki Odos") located in Greater Athens Area in Greece for the 2008-2011 period. The modeling results showed an adequate statistical fit and reveal a negative relationship between accident occurrence and the natural logarithm of speed in the accident location. This study attempts to contribute to the understanding of accident occurrence in motorways by developing novel models such as the rare-events logit for the first time in safety evaluation of motorways.},
	pages = {3399--3405},
	journaltitle = {Transportation Research Procedia},
	author = {Theofilatos, Athanasios and Yannis, George and Kopelias, Pantelis and Papadimitriou, Fanis},
	date = {2016},
	note = {Publisher: Elsevier B.V.
19 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {accident occurrence, logistic regression, rare events, traffic parameters},
}

@article{shankarampeta_few-shot_2021,
	title = {Few-Shot Class Incremental Learning with Generative Feature Replay},
	doi = {10/gjjgz7},
	pages = {259--267},
	issue = {Icpram},
	author = {Shankarampeta, Abhilash and Yamauchi, Koichiro},
	date = {2021},
	note = {{ISBN}: 0000000314770
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{tsai_comparative_2013,
	title = {A comparative study of hybrid machine learning techniques for customer lifetime value prediction},
	volume = {42},
	issn = {0368492X},
	doi = {10/f4wqzj},
	abstract = {Customer lifetime value ({CLV}) has received increasing attention in database marketing. Enterprises can retain valuable customers by the correct prediction of valuable customers. In the literature, many data mining and machine learning techniques have been applied to develop {CLV} models. Specifically, hybrid techniques have shown their superiorities over single techniques. However, it is unknown which hybrid model can perform the best in customer value prediction. Therefore, the purpose of this paper is to compares two types of commonlyused hybrid models by classification+classification and clustering+classification hybrid approaches, respectively, in terms of customer value prediction. To construct a hybrid model, multiple techniques are usually combined in a twostage manner, in which the first stage is based on either clustering or classification techniques, which can be used to preprocess the data. Then, the output of the first stage (i.e. the processed data) is used to construct the second stage classifier as the prediction model. Specifically, decision trees, logistic regression, and neural networks are used as the classification techniques and kmeans and selforganizing maps for the clustering techniques to construct six different hybrid models. The experimental results over a real case dataset show that the classification+classification hybrid approach performs the best. In particular, combining twostage of decision trees provides the highest rate of accuracy (99.73 percent) and lowest rate of Type I/{II} errors (0.22 percent/0.43 percent). The contribution of this paper is to demonstrate that hybrid machine learning techniques perform better than single ones. In addition, this paper allows us to find out which hybrid technique performs best in terms of {CLV} prediction. © 2013, Emerald Group Publishing Limited},
	pages = {357--370},
	number = {3},
	journaltitle = {Kybernetes},
	author = {Tsai, Chih Fong and hu, Ya Han and Hung, Chia Sheng and Hsu, Yu Feng},
	date = {2013},
	note = {10 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Customer information, Customer lifetime value, Data mining, Database marketing, Hybrid models, Machine learning},
}

@article{tsai_credit_2010,
	title = {Credit rating by hybrid machine learning techniques},
	volume = {10},
	issn = {15684946},
	doi = {10/dfxr4w},
	abstract = {It is very important for financial institutions to develop credit rating systems to help them to decide whether to grant credit to consumers before issuing loans. In literature, statistical and machine learning techniques for credit rating have been extensively studied. Recent studies focusing on hybrid models by combining different machine learning techniques have shown promising results. However, there are various types of combination methods to develop hybrid models. It is unknown that which hybrid machine learning model can perform the best in credit rating. In this paper, four different types of hybrid models are compared by 'Classification + Classification', 'Classification + Clustering', 'Clustering + Classification', and 'Clustering + Clustering' techniques, respectively. A real world dataset from a bank in Taiwan is considered for the experiment. The experimental results show that the 'Classification + Classification' hybrid model based on the combination of logistic regression and neural networks can provide the highest prediction accuracy and maximize the profit. © 2009 Elsevier B.V. All rights reserved.},
	pages = {374--380},
	number = {2},
	journaltitle = {Applied Soft Computing Journal},
	author = {Tsai, Chih Fong and Chen, Ming Lun},
	date = {2010},
	note = {107 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Consumer loans, Credit rating, Hybrid models, Machine learning, Maximum profits},
}

@article{tuncali_requirements-driven_2020,
	title = {Requirements-Driven Test Generation for Autonomous Vehicles with Machine Learning Components},
	volume = {5},
	issn = {23798858},
	doi = {10/gjjg2f},
	abstract = {Autonomous vehicles are complex systems that are challenging to test and debug. A requirements-driven approach to the development process can decrease the resources required to design and test these systems, while simultaneously increasing the reliability. We present a testing framework that uses signal temporal logic ({STL}), which is a precise and unambiguous requirements language. Our framework evaluates test cases against the {STL} formulae and additionally uses the requirements to automatically identify test cases that fail to satisfy the requirements. One of the key features of our tool is the support for machine learning ({ML}) components in the system design, such as deep neural networks. The framework allows evaluation of the control algorithms, including the {ML} components, and it also includes models of {CCD} camera, lidar, and radar sensors, as well as the vehicle environment. We use multiple methods to generate test cases, including covering arrays, which is an efficient method to search discrete variable spaces. The resulting test cases can be used to debug the controller design by identifying controller behaviors that do not satisfy requirements. The test cases can also enhance the testing phase of development by identifying critical corner cases that correspond to the limits of the system's allowed behaviors. We present {STL} requirements for an autonomous vehicle system, which capture both component-level and system-level behaviors. Additionally, we present three driving scenarios and demonstrate how our requirements-driven testing framework can be used to identify critical system behaviors, which can be used to support the development process.},
	pages = {265--280},
	number = {2},
	journaltitle = {{IEEE} Transactions on Intelligent Vehicles},
	author = {Tuncali, Cumhur Erkan and Fainekos, Georgios and Prokhorov, Danil and Ito, Hisahiro and Kapinski, James},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1908.01094},
	note = {17 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous vehicles, cyber-physical systems, system validation, system verification},
}

@article{ulbrich_defining_2015,
	title = {Defining and Substantiating the Terms Scene, Situation, and Scenario for Automated Driving},
	volume = {2015-Octob},
	doi = {10/gftjzj},
	abstract = {For the design and test of functional modules of an automated vehicle, it is essential to define interfaces. While interfaces on the perception side, like object lists, point clouds or occupancy grids, are to a certain degree settled already, they are quite vague in the consecutive steps of context modeling and in particular on the side of driving execution. The authors consider the scene as the central interface between perception and behavior planning \&amp; control. Within the behavior planning \&amp; control block, a situation is a central data container. A scenario is a common approach to substantiate test cases for functional modules and can be used to detail the functional description of a system. However, definitions of these terms are often-at best-vague or even contradictory. This paper will review these definitions and come up with a consistent definition for each term. Moreover, we present an example for the implementation of each of these interfaces.},
	pages = {982--988},
	journaltitle = {{IEEE} Conference on Intelligent Transportation Systems, Proceedings, {ITSC}},
	author = {Ulbrich, Simon and Menzel, Till and Reschka, Andreas and Schuldt, Fabian and Maurer, Markus},
	date = {2015},
	note = {{ISBN}: 9781467365956
147 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {★},
}

@article{wachenfeld_worst-time--collision_2016,
	title = {The worst-time-to-collision metric for situation identification},
	volume = {2016-Augus},
	doi = {10/ggntmz},
	abstract = {Currently, the introduction of highly automated vehicles is one of the major targets of the whole automotive industry. However, it is still unclear how to cope with the testing effort necessary to approve an automated vehicle. One possibility to reduce the testing effort is to focus the assessment on critical situations. To describe the criticality of these situations, metrics are required. Firstly, this paper states requirements on assessment metrics. Secondly, this paper introduces a simple but comprehensive metric to select objects and situations out of a typical test drive to reduce the amount of data saved for further analysis. As it must not be assumed that the same situations are critical for human drivers and for automation, the metric only relies on driving dynamics and the physical possibilities of the vehicle. The special feature of this metric is the worst case assumption for vehicle behavior. If a situation is uncritical, even with the worst possible maneuvers, it is allowed to be neglected in the assessment process.},
	pages = {729--734},
	issue = {Iv},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Wachenfeld, Walther and Junietz, Philipp and Wenzel, Raphael and Winner, Hermann},
	date = {2016},
	note = {{ISBN}: 9781509018215
30 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{vatcha_practical_2014,
	title = {Practical motion planning in unknown and unpredictable environments},
	volume = {79},
	issn = {1610742X},
	doi = {10/gjjg2j},
	abstract = {Motion planners for robots in unknown and dynamic environments often assume known obstacle geometry and use that to predict unknown motions of obstacles through tracking, but such an assumption may not be realistic. In [1], we introduced a collision-free perceiver ({CFP}) that can detect guaranteed collision-free trajectory segments in the unknown configuration-time ({CT}) space of a robot without assuming known obstacle geometry or motion. However, such a guarantee by the {CFP} is at the expense of a finite period for perception and processing of each collision-free {CT} point. In this paper, we address how to incorporate the {CFP}, taking into account its finite processing time, into real-time motion planning to enable a robot of high degree of freedom to plan and move at the same time in an unknown and unpredictable environment while minimizing unsafe stops when the robot may collide with an obstacle. The approach was implemented and tested in experiments with a real 7-{DOF} robot arm and a stereo-vision sensor, indicating the potential of the approach.},
	pages = {883--897},
	journaltitle = {Springer Tracts in Advanced Robotics},
	author = {Vatcha, Rayomand and Xiao, Jing},
	date = {2014},
	note = {{ISBN}: 9783642285714
3 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{tremblay_training_2018,
	title = {Training deep networks with synthetic data: Bridging the reality gap by domain randomization},
	volume = {2018-June},
	issn = {21607516},
	doi = {10/ggwbt5},
	abstract = {We present a system for training deep neural networks for object detection using synthetic images. To handle the variability in real-world data, the system relies upon the technique of domain randomization, in which the parameters of the simulator-such as lighting, pose, object textures, etc.-are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest. We explore the importance of these parameters, showing that it is possible to produce a network with compelling performance using only non-artistically-generated synthetic data. With additional fine-tuning on real data, the network yields better performance than using real data alone. This result opens up the possibility of using inexpensive synthetic data for training neural networks while avoiding the need to collect large amounts of hand-annotated real-world data or to generate high-fidelity synthetic worlds-both of which remain bottlenecks for many applications. The approach is evaluated on bounding box detection of cars on the {KITTI} dataset.},
	pages = {1082--1090},
	journaltitle = {{IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	author = {Tremblay, Jonathan and Prakash, Aayush and Acuna, David and Brophy, Mark and Jampani, Varun and Anil, Cem and To, Thang and Cameracci, Eric and Boochoon, Shaad and Birchfield, Stan},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1804.06516},
	note = {{ISBN}: 9781538661000
279 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{wadud_fully_2017,
	title = {Fully automated vehicles: A cost of ownership analysis to inform early adoption},
	volume = {101},
	issn = {09658564},
	url = {http://dx.doi.org/10.1016/j.tra.2017.05.005},
	doi = {10/gbm4jc},
	abstract = {Vehicle automation and its adoption by the vehicle purchasers is an active area of research among transportation academics. So far, most of the interest in the adoption of fully automated, driverless vehicles has focussed on private vehicles alone, yet full automation could be beneficial for commercial vehicles too. This paper identifies the vehicle sectors that will likely be the earliest adopters of full automation. Total cost of ownership analysis is used to compare the costs (and benefits) of vehicle automation for private vehicles among different income groups and commercial vehicles in the taxi and freight sectors in the {UK}. Commercial operations clearly benefit more from automation because the driver costs can be reduced substantially through automation. Among private users, households with the highest income will benefit more from automation because of their higher driving distances and higher perceived value of time, which can be used more productively through full automation.},
	pages = {163--176},
	number = {2017},
	journaltitle = {Transportation Research Part A: Policy and Practice},
	author = {Wadud, Zia},
	date = {2017},
	note = {Publisher: Elsevier Ltd
70 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous car, Driverless car, Early adoption, Total cost of ownership, Travel time use, Vehicle automation},
}

@article{wang_v2vnet_2020,
	title = {V2VNet: Vehicle-to-vehicle communication for joint perception and prediction},
	issn = {23318422},
	doi = {10/gjjg2n},
	abstract = {In this paper, we explore the use of vehicle-to-vehicle (V2V) communication to improve the perception and motion forecasting performance of self-driving vehicles. By intelligently aggregating the information received from multiple nearby vehicles, we can observe the same scene from different viewpoints. This allows us to see through occlusions and detect actors at long range, where the observations are very sparse or non-existent. We also show that our approach of sending compressed deep feature map activations achieves high accuracy while satisfying communication bandwidth requirements.},
	journaltitle = {{arXiv}},
	author = {Wang, Tsun Hsuan and Manivasagam, Sivabalan and Liang, Ming and Yang, Bin and Zeng, Wenyuan and Tu, James and Urtasun, Raquel},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2008.07519},
	note = {{ISBN}: 9783030585358
8 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous Driving, Motion Forecast, Object Detection},
}

@article{wang_traffic_2018,
	title = {Traffic Sensory Data Classification by Quantifying Scenario Complexity},
	volume = {2018-June},
	doi = {10/gjjg2k},
	abstract = {For unmanned ground vehicle ({UGV}) off-line testing and performance evaluation, massive amount of traffic scenario data is often required. The annotations in current off-line traffic sensory dataset typically include I) types of roadways {II}) scene types {III}) specific characteristics that are generally considered challenging for cognitive algorithms. While such annotations are helpful in manual selection of data, they are insufficient for comprehensive and quantitate measurement of per-roadway-segment scenario complexity. To resolve such limitations, we propose a traffic sensory data classification paradigm based on quantifying the scenario complexity for each roadway segment, where such quantification is jointly based on road semantic complexity and traffic element complexity. The road semantic complexity is a proposed measurement of the complexity incurred by the static elements such as curvy roads, intersections, merges and splits, which is predicted with a Support Vector Regression ({SVR}). The traffic element complexity is a measurement of complexity due to dynamic traffic elements, such as nearby vehicles and pedestrians. Experimental results and a case study verify the efficacy of the proposed method.},
	pages = {1543--1548},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Wang, Jiajie and Zhang, Chi and Liu, Yuehu and Zhang, Qilin},
	date = {2018},
	note = {{ISBN}: 9781538644522
10 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{wang_developing_2017,
	title = {Developing a platoon-wide Eco-Cooperative Adaptive Cruise Control ({CACC}) system},
	doi = {10/ggqd2k},
	abstract = {Connected and automated vehicle ({CAV}) technology has become increasingly popular. As an example, Cooperative Adaptive Cruise Control ({CACC}) systems are of high interest, allowing {CAVs} to communicate and cooperate with each other to form platoons, where one vehicle follows another with a predefined spacing or time gap. Although numerous studies have been conducted on {CACC} systems, very few have examined the protocols from the perspective of environmental sustainability, not to mention from a platoon-wide consideration. In this study, we propose a vehicle-To-vehicle (V2V) communication based Eco-{CACC} system, aiming to minimize the platoon-wide energy consumption and pollutant emissions at different stages of the {CACC} operation. A full spectrum of environmentally-friendly {CACC} maneuvers are explored and the associated protocols are developed, including sequence determination, gap closing and opening, platoon cruising with gap regulation, and platoon joining and splitting. Simulation studies of different scenarios are conducted using {MATLAB}/Simulink. Compared to an existing {CACC} system, the proposed one can achieve additional 2\% energy savings and additional 17\% pollutant emissions reductions during the platoon joining scenario.},
	pages = {1256--1261},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Wang, Ziran and Wu, Guoyuan and Hao, Peng and Boriboonsomsin, Kanok and Barth, Matthew},
	date = {2017},
	note = {{ISBN}: 9781509048045
29 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Eco-Cooperative Adaptive Cruise Control (Eco-{CACC}), Energy consumption, Pollutant emissions},
}

@article{wulfmeier_large-scale_2017,
	title = {Large-scale cost function learning for path planning using deep inverse reinforcement learning},
	volume = {36},
	issn = {17413176},
	doi = {10/gbzbnc},
	abstract = {We present an approach for learning spatial traversability maps for driving in complex, urban environments based on an extensive dataset demonstrating the driving behaviour of human experts. The direct end-to-end mapping from raw input data to cost bypasses the effort of manually designing parts of the pipeline, exploits a large number of data samples, and can be framed additionally to refine handcrafted cost maps produced based on manual hand-engineered features. To achieve this, we introduce a maximum-entropy-based, non-linear inverse reinforcement learning ({IRL}) framework which exploits the capacity of fully convolutional neural networks ({FCNs}) to represent the cost model underlying driving behaviours. The application of a high-capacity, deep, parametric approach successfully scales to more complex environments and driving behaviours, while at deployment being run-time independent of training dataset size. After benchmarking against state-of-the-art {IRL} approaches, we focus on demonstrating scalability and performance on an ambitious dataset collected over the course of 1 year including more than 25,000 demonstration trajectories extracted from over 120 km of urban driving. We evaluate the resulting cost representations by showing the advantages over a carefully, manually designed cost map and furthermore demonstrate its robustness towards systematic errors by learning accurate representations even in the presence of calibration perturbations. Importantly, we demonstrate that a manually designed cost map can be refined to more accurately handle corner cases that are scarcely seen in the environment, such as stairs, slopes and underpasses, by further incorporating human priors into the training framework.},
	pages = {1073--1087},
	number = {10},
	journaltitle = {International Journal of Robotics Research},
	author = {Wulfmeier, Markus and Rao, Dushyant and Wang, Dominic Zeng and Ondruska, Peter and Posner, Ingmar},
	date = {2017},
	note = {52 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Learning from demonstration, autonomous driving, cost functions, inverse reinforcement learning, neural networks},
}

@article{wulfmeier_watch_2016,
	title = {Watch this: Scalable cost-function learning for path planning in urban environments},
	volume = {2016-Novem},
	issn = {21530866},
	doi = {10/gjjg2q},
	abstract = {In this work, we present an approach to learn cost maps for driving in complex urban environments from a large number of demonstrations of human driving behaviour. The learned cost maps are constructed directly from raw sensor measurements, bypassing the effort of manually designing cost maps as well as features. When deploying the cost maps, the trajectories generated not only replicate human-like driving behaviour but are also demonstrably robust against systematic errors in putative robot configuration. To achieve this we deploy a Maximum Entropy based, non-linear {IRL} framework which uses Fully Convolutional Neural Networks ({FCNs}) to represent the cost model underlying expert driving behaviour. Using a deep, parametric approach enables us to scale efficiently to large datasets and complex behaviours while being run-time independent of dataset extent during deployment. We demonstrate scalability and performance on an ambitious dataset collected over the course of one year including more than 25k demonstration trajectories extracted from over 120km of driving and 13 different drivers. We evaluate against a carefully designed cost map and, in addition, demonstrate robustness to systematic errors by learning precise cost-maps even in the presence of system calibration perturbations.},
	pages = {2089--2095},
	journaltitle = {{IEEE} International Conference on Intelligent Robots and Systems},
	author = {Wulfmeier, Markus and Wang, Dominic Zeng and Posner, Ingmar},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1607.02329},
	note = {{ISBN}: 9781509037629
67 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{wasay_queriosity_2015,
	title = {Queriosity: Automated Data Exploration},
	doi = {10/gf9q7g},
	abstract = {Curiosity, a fundamental drive amongst higher living organisms, is what enables exploration, learning and creativity. In our increasingly data-driven world, data exploration, i.e., Making sense of mounting haystacks of data, is akin to intelligence for science, business and individuals. However, modern data systems-designed for data retrieval rather than exploration-only let us retrieve data and ask if it is interesting. This makes knowledge discovery a game of hit-And-Trial which can only be orchestrated by expert data scientists. We present the vision toward Queriosity, an automated and personalized data exploration system. Designed on the principles of autonomy, learning and usability, Queriosity envisions a paradigm shift in data exploration and aims to become a a personalized 'data robot' that provides a direct answer to what is interesting in a user's data set, instead of just retrieving data. Queriosity autonomously and continuously navigates toward interesting findings based on trends, statistical properties and interactive user feedback.},
	pages = {716--719},
	journaltitle = {Proceedings - 2015 {IEEE} International Congress on Big Data, {BigData} Congress 2015},
	author = {Wasay, Abdul and Athanassoulis, Manos and Idreos, Stratos},
	date = {2015},
	note = {Publisher: {IEEE}
{ISBN}: 9781467372787
29 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Curious data systems, Data analysis, Data exploration, Data systems},
}

@article{wotawa_quality_2018,
	title = {Quality assurance methodologies for automated driving},
	volume = {135},
	issn = {0932383X},
	url = {http://dx.doi.org/10.1007/s00502-018-0630-7},
	doi = {10/gjjg2p},
	abstract = {For safety critical systems like cars, trains, or airplanes quality assurance methods and techniques are crucial for preventing situations that may harm people. The case of automated driving represents the next level of safety critical systems where additional challenges arise. This includes the question of how to assure that artificial intelligence and machine learning based systems fulfill safety criticality requirements under all potential conditions and situations that may emerge during operation. In this paper, we first review simulation-based verification and validation methods for such systems and afterwards discuss necessary requirements and future research challenges that have to be solved in order to bring automated driving into practice without compromising safety requirements.},
	pages = {322--327},
	number = {4},
	journaltitle = {Elektrotechnik und Informationstechnik},
	author = {Wotawa, Franz and Peischl, Bernhard and Klück, Florian and Nica, Mihai},
	date = {2018},
	note = {Publisher: The Author(s)
{ISBN}: 0050201806307
8 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {{ADAS}, {SOTIF}, functional safety, testing, verification},
}

@article{xu_knowledge_2020,
	title = {Knowledge Distillation Meets Self-supervision},
	volume = {12354 {LNCS}},
	issn = {16113349},
	doi = {10/gjjg2r},
	abstract = {Knowledge distillation, which involves extracting the “dark knowledge” from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning. Unlike previous works that exploit architecture-specific cues such as activation and attention for distillation, here we wish to explore a more general and model-agnostic approach for extracting “richer dark knowledge” from the pre-trained teacher model. We show that the seemingly different self-supervision task can serve as a simple yet powerful solution. For example, when performing contrastive learning between transformed entities, the noisy predictions of the teacher network reflect its intrinsic composition of semantic and pose information. By exploiting the similarity between those self-supervision signals as an auxiliary task, one can effectively transfer the hidden information from the teacher to the student. In this paper, we discuss practical ways to exploit those noisy self-supervision signals with selective transfer for distillation. We further show that self-supervision signals improve conventional distillation with substantial gains under few-shot and noisy-label scenarios. Given the richer knowledge mined from self-supervision, our knowledge distillation approach achieves state-of-the-art performance on standard benchmarks, i.e., {CIFAR}100 and {ImageNet}, under both similar-architecture and cross-architecture settings. The advantage is even more pronounced under the cross-architecture setting, where our method outperforms the state of the art by an average of 2.3\% in accuracy rate on {CIFAR}100 across six different teacher-student pairs. The code and models are available at: https://github.com/xuguodong03/{SSKD}.},
	pages = {588--604},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Xu, Guodong and Liu, Ziwei and Li, Xiaoxiao and Loy, Chen Change},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2006.07114},
	note = {{ISBN}: 9783030585440
17 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{xu_vehicle_2011,
	title = {A vehicle model for micro-traffic simulation in dynamic urban scenarios},
	issn = {10504729},
	doi = {10/fj2z7k},
	abstract = {In order to improve energy efficiency of transport systems, eco-driving strategies are studied world-widely. However, most literatures on eco-driving based on traditional traffic flow models, are greatly simplified, and can not evaluate the effects on detailed driving behaviors. By referring to robot motion planning approaches, in this research a microscopic vehicle model is developed and it can represent different driving behaviors, such as aggressive or conservative driving; a collision detection algorithm is proposed that takes O(1) time to check for a trajectory's collision, enabling realtime planning; and a traffic simulation system is developed by incorporating traffic rules, so that the driving behaviors such as observing or not observing traffic rules can also be represented. Experiments are conducted on the simulation platform, and the performance of different driving behaviors on travel time, mileage, comfort and eco is studied. © 2011 {IEEE}.},
	pages = {2267--2274},
	journaltitle = {Proceedings - {IEEE} International Conference on Robotics and Automation},
	author = {Xu, Wenda and Yao, Wen and Zhao, Huijing and Zha, Hongbin},
	date = {2011},
	note = {{ISBN}: 9781612843865
11 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {★},
}

@article{xu_few-shot_2021,
	title = {Few-shot Object Detection via Sample Processing},
	issn = {21693536},
	doi = {10/gjjg2s},
	abstract = {Few-shot object detection ({FSOD}) eliminates the dependence on tremendous instances with manual annotations in conventional object detection. We deem that the scarcity of positive samples is the main reason that restricts the performance of {FSOD} detectors. In this paper, a novel {FSOD} model via sample processing, namely, {FSSP}, is proposed to detect objects accurately with only a few annotated samples, which is based on the structural design of the Siamese network and uses {YOLOv}3-{SPP} as the baseline. Central to {FSSP} are our designed self-attention ({SAM}) and positive-sample augmentation ({PSA}) modules. The former attempts to better extract the representative features of hard samples, and the latter expands the number and enriches the scale distribution of positive samples, inhibiting the growth of negative samples. For the fine-tuning phase, we modify the classification loss function to increase the punishment for hard samples. Experiments conducted on the {PASCAL} {VOC} and {MS} {COCO} datasets confirm that the proposed {FSSP} achieves competitive detection performance compared with state-of-the-art detectors.},
	pages = {29207--29221},
	journaltitle = {{IEEE} Access},
	author = {Xu, Honghui and Wang, Xinqing and Shao, Faming and Duan, Baoguo and Zhang, Peng},
	date = {2021},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Annotations, Detectors, Feature extraction, Few-shot learning, Image processing, Machine learning, Object detection, Shape, Task analysis, Training},
}

@article{yao_unsupervised_2019,
	title = {Unsupervised Traffic Accident Detection in First-Person Videos},
	issn = {21530866},
	doi = {10/gjjg2t},
	abstract = {Recognizing abnormal events such as traffic violations and accidents in natural driving scenes is essential for successful autonomous driving and advanced driver assistance systems. However, most work on video anomaly detection suffers from two crucial drawbacks. First, they assume cameras are fixed and videos have static backgrounds, which is reasonable for surveillance applications but not for vehicle-mounted cameras. Second, they pose the problem as one-class classification, relying on arduously hand-labeled training datasets that limit recognition to anomaly categories that have been explicitly trained. This paper proposes an unsupervised approach for traffic accident detection in first-person (dashboard-mounted camera) videos. Our major novelty is to detect anomalies by predicting the future locations of traffic participants and then monitoring the prediction accuracy and consistency metrics with three different strategies. We evaluate our approach using a new dataset of diverse traffic accidents, {AnAn} Accident Detection (A3D), as well as another publicly-available dataset. Experimental results show that our approach outperforms the state-of-the-art. Code and the dataset developed in this work are available at: https:llgithub.{comlMoonBtvdltad}-{IROS}2019},
	pages = {273--280},
	journaltitle = {{IEEE} International Conference on Intelligent Robots and Systems},
	author = {Yao, Yu and Xu, Mingze and Wang, Yuchen and Crandall, David J. and Atkins, Ella M.},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1903.00618},
	note = {{ISBN}: 9781728140049
26 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{yin_mastering_2017,
	title = {Mastering data complexity for autonomous driving with adaptive point clouds for urban environments},
	doi = {10/gbvrq6},
	abstract = {{LiDAR} sensors play a crucial role in autonomous driving and advanced driver assistance systems. By firing high-rate laser beams, a {LiDAR} device is able to project its surroundings as 2D or 3D point cloud, which can be used for different purposes such as object detection, map generation, localization, and navigation. Autonomous vehicles are often equipped with at least one multi-layer {LiDAR} sensor with 360-degree coverage to include as much information as possible in the point cloud. Such a device generates enormous amount of data which poses a challenge for data storage, real-Time computation, and data transmission, as autonomous vehicles are typically resource-constrained systems. This paper proposes a lightweight and adaptive point cloud data structure to reduce the size of a 3D point cloud. The suggested data structure can be flexibly configured with different parameters to adapt for precision, distance coverage, and reflectivity resolution. The precision of the data structure is evaluated using a 16-layer Velodyne {LiDAR} sensor ({VLP}-16) to collect data in the city area of {AstaZero} proving ground and Gothenburg downtown. Our results show that the adaptive data structure can consume only 1/8th of the original point cloud size and hence, it is particularly suitable for applications with limited hardware resources or certain tolerance to precision of the point cloud. The suggested concept is also generalizable to other types of point cloud providing sensors.},
	pages = {1364--1371},
	issue = {November},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Yin, Hang and Berger, Christian},
	date = {2017},
	note = {{ISBN}: 9781509048045
8 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{yang_driving_2019,
	title = {Driving Behavior Assessment and Anomaly Detection for Intelligent Vehicles},
	doi = {10/ggw4pq},
	abstract = {Ensuring safety of both traffic participants and passengers is an important challenge for rapidly growing autonomous vehicle technology. To this purpose, intelligent vehicles not only have to drive safe but must be able to safeguard itself from other abnormally driving vehicles and avoid potential collisions. Anomaly detection is one of the essential abilities in behavior analysis, which can be used to infer the moving intention of other vehicles and provide evidence for collision risk assessment. In this paper, we propose a behavior analysis method based on Hidden Markov Model ({HMM}) to assess the driving behavior of vehicles on the road and detect anomalous moments. The algorithm uses the real-time velocity and position of the surrounding vehicles provided by the Conditional Monte Carlo Dense Occupancy Tracker ({CMCDOT}) framework. Next, by associating with the road information, the movement of each vehicle can be classified into several observation states, namely, Approaching, Braking, Lane Changing, and Lane Keeping. Finally, by chaining these observation states using a Markov model, the abnormality of driving behavior can be inferred into Normal, Attention, and Risk. We perform experiments using {CARLA} simulator environment to simulate abnormal driving behaviors, and we provide results showing the successful detection of abnormal situations.},
	pages = {524--529},
	journaltitle = {Proceedings of the {IEEE} 2019 9th International Conference on Cybernetics and Intelligent Systems and Robotics, Automation and Mechatronics, {CIS} and {RAM} 2019},
	author = {Yang, Chule and Renzaglia, Alessandro and Paigwar, Anshul and Laugier, Christian and Wang, Danwei},
	date = {2019},
	note = {{ISBN}: 9781728134581
1 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{xue_survey_2018,
	title = {A Survey of Scene Understanding by Event Reasoning in Autonomous Driving},
	volume = {15},
	issn = {17518520},
	doi = {10/gjfz25},
	abstract = {Realizing autonomy is a hot research topic for automatic vehicles in recent years. For a long time, most of the efforts to this goal concentrate on understanding the scenes surrounding the ego-vehicle (autonomous vehicle itself). By completing low-level vision tasks, such as detection, tracking and segmentation of the surrounding traffic participants, e.g., pedestrian, cyclists and vehicles, the scenes can be interpreted. However, for an autonomous vehicle, low-level vision tasks are largely insufficient to give help to comprehensive scene understanding. What are and how about the past, the on-going and the future of the scene participants? This deep question actually steers the vehicles towards truly full automation, just like human beings. Based on this thoughtfulness, this paper attempts to investigate the interpretation of traffic scene in autonomous driving from an event reasoning view. To reach this goal, we study the most relevant literatures and the state-of-the-arts on scene representation, event detection and intention prediction in autonomous driving. In addition, we also discuss the open challenges and problems in this field and endeavor to provide possible solutions.},
	pages = {249--266},
	number = {3},
	journaltitle = {International Journal of Automation and Computing},
	author = {Xue, Jian Ru and Fang, Jian Wu and Zhang, Pu},
	date = {2018},
	note = {13 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous vehicle, event reasoning, intention prediction, scene representation, scene understanding},
}

@article{yin_when_2018,
	title = {When to use what data set for your self-driving car algorithm: An overview of publicly available driving datasets},
	volume = {2018-March},
	doi = {10/gh838q},
	abstract = {Data collection on public roads has been deemed a valuable activity along with the development of self-driving vehicles. The vehicle for data collection is typically equipped with a variety of sensors such as camera, {LiDAR}, radar, {GPS}, and {IMU}. The raw data of all sensors is logged on a disk while the vehicle is manually driven. The logged data can be subsequently used for training and testing different algorithms for autonomous driving, e.g., vehicle/pedestrian detection and tracking, {SLAM}, and motion estimation. Data collection is time-consuming and can sometimes be avoided by directly using existing datasets including sensor data collected by other researchers. A multitude of openly available datasets have been released to foster the research on automated driving. These datasets vary a lot in terms of traffic conditions, application focus, sensor setup, data format, size, tool support, and many other aspects. This paper presents an overview of 27 existing publicly available datasets containing data collected on public roads, compares each other from different perspectives, and provides guidelines for selecting the most suitable dataset for different purposes.},
	pages = {1--8},
	journaltitle = {{IEEE} Conference on Intelligent Transportation Systems, Proceedings, {ITSC}},
	author = {Yin, Hang and Berger, Christian},
	date = {2018},
	note = {{ISBN}: 9781538615256
27 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{yu_spatiotemporal_2020,
	title = {Spatiotemporal event detection: a review},
	volume = {13},
	issn = {17538955},
	doi = {10/ggrnwj},
	abstract = {The advancements of sensing technologies, including remote sensing, in situ sensing, social sensing, and health sensing, have tremendously improved our capability to observe and record natural and social phenomena, such as natural disasters, presidential elections, and infectious diseases. The observations have provided an unprecedented opportunity to better understand and respond to the spatiotemporal dynamics of the environment, urban settings, health and disease propagation, business decisions, and crisis and crime. Spatiotemporal event detection serves as a gateway to enable a better understanding by detecting events that represent the abnormal status of relevant phenomena. This paper reviews the literature for different sensing capabilities, spatiotemporal event extraction methods, and categories of applications for the detected events. The novelty of this review is to revisit the definition and requirements of event detection and to layout the overall workflow (from sensing and event extraction methods to the operations and decision-supporting processes based on the extracted events) as an agenda for future event detection research. Guidance is presented on the current challenges to this research agenda, and future directions are discussed for conducting spatiotemporal event detection in the era of big data, advanced sensing, and artificial intelligence.},
	pages = {1339--1365},
	number = {12},
	journaltitle = {International Journal of Digital Earth},
	author = {Yu, Manzhu and Bambacus, Myra and Cervone, Guido and Clarke, Keith and Duffy, Daniel and Huang, Qunying and Li, Jing and Li, Wenwen and Li, Zhenlong and Liu, Qian and Resch, Bernd and Yang, Jingchao and Yang, Chaowei},
	date = {2020},
	note = {10 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {{GeoAI}, cloud computing, computational challenges, digital earth, geography and geoscience, human dynamics, internet of things},
}

@article{zohdy_intersection_2016,
	title = {Intersection Management via Vehicle Connectivity: The Intersection Cooperative Adaptive Cruise Control System Concept},
	volume = {20},
	issn = {15472442},
	doi = {10/gjjg23},
	abstract = {Since the introduction of the vehicle infrastructure integration ({VII}) and connected vehicle ({CV}) initiatives in the United States, numerous in-vehicle technologies based on wireless communications are currently being deployed. One of these technologies is cooperative adaptive cruise control ({CACC}) systems, which provide better connectivity, safety, and mobility by allowing vehicles to travel in denser platoons through vehicle-to-vehicle (V2V) communication. Accordingly, the research presented in this article develops a simulation/optimization tool that optimizes the movement of {CACC}-equipped vehicles as a replacement for traditional intersection control. This system, which is named {iCACC}, assumes that the intersection controller receives vehicle requests to travel through an intersection and advises each vehicle on the optimum course of action ensuring no crashes occur while at the same time minimizing the intersection delay. Four intersection control scenarios are compared, namely: a traffic signal, an all-way stop control ({AWSC}), a roundabout, and the {iCACC} controller. The results show that the proposed {iCACC} system significantly reduces the average intersection delay and fuel consumption level by 90 and 45\%, respectively. Additionally, the article investigates the impact of vehicle dynamics, weather conditions, and level of market penetration of equipped vehicles on the future of automated vehicle control.},
	pages = {17--32},
	number = {1},
	journaltitle = {Journal of Intelligent Transportation Systems: Technology, Planning, and Operations},
	author = {Zohdy, Ismail H. and Rakha, Hesham A.},
	date = {2016},
	note = {94 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Connected Vehicles, Cooperative Cruise Control, Intersection Management},
}

@article{yu_intelligent_2018,
	title = {Intelligent land-vehicle model transfer trajectory planning method based on deep reinforcement learning},
	volume = {18},
	issn = {14248220},
	doi = {10/gjjg2v},
	abstract = {To address the problem of model error and tracking dependence in the process of intelligent vehicle motion planning, an intelligent vehicle model transfer trajectory planning method based on deep reinforcement learning is proposed, which is able to obtain an effective control action sequence directly. Firstly, an abstract model of the real environment is extracted. On this basis, a deep deterministic policy gradient ({DDPG}) and a vehicle dynamic model are adopted to jointly train a reinforcement learning model, and to decide the optimal intelligent driving maneuver. Secondly, the actual scene is transferred to an equivalent virtual abstract scene using a transfer model. Furthermore, the control action and trajectory sequences are calculated according to the trained deep reinforcement learning model. Thirdly, the optimal trajectory sequence is selected according to an evaluation function in the real environment. Finally, the results demonstrate that the proposed method can deal with the problem of intelligent vehicle trajectory planning for continuous input and continuous output. The model transfer method improves the model’s generalization performance. Compared with traditional trajectory planning, the proposed method outputs continuous rotation-angle control sequences. Moreover, the lateral control errors are also reduced.},
	pages = {1--22},
	number = {9},
	journaltitle = {Sensors (Switzerland)},
	author = {Yu, Lingli and Shao, Xuanya and Wei, Yadong and Zhou, Kaijun},
	date = {2018},
	pmid = {30200499},
	note = {{ISBN}: 8613055166724
16 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Deep reinforcement learning, End-to-end, Intelligent driving vehicle, Model transfer, Trajectory planning},
}

@article{zofka_pushing_2020,
	title = {Pushing {ROS} towards the Dark Side: A {ROS}-based Co-Simulation Architecture for Mixed-Reality Test Systems for Autonomous Vehicles},
	volume = {2020-Septe},
	doi = {10/gjjg3h},
	abstract = {Validation and verification of autonomous vehicles is still an unsolved problem. Although virtual approaches promise a cost efficient and reproducible solution, a most comprehensive and realistic representation of the real world traffic domain is required in order to make valuable statements about the performance of a highly automated driving ({HAD}) function. Models from different domain experts offer a repository of such representations. However, these models must be linked together for an extensive and uniform mapping of real world traffic domain for {HAD} performance assessment.Hereby, we propose the concept of a co-simulation architecture built upon the Robot Operating System ({ROS}) for both coupling and for integration of different domain expert models, immersion and stimulation of real pedestrians as well as {AD} systems into a common test system. This enables a unified way of generating ground truth for the performance assessment of multi-sensorial {AD} systems. We demonstrate the applicability of the {ROS} powered co-simulation by coupling behavior models in our mixed reality environment.},
	pages = {204--211},
	journaltitle = {{IEEE} International Conference on Multisensor Fusion and Integration for Intelligent Systems},
	author = {Zofka, Marc Rene and Tottel, Lars and Zipfl, Maximilian and Heinrich, Marc and Fleck, Tobias and Schulz, Patrick and Zollner, J. Marius},
	date = {2020},
	note = {{ISBN}: 9781728164229
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{zhao_accelerated_2017,
	title = {Accelerated Evaluation of Automated Vehicles Safety in Lane-Change Scenarios Based on Importance Sampling Techniques},
	volume = {18},
	issn = {15249050},
	doi = {10/f9gz4c},
	abstract = {Automated vehicles ({AVs}) must be thoroughly evaluated before their release and deployment. A widely used evaluation approach is the Naturalistic-Field Operational Test (N-{FOT}), which tests prototype vehicles directly on the public roads. Due to the low exposure to safety-critical scenarios, N-{FOTs} are time consuming and expensive to conduct. In this paper, we propose an accelerated evaluation approach for {AVs}. The results can be used to generate motions of the other primary vehicles to accelerate the verification of {AVs} in simulations and controlled experiments. Frontal collision due to unsafe cut-ins is the target crash type of this paper. Human-controlled vehicles making unsafe lane changes are modeled as the primary disturbance to {AVs} based on data collected by the University of Michigan Safety Pilot Model Deployment Program. The cut-in scenarios are generated based on skewed statistics of collected human driver behaviors, which generate risky testing scenarios while preserving the statistical information so that the safety benefits of {AVs} in nonaccelerated cases can be accurately estimated. The cross-entropy method is used to recursively search for the optimal skewing parameters. The frequencies of the occurrences of conflicts, crashes, and injuries are estimated for a modeled {AV}, and the achieved accelerated rate is around 2000 to 20 000. In other words, in the accelerated simulations, driving for 1000 miles will expose the {AV} with challenging scenarios that will take about 2 to 20 million miles of real-world driving to encounter. This technique thus has the potential to greatly reduce the development and validation time for {AVs}.},
	pages = {595--607},
	number = {3},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	author = {Zhao, Ding and Lam, Henry and Peng, Huei and Bao, Shan and {LeBlanc}, David J. and Nobukawa, Kazutoshi and Pan, Christopher S.},
	date = {2017},
	note = {Publisher: {IEEE}
134 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Active safety systems, automated vehicles ({AVs}), autonomous emergency braking ({AEB}), crash avoidance, importance sampling ({IS}), lane change},
}

@article{zhang_incident_2016,
	title = {An incident database for improving metro safety: The case of shanghai},
	volume = {84},
	issn = {18791042},
	url = {http://dx.doi.org/10.1016/j.ssci.2015.11.023},
	doi = {10/f79djg},
	abstract = {Large cities depend heavily on their metro systems to reduce traffic congestion, which is particularly the case with Shanghai, the largest and most developed city in China. For the purposes of enhancing the possibility in quantitative risk assessment and promoting the safety management level in Shanghai metro, an adaptable metro operation incident database ({MOID}) is therefore presented for containing details of all incidents that have occurred in metro operation. Taking compatibility and simplicity into consideration, Microsoft Access 2010 software is used for the comprehensive and thorough design of the {MOID}. Based on {MOID}, statistical characteristics of incident, such as types, causes, time, and severity, are discovered and 24 accident precursors are identified in Shanghai metro. The processes are demonstrated to show how the {MOID} can be used to identify trends in the incidents that have occurred and to anticipate and prevent future accidents. In order to promote the application of {MOID}, an organizational structure is proposed from the four aspects of supervision, research, implementation, and manufacturer. This research would be conducive to safety risk analysis in identifying relevant precursors in safety management and assessing safety level as a qualitative tool.},
	pages = {88--96},
	journaltitle = {Safety Science},
	author = {Zhang, Xiaoling and Deng, Yongliang and Li, Qiming and Skitmore, Martin and Zhou, Zhipeng},
	date = {2016},
	note = {Publisher: Elsevier Ltd
33 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Accident precursor, Database, Incident classification, Metro safety},
}

@article{zhang_roadview_2014,
	title = {{RoadView}: A traffic scene simulator for autonomous vehicle simulation testing},
	doi = {10/gjjrhd},
	abstract = {Simulation testing is an initial phase for the development of autonomous vehicle system. The developers can ensure the model they use behaves as close to the actual vehicle as possible in this cycle. This paper proposed a novel traffic scene modeling approach using image sequences and road {GIS} data. Such method was contributed to building a simulated environment for autonomous vehicle software systems testing and validating as well as its various algorithms. Based on the proposed approach, a traffic scene simulator, the {RoadView} is realized. Compared with the existed robot testing simulators such as the {USARSim} and the Gazebo, the {RoadView} can provide a more photorealistic scene. This simulator is employed to test performance of autonomous vehicle and evaluate its self-driving tasks. In such process the proposed approach is proved workable and effective for autonomous vehicle simulation testing.},
	pages = {1160--1165},
	issue = {c},
	journaltitle = {2014 17th {IEEE} International Conference on Intelligent Transportation Systems, {ITSC} 2014},
	author = {Zhang, Chi and Liu, Yuehu and Zhao, Danchen and Su, Yuanqi},
	date = {2014},
	note = {Publisher: {IEEE}
{ISBN}: 9781479960781
23 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{zenner_immersive_2020,
	title = {Immersive Process Model Exploration in Virtual Reality},
	volume = {26},
	issn = {19410506},
	doi = {10/gjjg3g},
	abstract = {In many professional domains, relevant processes are documented as abstract process models, such as event-driven process chains ({EPCs}). {EPCs} are traditionally visualized as 2D graphs and their size varies with the complexity of the process. While process modeling experts are used to interpreting complex 2D {EPCs}, in certain scenarios such as, for example, professional training or education, also novice users inexperienced in interpreting 2D {EPC} data are facing the challenge of learning and understanding complex process models. To communicate process knowledge in an effective yet motivating and interesting way, we propose a novel virtual reality ({VR}) interface for non-expert users. Our proposed system turns the exploration of arbitrarily complex {EPCs} into an interactive and multi-sensory {VR} experience. It automatically generates a virtual 3D environment from a process model and lets users explore processes through a combination of natural walking and teleportation. Our immersive interface leverages basic gamification in the form of a logical walkthrough mode to motivate users to interact with the virtual process. The generated user experience is entirely novel in the field of immersive data exploration and supported by a combination of visual, auditory, vibrotactile and passive haptic feedback. In a user study with N = 27 novice users, we evaluate the effect of our proposed system on process model understandability and user experience, while comparing it to a traditional 2D interface on a tablet device. The results indicate a tradeoff between efficiency and user interest as assessed by the {UEQ} novelty subscale, while no significant decrease in model understanding performance was found using the proposed {VR} interface. Our investigation highlights the potential of multi-sensory {VR} for less time-critical professional application domains, such as employee training, communication, education, and related scenarios focusing on user interest.},
	pages = {2104--2114},
	number = {5},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Zenner, André and Makhsadov, Akhmajon and Klingner, Sören and Liebemann, David and Krüger, Antonio},
	date = {2020},
	pmid = {32070982},
	note = {0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Virtual reality, business process models, immersion, immersive data analysis, multi-sensory feedback, passive haptics},
}

@article{zeng_dsdnet_2020,
	title = {{DSDNet}: Deep structured self-driving network},
	doi = {10/gjjg2x},
	abstract = {In this paper, we propose the Deep Structured self-Driving Network ({DSDNet}), which performs object detection, motion prediction, and motion planning with a single neural network. Towards this goal, we develop a deep structured energy based model which considers the interactions between actors and produces socially consistent multimodal future predictions. Furthermore, {DSDNet} explicitly exploits the predicted future distributions of actors to plan a safe maneuver by using a structured planning cost. Our sample-based formulation allows us to overcome the difficulty in probabilistic inference of continuous random variables. Experiments on a number of large-scale self driving datasets demonstrate that our model significantly outperforms the state-of-the-art.},
	journaltitle = {{arXiv}},
	author = {Zeng, Wenyuan and Wang, Shenlong and Liao, Renjie and Chen, Yun and Yang, Bin and Urtasun, Raquel},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2008.06041},
	note = {11 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous driving, Motion planning, Motion prediction},
}

@article{zhan_online_2020,
	title = {Online deep clustering for unsupervised representation learning},
	issn = {10636919},
	doi = {10/gg9942},
	abstract = {Joint clustering and feature learning methods have shown remarkable performance in unsupervised representation learning. However, the training schedule alternating between feature clustering and network parameters update leads to unstable learning of visual representations. To overcome this challenge, we propose Online Deep Clustering ({ODC}) that performs clustering and network update simultaneously rather than alternatingly. Our key insight is that the cluster centroids should evolve steadily in keeping the classifier stably updated. Specifically, we design and maintain two dynamic memory modules, i.e., samples memory to store samples' labels and features, and centroids memory for centroids evolution. We break down the abrupt global clustering into steady memory update and batch-wise label re-assignment. The process is integrated into network update iterations. In this way, labels and the network evolve shoulder-to-shoulder rather than alternatingly. Extensive experiments demonstrate that {ODC} stabilizes the training process and boosts the performance effectively.},
	pages = {6687--6696},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Zhan, Xiaohang and Xie, Jiahao and Liu, Ziwei and Ong, Yew Soon and Loy, Chen Change},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2006.10645},
	note = {19 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{zhang_machine_2019,
	title = {A Machine Learning-Based Defensive Alerting System against Reckless Driving in Vehicular Networks},
	volume = {68},
	issn = {19399359},
	doi = {10/gjjg2z},
	abstract = {Reckless driving severely threatens the safety of innocent people, which accounts for around 33\% of all fatalities in major vehicle accidents. However, most existing efforts focus on the detection and adjustment of a vehicle's own driving behavior, whose effectiveness is very limited. In this paper, we develop a defensive alerting system to detect and notify the threats posed by reckless vehicles. By integrating the computation capability of a cloud server with that of vehicles nowadays, we propose a cooperative driving performance rating ({CDPR}) mechanism to automatically and intelligently detect reckless vehicles based on machine learning algorithms. To support such a defensive alerting system, a three-tier system architecture is developed from existing vehicular networks, which consists of vehicles, road-side units ({RSU}) and a cloud server. Moreover, given the fact that most vehicles can be trusted to drive safely, to further reduce the transmission load of the {CDPR} mechanism, we design our scheme in such a way that every vehicle only uploads the data of driving maneuvers with reckless potential to {RSUs}. Based on the aggregated data, the cloud server globally rates every vehicle's driving performance by using support vector machine ({SVM}) and decision-tree machine learning models. We finally implement the proposed {CDPR} mechanism into a popular traffic simulator, Simulation of Urban {MObility} ({SUMO}), for evaluation. Simulation results illustrate that our defensive alerting system can accurately detect reckless vehicles and effectively provide timely alerts.},
	pages = {12227--12238},
	number = {12},
	journaltitle = {{IEEE} Transactions on Vehicular Technology},
	author = {Zhang, Lan and Yan, Li and Fang, Yuguang and Fang, Xuming and Huang, Xiaoxia},
	date = {2019},
	note = {3 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Cooperative driving behavior rating, Defensive reckless driving alert, Machine learning, {SUMO} simulation},
}

@article{vlachos_autonomous_2017,
	title = {Autonomous driving in 5G: Mitigating interference in {OFDM}-based vehicular communications},
	volume = {2017-June},
	issn = {23784873},
	doi = {10/gjjg2g},
	abstract = {Automotive industry will be greatly benefited by the advent of 5G Networking and the huge boost in performance and coverage it will support. Road safety and traffic efficiency services will be significantly upgraded through seamlessly interconnected devices, while latency decrease will most likely allow autonomous driving to become a commodity, available to everyone. This technology will have a huge societal and economical impact, since it will render severe traffic accidents, long commute times and increased energy consumption obsolete. Current vehicular communication systems are usually equipped with orthogonal frequency division multiplexing ({OFDM}) transceivers that operate in suboptimal modes for the upcoming 5G standards. The problem originates in the existing intercarrier interference ({ICI}) on the receiver end, often partially tackled by integrated successive interference cancellation ({OSIC}) architectures. However, for decreasing complexity, system designers attempt to mitigate {ICI} by considering only a small number or sub-carriers, leading to error floor introduction and unacceptable bit-error-rates ({BER}). This paper presents an {OSIC}-based solution for cancelling interference in {OFDM} systems operating over specific conditions such as doubly selective channels. The experimental results indicated that the proposed equalizer outperforms all existing non-banded {ICI} cancellation methods, by achieving lower {BER} despite operating in a resource-savvy manner.},
	journaltitle = {{IEEE} International Workshop on Computer Aided Modeling and Design of Communication Links and Networks, {CAMAD}},
	author = {Vlachos, Evangelos and Lalos, Aris S. and Berberidis, Kostas and Tselios, Christos},
	date = {2017},
	note = {{ISBN}: 9781509063024
10 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {5G, {ADV}, {ICI}, {OFDM}, {OSIC}, V2X},
}

@article{singh_dock_2018,
	title = {{DOCK}: Detecting objects by transferring common-sense knowledge},
	volume = {11217 {LNCS}},
	issn = {23318422},
	doi = {10/gjjgz8},
	abstract = {We present a scalable approach for Detecting Objects by transferring Common-sense Knowledge ({DOCK}) from source to target categories. In our setting, the training data for the source categories have bounding box annotations, while those for the target categories only have image-level annotations. Current state-of-the-art approaches focus on image-level visual or semantic similarity to adapt a detector trained on the source categories to the new target categories. In contrast, our key idea is to (i) use similarity not at the image-level, but rather at the region-level, and (ii) leverage richer common-sense (based on attribute, spatial, etc.) to guide the algorithm towards learning the correct detections. We acquire such common-sense cues automatically from readily-available knowledge bases without any extra human effort. On the challenging {MS} {COCO} dataset, we find that common-sense knowledge can substantially improve detection performance over existing transfer-learning baselines.},
	pages = {1--17},
	issue = {i},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Singh, Krishna Kumar and Divvala, Santosh and Farhadi, Ali and Lee, Yong Jae},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1804.01077},
	note = {{ISBN}: 9783030012601
12 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{ultsch_integration_1995,
	title = {Integration of neural networks with knowledge-based systems},
	volume = {4},
	doi = {10/bnp4cd},
	abstract = {Existing prejudices of some Artificial Intelligence researchers against neural networks are hard to break. One of their most important argument is that neural networks are not able to explain their decisions. Further they claim that neural networks are not able so solve the variable binding problem for unification. We show in this paper that neural networks and knowledge-based systems must not be competitive, but are capable to complete each other. The disadvantages of the one paradigm are the advantages of the other and vice versa. We show several ways to integrate both paradigms in the areas of explorative data analysis, knowledge acquisition, introspection, and unification. Our approach to such hybrid systems has been proved in real world applications.},
	pages = {1828--1833},
	journaltitle = {{IEEE} International Conference on Neural Networks - Conference Proceedings},
	author = {Ultsch, Alfred and Korus, Dieter},
	date = {1995},
	note = {18 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{van_den_berg_lqg-mp_2011,
	title = {{LQG}-{MP}: Optimized path planning for robots with motion uncertainty and imperfect state information},
	volume = {6},
	issn = {2330765X},
	doi = {10/gjjg2m},
	abstract = {This paper presents {LQG}-{MP} (linear-quadratic Gaussian motion planning), a new approach to robot motion planning that takes into account the sensors and the controller that will be used during execution of the robot's path. {LQG}-{MP} is based on the linear-quadratic controller with Gaussian models of uncertainty, and explicitly characterizes in advance (i.e., before execution) the a-priori probability distributions of the state of the robot along its path. These distributions can be used to assess the quality of the path, for instance by computing the probability of avoiding collisions. Many methods can be used to generate the needed ensemble of candidate paths from which the best path is selected; in this paper we report results using the {RRT}-algorithm. We study the performance of {LQG}-{MP} with simulation experiments in three scenarios involving a kinodynamic car-like robot, multi-robot planning with differential-drive robots, and a 6-{DOF} manipulator.},
	pages = {129--136},
	journaltitle = {Robotics: Science and Systems},
	author = {Van Den Berg, Jur and Abbeel, Pieter and Goldberg, Ken},
	date = {2011},
	note = {{ISBN}: 9780262516815
192 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{sontges_worst-case_2018,
	title = {Worst-case Analysis of the Time-To-React Using Reachable Sets},
	volume = {2018-June},
	doi = {10/gjjg3d},
	abstract = {Collision mitigation and collision avoidance systems in intelligent vehicles reduce the severity and number of accidents. To determine the optimal point in time at which such systems should intervene, time-based criticality metrics such as the Time-To-React ({TTR}) are commonly used. The {TTR} describes the last point in time along the current trajectory at which an evasive trajectory exists. In this paper, we present a novel approach to determine the point in time after which it is guaranteed that no evasive maneuver exists, i.e., by using reachable sets, we over-approximate the {TTR}. Our deterministic upper bound of the {TTR} can be used to trigger a collision mitigation system or to find a feasible emergency maneuver which avoids the collision. We demonstrate the efficient computation of the tight over-approximated {TTR} in different urban and rural traffic scenarios, and compare our results to an estimated {TTR} using an optimization-based trajectory planner.},
	pages = {1891--1897},
	journaltitle = {{IEEE} Intelligent Vehicles Symposium, Proceedings},
	author = {Sontges, Sebastian and Koschi, Markus and Althoff, Matthias},
	date = {2018},
	note = {{ISBN}: 9781538644522
7 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{tanprasert_combining_2017,
	title = {Combining Unsupervised Anomaly Detection and Neural Networks for Driver Identification},
	volume = {2017},
	issn = {20423195},
	doi = {10/gjjg2c},
	abstract = {This paper proposes an algorithm for real-time driver identification using the combination of unsupervised anomaly detection and neural networks. The proposed algorithm uses nonphysiological signals as input, namely, driving behavior signals from inertial sensors (e.g., accelerometers) and geolocation signals from {GPS} sensors. First anomaly detection is performed to assess if the current driver is whom he/she claims to be. If an anomaly is detected, the algorithm proceeds to find relevant features in the input signals and use neural networks to identify drivers. To assess the proposed algorithm, real-world data are collected from ten drivers who drive different vehicles on several routes in real-world traffic conditions. Driver identification is performed on each of the seven-second-long driving behavior signals and geolocation signals in a streaming manner. It is shown that the proposed algorithm can achieve relatively high accuracy and identify drivers within 13 seconds. The proposed algorithm also outperforms the previously proposed driver identification algorithms. Furthermore, to demonstrate how the proposed algorithm can be deployed in real-world applications, results from real-world data associated with each operation of the proposed algorithm are shown step-by-step.},
	journaltitle = {Journal of Advanced Transportation},
	author = {Tanprasert, Thitaree and Saiprasert, Chalermpol and Thajchayapong, Suttipong},
	date = {2017},
	note = {7 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{baker_journal_2009,
	title = {Journal of the Association for Information Systems Using Visual Representations of Data to Enhance Sensemaking in Data Exploration Tasks *},
	volume = {10},
	doi = {10/ggc5tc},
	pages = {533--560},
	number = {7},
	journaltitle = {Journal of the Association for Information Systems},
	author = {Baker, Jeff},
	date = {2009},
	keywords = {Information visualization, visual representation,, visual representation},
}

@article{stephanidis_user_2014,
	title = {User Experience Observations on Factors That Affect Performance in a Road-Crossing Training Application for Children Using the {CAVE}},
	volume = {8524 {LNCS}},
	issn = {16113349},
	doi = {10/gjjrhg},
	issue = {{PART} 2},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Stephanidis, Constantine},
	date = {2014},
	note = {{ISBN}: 9783319074849
20 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{saffarzadeh_general_2013,
	title = {A general formulation for time-to-collision safety indicator},
	volume = {166},
	issn = {0965092X},
	doi = {10/f5bbvt},
	abstract = {Time to collision is an important time-based safety indicator for detecting rear-end conflicts in traffic safety evaluations. A major weakness of the time to collision notion is the assumption of constant velocities during the course of an accident. In this paper we utilise equations of motion to develop a generalised formulation for time to collision by relaxing the assumption of constant velocity, constant acceleration and so on. This paper also illustrates how this concept can be applied to real-world data, and detailed data gathered in the {NGSIM} project are used. Then time to collision is calculated based on the assumption of constant speed, constant acceleration and linear acceleration for leading and following vehicles. Results indicate that in the third case (linear acceleration), the average duration of exposure to critical time to collision values is greater than the others. So, applying time to collision based on the assumption of linear acceleration in collision avoidance systems would decrease driver errors more than other cases.},
	pages = {294--304},
	number = {5},
	journaltitle = {Proceedings of the Institution of Civil Engineers: Transport},
	author = {Saffarzadeh, Mahmoud and Nadimi, Navid and Naseralavi, Saber and Mamdoohi, Amir Reza},
	date = {2013},
	note = {23 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Hazards/traffic engineering, Mathematical modelling/safety},
}

@article{rosique_systematic_2019,
	title = {A systematic review of perception system and simulators for autonomous vehicles research},
	volume = {19},
	issn = {14248220},
	doi = {10/ggb48s},
	abstract = {This paper presents a systematic review of the perception systems and simulators for autonomous vehicles ({AV}). This work has been divided into three parts. In the first part, perception systems are categorized as environment perception systems and positioning estimation systems. The paper presents the physical fundamentals, principle functioning, and electromagnetic spectrum used to operate the most common sensors used in perception systems (ultrasonic, {RADAR}, {LiDAR}, cameras, {IMU}, {GNSS}, {RTK}, etc.). Furthermore, their strengths and weaknesses are shown, and the quantification of their features using spider charts will allow proper selection of different sensors depending on 11 features. In the second part, the main elements to be taken into account in the simulation of a perception system of an {AV} are presented. For this purpose, the paper describes simulators for model-based development, the main game engines that can be used for simulation, simulators from the robotics field, and lastly simulators used specifically for {AV}. Finally, the current state of regulations that are being applied in different countries around the world on issues concerning the implementation of autonomous vehicles is presented.},
	pages = {1--29},
	number = {3},
	journaltitle = {Sensors (Switzerland)},
	author = {Rosique, Francisca and Navarro, Pedro J. and Fernández, Carlos and Padilla, Antonio},
	date = {2019},
	pmid = {30764486},
	note = {67 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous vehicle, {LiDAR}, Model based design, Perception system, Simulator},
}

@article{mutzenich_updating_2021,
	title = {Updating our understanding of situation awareness in relation to remote operators of autonomous vehicles},
	volume = {6},
	issn = {23657464},
	url = {https://doi.org/10.1186/s41235-021-00271-8},
	doi = {10/gjjgzk},
	abstract = {The introduction of autonomous vehicles ({AVs}) could prevent many accidents attributable to human driver error. However, even entirely driverless vehicles will sometimes require remote human intervention. Current taxonomies of automated driving do not acknowledge the possibility of remote control of {AVs} or the challenges that are unique to such a driver in charge of a vehicle that they are not physically occupying. Yet there are significant differences between situation awareness ({SA}) in normal driving contexts and {SA} in these remote driving operations. We argue that the established understanding of automated driving requires updating to include the context of remote operation that is likely to come in to play at higher levels of automation. It is imperative to integrate the role of the remote operator within industry standard taxonomies, so that regulatory frameworks can be established with regards to the training required for remote operation, the necessary equipment and technology, and a comprehensive inventory of the use cases under which we could expect remote operation to be carried out. We emphasise the importance of designing control interfaces in a way that will maximise remote operator ({RO}) {SA} and we identify some principles for designing systems aimed at increasing an {RO}’s sense of embodiment in the {AV} that requires temporary control.},
	number = {1},
	journaltitle = {Cognitive Research: Principles and Implications},
	author = {Mutzenich, Clare and Durant, Szonya and Helman, Shaun and Dalton, Polly},
	date = {2021},
	note = {Publisher: Springer International Publishing
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{raissi_autonomous_2019,
	title = {Autonomous Cars, 5G Mobile Networks and Smart Cities: Beyond the Hype},
	doi = {10/gjjgzt},
	abstract = {This paper proposes one more slice of the next-generation self-driving automobile. It introduces an innovative and smart architecture that enables collaborative interactions between the autonomous cars and the urban devices available in the city. The aim of this architecture is to achieve the smart city vision where the different involved entities - such as cars, homes, citizens - exchange useful information and collaborate to reach common goals like decrease the traffic congestion. The proposed architecture sits at edges of 5G mobile telecommunication network and relies on 5G key enablers such as multi-access edge computing. A realistic use case that illustrates information exchange between an autonomous car and neighboring devices during a trip is provided. As for validation, a prototype implementing the use case was developed and evaluated. The lessons learned from the performed experiments are discussed.},
	pages = {180--185},
	journaltitle = {Proceedings - 2019 {IEEE} 28th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises, {WETICE} 2019},
	author = {Raissi, Fatma and Yangui, Sami and Camps, Frederic},
	date = {2019},
	note = {Publisher: {IEEE}
{ISBN}: 9781728106762
6 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Autonomous car, {MEC}, Presence server, {SIP}},
}

@article{ramachandra_survey_2020,
	title = {A Survey of Single-Scene Video Anomaly Detection},
	issn = {19393539},
	doi = {10/gjjgzv},
	abstract = {This survey article summarizes research trends on the topic of anomaly detection in video feeds of a single scene. We discuss the various problem formulations, publicly available datasets and evaluation criteria. We categorize and situate past research into an intuitive taxonomy and provide a comprehensive comparison of the accuracy of many algorithms on standard test sets. Finally, we also provide best practices and suggest some possible directions for future research.},
	pages = {1--20},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ramachandra, Bharathkumar and Jones, Michael and Vatsavai, Ranga Raju},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2004.05993},
	note = {2 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {abnormal event detection, surveillance, video anomaly detection},
}

@article{sadat_jointly_2019,
	title = {Jointly Learnable Behavior and Trajectory Planning for Self-Driving Vehicles},
	issn = {21530866},
	doi = {10/gjjgzx},
	abstract = {The motion planners used in self-driving vehicles need to generate trajectories that are safe, comfortable, and obey the traffic rules. This is usually achieved by two modules: behavior planner, which handles high-level decisions and produces a coarse trajectory, and trajectory planner that generates a smooth, feasible trajectory for the duration of the planning horizon. These planners, however, are typically developed separately, and changes in the behavior planner might affect the trajectory planner in unexpected ways. Furthermore, the final trajectory outputted by the trajectory planner might differ significantly from the one generated by the behavior planner, as they do not share the same objective. In this paper, we propose a jointly learnable behavior and trajectory planner. Unlike most existing learnable motion planners that address either only behavior planning, or use an uninterpretable neural network to represent the entire logic from sensors to driving commands, our approach features an interpretable cost function on top of perception, prediction and vehicle dynamics, and a joint learning algorithm that learns a shared cost function employed by our behavior and trajectory components. Experiments on real-world self-driving data demonstrate that jointly learned planner performs significantly better in terms of both similarity to human driving and other safety metrics, compared to baselines that do not adopt joint behavior and trajectory learning.},
	pages = {3949--3956},
	journaltitle = {{IEEE} International Conference on Intelligent Robots and Systems},
	author = {Sadat, Abbas and Ren, Mengye and Pokrovsky, Andrei and Lin, Yen Chen and Yumer, Ersin and Urtasun, Raquel},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1910.04586},
	note = {{ISBN}: 9781728140049
20 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{neumeier_teleoperation_2019,
	title = {Teleoperation: The Holy Grail to solve problems of automated driving? Sure, but latency matters},
	doi = {10/gjjgzm},
	abstract = {In the domain of automated driving, numerous (technological) problems were solved in recent years, but still many limitations are around that could eventually prevent the deployment of automated driving systems ({ADS}) beyond {SAE} level 3. A remote operating fallback authority might be a promising solution. In order for teleoperation to function reliably and universal, it will make use of existing infrastructure, such as cellular networks. Unfortunately, cellular networks might suffer from variable performance. In this work, we investigate the effects of latency on task performance and perceived workload for different driving scenarios. Results from a simulator study (N=28) suggest that latency has negative influence on driving performance and subjective factors and led to a decreased confidence in Teleoperated Driving during the study. A latency of about 300 ms already led to a deteriorated driving performance, whereas variable latency did not consequently deteriorate driving performance.},
	pages = {186--197},
	issue = {September},
	journaltitle = {Proceedings - 11th International {ACM} Conference on Automotive User Interfaces and Interactive Vehicular Applications, {AutomotiveUI} 2019},
	author = {Neumeier, Stefan and Wintersberger, Philipp and Frison, Anna Katharina and Becher, Armin and Facchi, Christian and Riener, Andreas},
	date = {2019},
	note = {{ISBN}: 9781450368841
12 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Latency, Qos, Teleoperated driving, User study},
}

@article{ochocki_demo_2016,
	title = {Demo: Communication requirements of {CACC} for high-density platooning},
	volume = {0},
	issn = {21579865},
	doi = {10/gjjgzn},
	abstract = {Platooning is one of the most relevant Vehicle-to-Everything (V2X) services which has attracted attention of both academia and industry. High-density platooning has the potential to reduce fuel consumption due to the reduced air drag and to increase the capacity of the roads due to the smoother traffic flow. The aim of this demo is to illustrate the benefits of Vehicle-to-Vehicle (V2V) communication for high-density car platooning. For this purpose, the demo compares Adaptive Cruise Control ({ACC}), which relies on on-board sensors only, and Cooperative Adaptive Cruise Control ({CACC}), which relies both on sensor measurements and periodic exchange of Cooperative Awareness Message ({CAM}) messages among vehicles. During the demo presentation, audience can configure {ACC} and {CACC} parameters and observe results of platoon simulation (e.g. average inter-car distance) in real-time. The demo illustrates 5G V2V communication requirements for high-density platooning by showing the impact of {CAM} message latency and reliability on {CACC} performance.},
	pages = {31--32},
	journaltitle = {{IEEE} Vehicular Networking Conference, {VNC}},
	author = {Ochocki, Marcin and Vukadinovic, Vladimir and Januszewski, Maciej and De La Iglesia, Idoia},
	date = {2016},
	note = {Publisher: {IEEE}
{ISBN}: 9781509051977
3 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{nienhuser_relevance_2011,
	title = {Relevance estimation of traffic elements using Markov logic networks},
	doi = {10/frnn49},
	abstract = {Complex traffic situations e.g. at intersections consist of many traffic participants, traffic elements and relations between them. The behavior of participants is constrained by implicit and explicit traffic rules. We are interested in estimating whether a given traffic element - a traffic sign, a traffic light - is relevant in the current driving situation, i.e. affects the set of possible legal actions. A wide variety of properties influences the relevance. The route to take for example affects which traffic lights are relevant and the current weather situation affects whether a speed limit restricted by a supplementary sign is in effect. We use first-order logic to model such relations and apply reasoning to decide upon the relevance of static traffic elements. The need for perfect information is alleviated with the help of Markov logic networks, reconciling hard decision rules on the one hand and uncertainty intrinsic to the environment perception process on the other hand. The evaluation of twelve intersection scenes shows very promising results for the relevance estimation of traffic lights: Markov logic networks are able to judge whether enough information is available and determine the relevant traffic lights reliably in such cases. © 2011 {IEEE}.},
	pages = {1659--1664},
	journaltitle = {{IEEE} Conference on Intelligent Transportation Systems, Proceedings, {ITSC}},
	author = {Nienhüser, Dennis and Gumpp, Thomas and Zöllner, J. Marius},
	date = {2011},
	note = {{ISBN}: 9781457721984
14 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{neumann_hybrid_2006,
	title = {A Hybrid Machine Learning Approach for Information Extraction from Free Text},
	doi = {10/dp86nc},
	abstract = {We present a hybrid machine learning approach for information extraction from unstructured documents by integrating a learned classifier based on the Maximum Entropy Modeling ({MEM}), and a classifier based on our work on Data-Oriented Parsing ({DOP}). The hybrid behavior is achieved through a voting mechanism applied by an iterative tag-insertion algorithm. We have tested the method on a corpus of German newspaper articles about company turnover, and achieved 85.2\% F-measure using the hybrid approach, compared to 79.3\% for {MEM} and 51.9\% for {DOP} when running them in isolation.},
	pages = {390--397},
	journaltitle = {From Data and Information Analysis to Knowledge Engineering},
	author = {Neumann, Günter},
	date = {2006},
	note = {4 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{okechukwu_understanding_2011,
	title = {Understanding Virtual Reality Technology: Advances and Applications},
	doi = {10/gjjg3f},
	abstract = {Virtual Reality ({VR}) is not an entirely new concept; it has existed in various forms since the late 1960s. It has been known by names such as synthetic environment, cyberspace, artificial reality, simulator technology and so on and so forth before {VR} was eventually adopted. The latest manifestation of {VR} is desktop {VR}. Desktop {VR} is also known by other names such as Window on World ({WoW}) or non-immersive {VR} (Onyesolu, 2006). As a result of proliferation of desktop {VR}, the technology has continued to develop applications that are less than fully immersive. These non-immersive {VR} applications are far less expensive and technically daunting and have made inroads into industry training and development. {VR} has perhaps at last come within the realm of possibility for general creation and use most especially in education where computer-based virtual learning environments ({VLE}) are packaged as desktop {VR}. This, in turn, points the way for its inclusion in educational programs (Ausburn \& Ausburn, 2004). These computer-based virtual learning environments ({VLEs}) have opened new realms in the teaching, learning, and practice of medicine, physical sciences and engineering among others. {VLEs} provide students with the opportunity to achieve learning goals. {VLE}-based applications have thus emerged in mainstream education in schools and universities as successful tools to supplement traditional teaching methods. These learning environments have been discovered to have greater pedagogical effectiveness on learners. Virtual learning environments provide three-dimensional (3D) insights into the structures and functions of any system desired. Students can thereby learn the principles of such system in a fast, effective and pleasurable way by interacting with and navigating through the environment created for such system (Onyesolu, 2009a; Onyesolu, 2009b). It is known that {VR} can make the artificial as realistic as, and even more realistic than, the real (Negroponte, 1995).},
	issue = {June 2015},
	journaltitle = {Advances in Computer Science and Engineering},
	author = {Okechukwu, Moses and Udoka, Felista},
	date = {2011},
	note = {32 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{ok_path_2013,
	title = {Path planning with uncertainty: Voronoi Uncertainty Fields},
	issn = {10504729},
	doi = {10/gf2987},
	abstract = {In this paper, a two-level path planning algorithm that deals with map uncertainty is proposed. The higher level planner uses modified generalized Voronoi diagrams to guarantee finding a connected path from the start to the goal if a collision-free path exists. The lower level planner considers uncertainty of the observed obstacles in the environment and assigns repulsive forces based on their distance to the robot and their positional uncertainty. The attractive forces from the Voronoi nodes and the repulsive forces from the uncertainty-biased potential fields form a hybrid planner we call Voronoi Uncertainty Fields ({VUF}). The proposed planner has two strong properties: (1) bias against uncertain obstacles, and (2) completeness. We analytically prove the properties and run simulations to validate our method in a forest-like environment. © 2013 {IEEE}.},
	pages = {4596--4601},
	journaltitle = {Proceedings - {IEEE} International Conference on Robotics and Automation},
	author = {Ok, Kyel and Ansari, Sameer and Gallagher, Billy and Sica, William and Dellaert, Frank and Stilman, Mike},
	date = {2013},
	note = {{ISBN}: 9781467356411
37 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Collision Avoidance, Motion and Path Planning, {SLAM}},
}

@article{park_machine_2019,
	title = {Machine learning vs. hybrid machine learning model for optimal operation of a chiller},
	volume = {25},
	issn = {2374474X},
	url = {https://doi.org/10.1080/23744731.2018.1510270},
	doi = {10/gjjgzp},
	abstract = {This article compares two modeling approaches for optimal operation of a turbo chiller installed in an office building: (1) a machine learning model developed with artificial neural network ({ANN}) and (2) a hybrid machine learning model developed with the {ANN} model and available physical knowledge of the chiller. Before developing the {ANN} model of the chiller, the authors used Gaussian mixture model in order to check the validity of measured data. Then, the hybrid model was developed by combining the {ANN} model and physics-based regression equations from the {EnergyPlus} engineering reference. It was found that both the {ANN} and hybrid {ANN} model are satisfactory to predict the chiller’s power consumption: mean bias error ({MBE}) = −2.63\%, coefficient of variation of the root mean square error ({CVRMSE}) = 8.05\% by the {ANN} model; {MBE} = −3.99\%, {CVRMSE} = 11.98\% by the hybrid {ANN} model. However, the hybrid model requires fewer inputs (four inputs) than the {ANN} model (eight inputs). The energy savings of both models are similar coefficient of performance ({COP}) = 4.32 by the optimal operation of the {ANN} model; {COP} = 4.44 by the optimal operation of the hybrid {ANN} model. In addition, the hybrid {ANN} model can be applied where the {ANN} model is unable to provide accurate predictions.},
	pages = {209--220},
	number = {2},
	journaltitle = {Science and Technology for the Built Environment},
	author = {Park, Sung Ho and Ahn, Ki Uhn and Hwang, Seungho and Choi, Sunkyu and Park, Cheol Soo},
	date = {2019},
	note = {Publisher: Taylor \& Francis
2 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{rose_reinforcement_2020,
	title = {A reinforcement learning approach to rare trajectory sampling},
	issn = {23318422},
	doi = {10/gjjgzw},
	abstract = {Very often when studying non-equilibrium systems one is interested in analysing dynamical behaviour that occurs with very low probability, so called rare events. In practice, since rare events are by definition atypical, they are often difficult to access in a statistically significant way. What are required are strategies to “make rare events typical” so that they can be generated on demand. Here we present such a general approach to adaptively construct a dynamics that efficiently samples atypical events. We do so by exploiting the methods of reinforcement learning ({RL}), which refers to the set of machine learning techniques aimed at finding the optimal behaviour to maximise a reward associated with the dynamics. We consider the general perspective of dynamical trajectory ensembles, whereby rare events are described in terms of ensemble reweighting. By minimising the distance between a reweighted ensemble and that of a suitably parametrised controlled dynamics we arrive at a set of methods similar to those of {RL} to numerically approximate the optimal dynamics that realises the rare behaviour of interest. As simple illustrations we consider in detail the problem of excursions of a random walker, for the case of rare events with a finite time horizon; and the problem of a studying current statistics of a particle hopping in a ring geometry, for the case of an infinite time horizon. We discuss natural extensions of the ideas presented here, including to continuous-time Markov systems, first passage time problems and non-Markovian dynamics.},
	journaltitle = {{arXiv}},
	author = {Rose, Dominic C. and Mair, Jamie F. and Garrahan, Juan P.},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2005.12890},
	note = {2 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{soldati_sphere_2007,
	title = {Sphere viz - Data exploration in a virtual reality environment},
	issn = {10939547},
	doi = {10/b7jhcf},
	abstract = {We present {SphereViz}, a novel 3D user interface for the visual exploration of multi-dimensional data sets in virtual reality environments. {SphereViz} builds on known visualization and search concepts like {RadViz} and {RelevanceSphere}. It combines them with 3D-interaction techniques like World in Miniature for projection in virtual environments. A prototype implementation of {SphereViz} allows to study, on one hand, the visualization methods of images in 3D space, and on the other hand, intuitive search methods and adequate interaction techniques. © 2007 {IEEE}.},
	pages = {680--683},
	journaltitle = {Proceedings of the International Conference on Information Visualisation},
	author = {Soldati, Marco and Doulis, Mario and Csillaghy, André},
	date = {2007},
	note = {{ISBN}: 0769529003
7 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{pavlitskaya_using_2020,
	title = {Using mixture of expert models to gain insights into semantic segmentation},
	volume = {2020-June},
	issn = {21607516},
	doi = {10/gjjgzq},
	abstract = {Not only correct scene understanding, but also ability to understand the decision making process of neural networks is essential for safe autonomous driving. Current work mainly focuses on uncertainty measures, often based on Monte Carlo dropout, to gain at least some insight into a models confidence. We investigate a mixture of experts architecture to achieve additional interpretability while retaining comparable result quality.By being able to use both the overall model output as well as retaining the possibility to take into account individual expert outputs, the agreement or disagreement between those individual outputs can be used to gain insights into the decision process. Expert networks are trained by splitting the input data into semantic subsets, e.g. corresponding to different driving scenarios, to become experts in those domains. An additional gating network that is also trained on the same input data is consequently used to weight the output of individual experts. We evaluate this mixture of expert setup on the A2D2 dataset and achieve similar results to a baseline {FRRN} network trained on all available data, while getting additional information.},
	pages = {1399--1406},
	journaltitle = {{IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	author = {Pavlitskaya, Svetlana and Hubschneider, Christian and Weber, Michael and Moritz, Ruby and Huger, Fabian and Schlicht, Peter and Zollner, J. Marius},
	date = {2020},
	note = {{ISBN}: 9781728193601
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{park_semantic_2019,
	title = {Semantic image synthesis with spatially-adaptive normalization},
	volume = {2019-June},
	issn = {10636919},
	doi = {10/gg9hw6},
	abstract = {We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. Instead, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. Experiments on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. Code is available upon publication.},
	pages = {2332--2341},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Park, Taesung and Liu, Ming Yu and Wang, Ting Chun and Zhu, Jun Yan},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1903.07291},
	note = {{ISBN}: 9781728132938
583 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Computational Photography, Deep Learning, Image and Video Synthesis, Vision + Graphics},
}

@article{von_rueden_combining_2020,
	title = {Combining Machine Learning and Simulation to a Hybrid Modelling Approach: Current and Future Directions},
	volume = {12080 {LNCS}},
	issn = {16113349},
	doi = {10/gjjg2h},
	abstract = {In this paper, we describe the combination of machine learning and simulation towards a hybrid modelling approach. Such a combination of data-based and knowledge-based modelling is motivated by applications that are partly based on causal relationships, while other effects result from hidden dependencies that are represented in huge amounts of data. Our aim is to bridge the knowledge gap between the two individual communities from machine learning and simulation to promote the development of hybrid systems. We present a conceptual framework that helps to identify potential combined approaches and employ it to give a structured overview of different types of combinations using exemplary approaches of simulation-assisted machine learning and machine-learning assisted simulation. We also discuss an advanced pairing in the context of Industry 4.0 where we see particular further potential for hybrid systems.},
	pages = {548--560},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {von Rueden, Laura and Mayer, Sebastian and Sifa, Rafet and Bauckhage, Christian and Garcke, Jochen},
	date = {2020},
	note = {{ISBN}: 9783030445836
3 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Hybrid approaches, Machine learning, Simulation},
}

@article{st_lawrence_video_2020,
	title = {Video from user-generated content as a source of pre-crash scenario naturalistic driving data},
	volume = {0},
	issn = {1538957X},
	url = {https://doi.org/10.1080/15389588.2020.1829920},
	doi = {10/gjjgz9},
	abstract = {Objective: The objective of this study was to investigate the use of public video from internet user-generated content as a means of collecting naturalistic driving data. Methods: A convenience sample of 38 videos comprised of 203 events was extracted from publicly available channels on the {YouTube}™ platform. Each event was manually reviewed and pseudo-coded according to a subset of current {CRSS} variables. Pre-crash scenarios were coded using categories developed for prior {NHTSA} analysis. Results: Crashes represented 67\% of the reviewed cases. Collisions with motor vehicles accounted for 84\% of all crashes in the sample. Pre-crash scenarios were able to be determined for all crashes and near-crashes. The most prevalent pre-crash scenario types in the video data were Crossing Paths (41\%), Rear End (21\%), and Lane Change (17\%). The top pre-crash scenarios from Swanson et al., were Rear End (31\%), Crossing Paths (21\%), and Lane Change (12\%). The most prevalent pre-near crash scenario types in the video data were Crossing Paths (32\%), Lane Change (30\%), and Pedestrian (12\%). Conclusions: The most prevalent pre-crash scenarios in the video data were similar to those in data from {FARS} and {NASS}-{GES}. Though not nationally representative, this preliminary study demonstrated that user-generated content may be useful as a source of inexpensive naturalistic data and provides sufficient detail to capture important pre-crash, near-crash and crash information.},
	pages = {1--3},
	number = {0},
	journaltitle = {Traffic Injury Prevention},
	author = {St. Lawrence, Schuyler and Hallman, Jason and Sherony, Rini},
	date = {2020},
	note = {Publisher: Taylor \& Francis
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
	keywords = {Naturalistic driving, pre-crash scenarios, user-generated content, video data},
}

@article{zhao_online_2011,
	title = {Online detection of unusual events in videos via dynamic sparse coding},
	issn = {10636919},
	doi = {10/fqvft2},
	abstract = {Real-time unusual event detection in video stream has been a difficult challenge due to the lack of sufficient training information, volatility of the definitions for both normality and abnormality, time constraints, and statistical limitation of the fitness of any parametric models. We propose a fully unsupervised dynamic sparse coding approach for detecting unusual events in videos based on online sparse re-constructibility of query signals from an atomically learned event dictionary, which forms a sparse coding bases. Based on an intuition that usual events in a video are more likely to be reconstructible from an event dictionary, whereas unusual events are not, our algorithm employs a principled convex optimization formulation that allows both a sparse reconstruction code, and an online dictionary to be jointly inferred and updated. Our algorithm is completely un-supervised, making no prior assumptions of what unusual events may look like and the settings of the cameras. The fact that the bases dictionary is updated in an online fashion as the algorithm observes more data, avoids any issues with concept drift. Experimental results on hours of real world surveillance video and several Youtube videos show that the proposed algorithm could reliably locate the unusual events in the video sequence, outperforming the current state-of-the-art methods. © 2011 {IEEE}.},
	pages = {3313--3320},
	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Zhao, Bin and Fei-Fei, Li and Xing, Eric P.},
	date = {2011},
	note = {{ISBN}: 9781457703942
354 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@inproceedings{koduri_aureate_2018,
	title = {{AUREATE}: An Augmented Reality Test Environment for Realistic Simulations},
	rights = {All rights reserved},
	doi = {10/gjjg29},
	abstract = {Automated driving is currently one of the most active areas of research worldwide. While the general progress in developing specific algorithms for perception, planning and control tasks is very advanced, testing and validation of the resulting functions is still challenging due to the large number of possible scenarios and generation of ground-truth. Currently, real world testing and simulations are used in combination to overcome some of these challenges. While real world testing does not suffer from imperfect sensor models and environments, it is expensive, slow and not accurately repeatable and therefore unable to capture all possible scenarios. However, simulation models are not sophisticated enough to fully replace real world testing. In this paper, we propose a workflow that is capable of augmenting real sensor-level data with simulated sensor data. With this approach we are able to generate scenarios which are as realistic as possible while also being flexible with the ability to insert arbitrary objects. This sensor-level based approach enables testing of the whole algorithm chain for automated driving, including perception, object-detection, scene understanding, path planning, decision making, and control.},
	booktitle = {{SAE} Technical Papers},
	author = {Koduri, Tejaswi and Bogdoll, Daniel and Paudel, Shreyasha and Sholingar, Gautham},
	date = {2018},
	note = {{ISSN}: 01487191
0 citations (Semantic Scholar/{DOI}) [2021-03-25]},
}

@article{technical_solution_working_group_wg2_driving_2019,
	title = {Driving Data to the Edge: The Challenge of Traffic Distribution},
	journaltitle = {Aecc},
	author = {{Technical Solution Working Group (WG2).}},
	date = {2019},
}

@article{nagel_methods_2001,
	title = {Methods for Visual Mining of Data in Virtual Reality},
	abstract = {Recent advances in technology have made it possible to use 3-D Virtual Reality for Visual Data Mining. This paper presents a mod-ular system architecture with a series of tools for explorative analysis of large data sets in Virtual Reality. A 3-D Scatter Plot tool is extended to become an " Object Property Space " , where data records are visualized as objects with as many statistical variables as possible represented as object properties like shape, color, etc. A working hypothesis is that the free and real-time navigation of the observer in the immersive virtual space will support the chances of finding interesting data structures and relationships. The system is now ready to be used for experiments to validate the hypothesis.},
	pages = {13--27},
	issue = {October},
	journaltitle = {Proceedings of the International Workshop on Visual Data Mining},
	author = {Nagel, Henrik R and Granum, Erik and Musaeus, Peter},
	date = {2001},
	pmid = {19615404},
	note = {{ISBN}: 1873-7528 (Electronic)},
	keywords = {data exploration, perception, visualization},
}

@article{milovanovic_vision_2020,
	title = {Vision Guidance for Autonomous Vehicles : Comparative Study Of 3D Video Vision Guidance for Autonomous Vehicles : Comparative Study Of 3D Video and Point Cloud},
	issue = {January},
	author = {Milovanović, Dragorad and Kukolj, Dragan and Nemet, Sandra},
	date = {2020},
	keywords = {3d video, a trajectory, autonomous driving, finally, how it generates its, or in other words, path planning is how, point cloud, route to follow, the car plans the, the control system uses, they will follow, trajectory},
}

@article{aecc_operational_2020,
	title = {Operational Behavior of a High Definition Map Application White Paper},
	author = {{AECC}},
	date = {2020},
}

@article{aecc_general_2018,
	title = {General Principle and Vision},
	pages = {1--15},
	author = {{AECC}},
	date = {2018},
}

@article{guan_uncertainty-aware_2021,
	title = {Uncertainty-Aware Unsupervised Domain Adaptation in Object Detection},
	url = {http://arxiv.org/abs/2103.00236},
	abstract = {Unsupervised domain adaptive object detection aims to adapt detectors from a labelled source domain to an unlabelled target domain. Most existing works take a two-stage strategy that first generates region proposals and then detects objects of interest, where adversarial learning is widely adopted to mitigate the inter-domain discrepancy in both stages. However, adversarial learning may impair the alignment of well-aligned samples as it merely aligns the global distributions across domains. To address this issue, we design an uncertainty-aware domain adaptation network ({UaDAN}) that introduces conditional adversarial learning to align well-aligned and poorly-aligned samples separately in different manners. Specifically, we design an uncertainty metric that assesses the alignment of each sample and adjusts the strength of adversarial learning for well-aligned and poorly-aligned samples adaptively. In addition, we exploit the uncertainty metric to achieve curriculum learning that first performs easier image-level alignment and then more difficult instance-level alignment progressively. Extensive experiments over four challenging domain adaptive object detection datasets show that {UaDAN} achieves superior performance as compared with state-of-the-art methods.},
	pages = {1--12},
	author = {Guan, Dayan and Huang, Jiaxing and Xiao, Aoran and Lu, Shijian and Cao, Yanpeng},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.00236},
	note = {1 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
}

@article{yang_instance_2021,
	title = {Instance Localization for Self-supervised Detection Pretraining},
	url = {http://arxiv.org/abs/2102.08318},
	abstract = {Prior research on self-supervised learning has led to considerable progress on image classification, but often with degraded transfer performance on object detection. The objective of this paper is to advance self-supervised pretrained models specifically for object detection. Based on the inherent difference between classification and detection, we propose a new self-supervised pretext task, called instance localization. Image instances are pasted at various locations and scales onto background images. The pretext task is to predict the instance category given the composited images as well as the foreground bounding boxes. We show that integration of bounding boxes into pretraining promotes better task alignment and architecture alignment for transfer learning. In addition, we propose an augmentation method on the bounding boxes to further enhance the feature alignment. As a result, our model becomes weaker at Imagenet semantic classification but stronger at image patch localization, with an overall stronger pretrained model for object detection. Experimental results demonstrate that our approach yields state-of-the-art transfer learning results for object detection on {PASCAL} {VOC} and {MSCOCO}.},
	pages = {1--10},
	author = {Yang, Ceyuan and Wu, Zhirong and Zhou, Bolei and Lin, Stephen},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.08318},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
}

@article{varischio_hybrid_2021,
	title = {Hybrid Point Cloud Semantic Compression for Automotive Sensors: A Performance Evaluation},
	url = {http://arxiv.org/abs/2103.03819},
	abstract = {In a fully autonomous driving framework, where vehicles operate without human intervention, information sharing plays a fundamental role. In this context, new network solutions have to be designed to handle the large volumes of data generated by the rich sensor suite of the cars in a reliable and efficient way. Among all the possible sensors, Light Detection and Ranging ({LiDAR}) can produce an accurate 3D point cloud representation of the surrounding environment, which in turn generates high data rates. For this reason, efficient point cloud compression is paramount to alleviate the burden of data transmission over bandwidth-constrained channels and to facilitate real-time communications. In this paper, we propose a pipeline to efficiently compress {LiDAR} observations in an automotive scenario. First, we leverage the capabilities of {RangeNet}++, a Deep Neural Network ({DNN}) used to semantically infer point labels, to reduce the channel load by selecting the most valuable environmental data to be disseminated. Second, we compress the selected points using Draco, a 3D compression algorithm which is able to obtain compression up to the quantization error. Our experiments, validated on the Semantic {KITTI} dataset, demonstrate that it is possible to compress and send the information at the frame rate of the {LiDAR}, thus achieving real-time performance.},
	issue = {Icc},
	author = {Varischio, Andrea and Mandruzzato, Francesco and Bullo, Marcello and Giordani, Marco and Testolina, Paolo and Zorzi, Michele},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.03819},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
}

@article{saha_self-supervised_2021,
	title = {Self-supervised Multisensor Change Detection},
	url = {http://arxiv.org/abs/2103.05102},
	abstract = {Multimodal and multisensor data analysis is a long-standing goal in machine learning research. In this paper we revisit multisensor analysis in context of self-supervised change detection in bi-temporal satellite images. Most change detection methods assume that pre-change and post-change images are acquired by the same sensor. However, in many real-life scenarios, e.g., natural disaster, it is more practical to use the latest available images before and after the occurrence of incidence, which may be acquired using different sensors. In particular, we are interested in the combination of the images acquired by optical and Synthetic Aperture Radar ({SAR}) sensors. While optical images are like the natural images dealt in computer vision, {SAR} images appear vastly different even when capturing the same scene. Adding to this, change detection methods are often constrained to use only target image-pair, no labeled data, and no additional unlabeled data. Such constraints limit the scope of traditional supervised machine learning and unsupervised generative approaches for multi-sensor change detection. Recent rapid development of self-supervised learning methods has shown that some of them can even work with only few images. Motivated by this, in this work we propose a method for multi-sensor change detection using only the unlabeled target bi-temporal images that are used for training a network in self-supervised fashion by using deep clustering and contrastive learning. The trained network is evaluated on multi-modal satellite data showing change and the benefits of our self-supervised approach are demonstrated.},
	pages = {1--8},
	author = {Saha, Sudipan and Ebel, Patrick and Zhu, Xiao Xiang},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2103.05102},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
}

@article{van_bekkum_modular_2021,
	title = {Modular Design Patterns for Hybrid Learning and Reasoning Systems: a taxonomy, patterns and use cases},
	url = {http://arxiv.org/abs/2102.11965},
	abstract = {The unification of statistical (data-driven) and symbolic (knowledge-driven) methods is widely recognised as one of the key challenges of modern {AI}. Recent years have seen large number of publications on such hybrid neuro-symbolic {AI} systems. That rapidly growing literature is highly diverse and mostly empirical, and is lacking a unifying view of the large variety of these hybrid systems. In this paper we analyse a large body of recent literature and we propose a set of modular design patterns for such hybrid, neuro-symbolic systems. We are able to describe the architecture of a very large number of hybrid systems by composing only a small set of elementary patterns as building blocks. The main contributions of this paper are: 1) a taxonomically organised vocabulary to describe both processes and data structures used in hybrid systems; 2) a set of 15+ design patterns for hybrid {AI} systems, organised in a set of elementary patterns and a set of compositional patterns; 3) an application of these design patterns in two realistic use-cases for hybrid {AI} systems. Our patterns reveal similarities between systems that were not recognised until now. Finally, our design patterns extend and refine Kautz' earlier attempt at categorising neuro-symbolic architectures.},
	pages = {1--20},
	author = {van Bekkum, Michael and de Boer, Maaike and van Harmelen, Frank and Meyer-Vitali, André and Teije, Annette ten},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2102.11965},
	note = {0 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
}

@article{chai_multipath_2019,
	title = {{MultiPath}: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction},
	url = {http://arxiv.org/abs/1910.05449},
	abstract = {Predicting human behavior is a difficult and crucial task required for motion planning. It is challenging in large part due to the highly uncertain and multi-modal set of possible outcomes in real-world domains such as autonomous driving. Beyond single {MAP} trajectory prediction, obtaining an accurate probability distribution of the future is an area of active interest. We present {MultiPath}, which leverages a fixed set of future state-sequence anchors that correspond to modes of the trajectory distribution. At inference, our model predicts a discrete distribution over the anchors and, for each anchor, regresses offsets from anchor waypoints along with uncertainties, yielding a Gaussian mixture at each time step. Our model is efficient, requiring only one forward inference pass to obtain multi-modal future distributions, and the output is parametric, allowing compact communication and analytical probabilistic queries. We show on several datasets that our model achieves more accurate predictions, and compared to sampling baselines, does so with an order of magnitude fewer trajectories.},
	issue = {{CoRL}},
	author = {Chai, Yuning and Sapp, Benjamin and Bansal, Mayank and Anguelov, Dragomir},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1910.05449},
	note = {100 citations (Semantic Scholar/{arXiv}) [2021-03-25]},
}

@article{waymo_waymo_2020,
	title = {Waymo Safety Report Our Mission},
	issue = {September},
	author = {{Waymo}},
	date = {2020},
}

@article{scanlon_waymo_2021,
	title = {Waymo Simulated Driving Behavior in Reconstructed Fatal Crashes within an Autonomous Vehicle Operating Domain},
	pages = {1--24},
	author = {Scanlon, John M and Kusano, Kristofer D and Daniel, Tom and Alderson, Christopher and Ogle, Alexander and Victor, Trent},
	date = {2021},
}

@article{bergenhem_overview_2012,
	title = {{OVERVIEW} {OF} {PLATOONING} {SYSTEMS}},
	pages = {1--7},
	issue = {October},
	author = {Bergenhem, Carl and Pettersson, Henrik and Coelingh, Erik and Englund, Cristofer and Shladover, Steven and Tsugawa, Sadayuki},
	date = {2012},
	keywords = {cooperative traffic system, energy its, etsi its-g5, gcdc, highway automation, its, path, platooning, road-train, sartre, vehicle communication, vehicle to},
}

@article{gsma_connecting_2019,
	title = {Connecting Vehicles today and in the 5G era with C-V2X (Cellular Vehicle-to-everything)},
	url = {www.gsma.com/automotive},
	author = {{GSMA}},
	date = {2019},
}

@article{abdi_simultaneous_2020,
	title = {{SIMULTANEOUS} {CLASSIFICATION} {AND} {OUT}-{OF}-{DISTRIBUTION} {DETECTION} {USING} {DEEP} {NEURAL} {NETWORKS}},
	volume = {7},
	pages = {1--8},
	number = {6},
	author = {Abdi, Farhan Douksieh and Wenjuan, Lian},
	date = {2020},
}

@article{liu_open_2020,
	title = {Open Compound Domain Adaptation},
	pages = {12406--12415},
	author = {Liu, Ziwei and Miao, Zhongqi and Zhan, Xiaohang and Lin, Dahua and Yu, Stella X and Icsi, U C Berkeley},
	date = {2020},
}

@article{gonzalez_self-supervised_2021,
	title = {Self-supervised out-of-distribution detection for Cardiac {MR} segmentation},
	pages = {1--12},
	author = {Gonzalez, Camila},
	date = {2021},
	keywords = {distribution shift, out-of-distribution detection, self-supervision},
}
@article{karl_communication_2018,
	title = {{COMMUNICATION} {WITH} {HUMAN} {MOTIVATION} : How solving the Image Captioning-Retrieval problem creates conversations},
	issue = {Cv},
	author = {Karl, Fabian Alexander},
	date = {2018},
}

@article{laine_generative_2021,
	title = {{GENERATIVE} {ADVERSARIAL} {NEURAL} {NETWORK} {ASSISTED} {VIDEO} {COMPRESSION} {AND} {BROADCAST}},
	volume = {2021},
	author = {Laine, Samuli Matias and Fi, Vantaa and Luebke, David Patrick},
	date = {2021},
}

@article{robotics_defense_2021,
	title = {In Defense of Knowledge Distillation for Task Incremental Learning and its Application in 3D Object Detection},
	pages = {1--8},
	issue = {February},
	author = {Robotics, Ieee},
	date = {2021},
}

@article{koopman_edge_2019,
	title = {Edge Cases and Autonomous Vehicle Safety},
	issue = {February},
	author = {Koopman, Philip},
	date = {2019},
}

@article{hoffmann_corner_2020,
	title = {Corner Cases und ihre Tücken},
	pages = {24--27},
	author = {Hoffmann, M and Pohl, A and Mlynarski, P Prill U N D M and G, D and Paradigmen, Die and Software, Von and Einsatz, Den and Intelligenz, Von Künstlicher and Beispiel, Das},
	date = {2020},
}

@article{nahas_improving_2018,
	title = {Improving the Behavior of a Distributed Adaptive Cruise Control System by Reducing Transmission Jitter American Journal of Engineering Research ( {AJER} ) Open Access Improving the Behavior of a Distributed Adaptive Cruise Control System by Reducing Transmi},
	pages = {106--112},
	issue = {May},
	author = {Nahas, Mouaaz},
	date = {2018},
}

@article{arquitectura_genetic_2015,
	title = {Genetic Algorithm for Dynamic Path Planning},
	volume = {53},
	issn = {1098-6596},
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent {MM}- {GBSA} calculations. Using the best {RMSD} among the top 10 scoring poses as a metric, the success rate ({RMSD} ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide {SP} settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta {FlexPepDock} method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	pages = {1689--1699},
	number = {9},
	journaltitle = {Acta Universitatis Agriculturae et Silviculturae Mendelianae Brunensis},
	author = {Arquitectura, Energía Y and Introducci, Tulo I and {赫晓霞} and Iv, Tulo and Teatinas, L A S and Conclusiones, Tulo V I I and Contemporáneo, Perspectivas D E U S O and Evaluaci, Tulo V and Ai, Farshad and Jakubiec, John Alstan and Weeks, Digital Photography COMPLETE COURSE Learn Everithing You Need to Know in 20 and Mu, Ari and Inan, Tuǧba and Sierra Garriga, Carlos and Library, Pao Yue-kong and Hom, Hung and Kong, Hong and Castilla, Nuria and Uzaimi, Achmad and Febriand Abdel, Jack and Armaidah, Rita and Por, Paso and Para, Paso and Pol, Responsables and Julio, Ticos and La, Antecedentes and Herder, De and Olbina, Svetlana and Valerio Ubierna, Ignacio and Hafiz, Dalia O and Jones, James R and Gibbons, Ronald B and Schubert, Robert and Araji, Mohamad T and رسولی, چاپار and Sky, Date and Mbe, H P R and Mbe, Perez and Rmse, H P R and Rmse, Perez and Calculo, Práctica and Phantom, D E L and Toma, Toma and Media, Toma and Bertolotti, Dimas and Karmarkar, Bharati and Mu, Ari and Jacobsson, Emma and Eriksson, Fredrik and Pereira, Alvaro Luis dos Santos and Amaral, G. and Bushee, J. and Cordani, U. G. and KAWASHITA, KOJI and Reynolds, J. H. and ALMEIDA, Fernando FLÁVIO MARQUES De E and de Almeida, F. F.M. and Hasui, Y. and de Brito Neves, B. B. and Fuck, R. A. and Oldenzaal, Zorgcentrum and Guida, Aimée and Tchalenko, J S and Peacock, David C.P. and Sanderson, David J. and Rotevatn, Atle and Nixon, C. W. and Rotevatn, Atle and Sanderson, David J. and Zuluaga, L. F. and Dimmen, V. and Rotevatn, Atle and Sanderson, David J. and {1} and Peacock, David C.P. and Nixon, C. W. and Rotevatn, Atle and Sanderson, David J. and Zuluaga, L. F. and Crider, Juliet G. and Tjia, H D and Kim, Young Seog and Peacock, David C.P. and Sanderson, David J. and Katz, Yoram and Weinberger, Ram and Aydin, Atilla and Peacock, David C.P. and Marrett, Randall and Peacock, David C.P. and Doblas, Miguel and Petit, J. P. and Hancock, P. L. and Barka, A. A. and Barron, K and Paulista, Universidade Estadual and Em, Programa D E Pós-graduação and Biológicas, Ciências and Souza, Iata Anderson De E and Viana, Adilson and Júnior, Soares and {Jacques Angelier} and Angelier, Jacques and Mechler, P. and Burini, Arthur and Marginal, Brazilian and Clemente, Pilar and Beasley, Craig J and Dribus, John R and Mawson, By Nicola and Tubbs, Bonnie and Fugita, A. M. and Jos, Mareio and Magalh, Jullano and {ANP - Agência Nacional do Petróleo} and Garcia, Sávio Francis de Melo and Filho, André Danderfer Antonio Thomaz and Lamotte, Dominique Frizon De and Rudkiewicz, Jean Luc and Lima, João Victor and Mohriak, Webster Ueipass and Pereira, Márcio J and Moreira, Jobel Lourenço Pinheiro and Valdetaro, Claudio and Gil, Joâo Alexandre and Machado, Marco Antônio Pinheiro and Souza, Iata Anderson De E and Dirk, Hans and Castro, Joel Carneiro De and De Geociências, Instituto and De Pós-Graduação, Programa and Geociências, Em and Balbinot, Mariana and Da Bacia De Santos, Cretáceo-Terciário and Chang, Hung Kiang and Matos, Gabriel Correa De and Lima, Rodrigo Dias and Santos, Bacia De E and Haupert, Isabelle and Manatschal, Gianreto and Decarlis, Alessandro and Unternehr, Patrick and Macedo, Juliano Magalhães and Corrêa, Fernando Santos and Alberto, Gustavo and Jorge, Correia and Corrêa, Rui and Bueno, Gilmar Vital and A. C. Vidal, C. H. Kiang, F. S. Correa, F. L. Fernandes, J. C. de Castro, J. S. Tinen, L. Koike, M. L. Assine {and} S. P. Rostirolla and Moreira, Jobel Lourenço Pinheiro and Esteves, Carlos Augusto and Rodrigues, José Joaquim and Vasconcelos, Claudemir Severiano de and De Mio, Eduardo and Chang, Hung Kiang and Corrêa, Fernando Santos and Santos, Bacia De E and Barroso, Alberto Silva and Tarcísio, Celso and Silva, De Souza Da and Carlos, Luiz Luis and Pires, Gomes L.C. and Damasceno, Luis Carlos and Filoco, Paulo Roberto and Assine, Mario Luis Mário Luis and Corrêa, Fernando Santos and Chang, Hung Kiang and Santos, Bacia De E and Balbinot, Mariana and Santos, Bacia De E and Dolares, Maria and Carvalho, De and Praça, Uyara Mundlm and De, Josê Jorge and Vincentelli, Maria Gabriela C and Barbosa, Mauro and Karam*, Márcia Kuhn and Porto, Roberto and Saito, Makoto and Contreras, Jorham and Zühlke, Rainer and Bowman, Scott and Bechstädt, Thilo and Moreira, Jobel Lourenço Pinheiro and Carminatti, Mário and Chang, Hung Kiang and Assine, Mario Luis Mário Luis and Corrêa, Fernando Santos and Tinen, Julio Setsuo and Vidal, Alexandre Campane and Koike, Luzia and Garcia, Sávio Francis de Melo and Letouzey, Jean and Rudkiewicz, Jean Luc and Danderfer Filho, André and Frizon de Lamotte, Dominique and {Hung Kiang Chang} and Kowsmann, Renato Oscar and Figueiredo, Antonio M.G. Manuel Ferreira and Bender, AndréAdriano and Zalán, Pedro Victor and Paulo, Universidade D E S Ã O A O and N, Publicação Especial and Ulbrich, HORSTPETER H. G. J. and Miranda, Carlos Eduardo and Aslanian, Daniel and Moulin, Maryline and Olivet, Jean Louis and Unternehr, Patrick and Matias, Luis and Bache, François and Rabineau, Marina and Nouzé, Hervé and Klingelheofer, Frauke and Contrucci, Isabelle and Labails, Cinthia and Fluminense, Universidade Federal and Blaich, Olav A. and Faleide, Jan Inge and Tsikalas, Filippos and Gordon, A. C. and Mohriak, Webster Ueipass and Brune, Sascha and Williams, Simon E. and Dietmar Müller, R. and Cobbold, Peter R. and Davy, P. and Gapais, D. and Rossello, E. A. and Sadybakasov, E. and Thomas, J. C. and Tondji Biyo, J. J. and de Urreiztieta, M. and Meisling, Kristian E. and Cobbold, Peter R. and Mount, Van S. and Suria, Chandra and Legg, Christopher and Gordon, Stuart and Crawford, Mike and He, Zhiyong and Elliot, Ed and Quo, Lih and Chiossi, Dario and Green, Paul F and Japsen, Peter and Bonow, Johan and Cogné, Nathan and Gallagher, Kerry and Cobbold, Peter R. and Riccomini, CLÁUDIO Claudio and Gautheron, Cecile and Strozyk, Frank and Back, Stefan and Kukla, Peter A. and Mohriak, Webster Ueipass and Bassetto, Marcelo and Vieira, Ines Santos and Stanton, Natasha and Schmitt, Renata da Silva and Galdeano, A. and Maia, Márcia and Mane, M. A. and Dehler, Nolan Maia and Magnavita, Luciano Portugal and Gomes, Leonardo Correa Lucy and Rigoti, Caesar Augusto and de Oliveira, João Alberto Bach and Sant'Anna, Marília Vidigal and da Costa, Felipe Garcia Domingues and Gallagher, Kerry and Hawkesworth, Chris J. and Mantovani, Marta S.M. M.S. and Svartman Dias, Anna Eliza and Hayman, Nicholas W. and Lavier, Luc L. and Direen, Nicholas G. and Stagg, Howard M.J. and Symonds, Philip A. and Norton, Ian O. and Evain, M and Afilhado, A and Rigoti, Caesar Augusto and Loureiro, A and Alves, D and Klingelhoefer, Frauke and Feld, A and Fuck, R. A. and Soares, J and Evain, M and Afilhado, A and Rigoti, Caesar Augusto and Loureiro, A and Alves, D and Brenner, Thomas and Ferrari, ANDRÉ LUIZ and Penha, Hélio and Silva, A and Alves, E C and Franke, Dieter and Gallagher, Kerry and Hawkesworth, Chris J. and Mantovani, Marta S.M. M.S. and Garcia, Sávio Francis de Melo and Letouzey, Jean and Rudkiewicz, Jean Luc and Danderfer Filho, André and Frizon de Lamotte, Dominique and Gomes, Paulo Otávio and Kilsdonk, Bill and Grow, Tim and Minken, Jon and Barragan, Roberto and Heine, C. and Zoethout, J. and Müller, R. Dietmar and Souza, Iata Anderson De E and Kellogg, J. N. and Mohriak, Webster Ueipass and Kvarven, Trond and Mjelde, Rolf and Hjelstuen, Berit Oline and Faleide, Jan Inge and Thybo, Hans and Flueh, Ernst R. and Murai, Yoshio and Lavier, Luc L. and Manatschal, Gianreto and Cobbold, Peter R. and Meisling, Kristian E. and Mount, Van S. and Souza, Pricilla Camões Martins de and Schmitt, Renata da Silva and Stanton, Natasha and Lima, João Victor and Mohriak, Webster Ueipass and Paula, Osni B. and Leroy, Sylvie and Rosendahl, B. R. and Nemčok, Michal and Enciso, G. and Fainstein, Roberto and Moulin, Maryline and Aslanian, Daniel and Rabineau, Marina and Patriat, Martin and Matias, Luis and Olivet, Jean Louis and Contrucci, Isabelle and Matias, Luis and Géli, Louis and Klingelhoefer, Frauke and Nouzé, Hervé and Réhault, Jean Pierre and Unternehr, Patrick and Riccomini, CLÁUDIO Claudio and Peloggia, A. U.G. and Saloni, J. C.L. and Kohnke, M. W. and Figueira, R. M. and Nemčok, Michal and Henk, A. and Allen, R. and Sikora, P. J. and Stuart, C. J. and Rosendahl, B. R. and Welker, C. and Smith, S and Nemčok, Michal and Sinha, Sudipta T. and Stuart, C. J. and Welker, C. and Choudhuri, M. and Sharma, S. P. and Misra, Achyuta Ayan and Sinha, N. and Venkatraman, S. and Pérez-Gussinye, Marta and Richetti, P. C. and Schmitt, Renata da Silva and Reeves, C. and Clerc, Camille and Ringenbach, Jean Claude and Jolivet, Laurent and Ballard, Jean François and Savastano, Vítor Lamy Mesiano and Schmitt, Renata da Silva and Araújo, Mário Neto Cavalcanti de and Inocêncio, Leonardo Campos and Scotchman, I. C. and Gilchrist, G. and Kusznir, N. J. and Roberts, A. M. and Fletcher, R. and Sibuet, Jean Claude and Tucholke, Brian E. and Souza, Pricilla Camões Martins de and Schmitt, Renata da Silva and Stanton, Natasha and Ponte-Neto, Cosme and Bijani, Rodrigo and Masini, Emmanuel and Fontes, S. and Flexor, J. M. and Ponte-Neto, Cosme and Bijani, Rodrigo and Ibanez, Delano M. and Pestilho, André L.S. and Turra, Bruno B. and Destro, Nivaldo and Miranda, Fernando P. and Riccomini, CLÁUDIO Claudio and Lammoglia, Talita and Dubois, Daniel S. and Schmidt, Jaques S. and Cogné, Nathan and Cobbold, Peter R. and Riccomini, CLÁUDIO Claudio and Gallagher, Kerry and Stica, Juliano M and Zalán, Pedro Victor and Ferrari, ANDRÉ LUIZ and Sant'Anna, L. G. and Riccomini, CLÁUDIO Claudio and Rodrigues-Francisco, B. H. and Sial, Alcides Nobrega and Carvalho, M. D. and Moura, C. A.V. and Gomes, Paulo Otávio and Kilsdonk, Bill and Minken, Jon and Grow, Tim and Barragan, Roberto and Parry, Jonathan and Martins, Wisley and Engelmann de Oliveira, Christie Helouise and Jelinek, Andréa Ritter and Chemale, Farid and Cupertino, José Antônio and THOMAZ FILHO, Antonio ANTÔNIO and MIZUSAKI, Ana Maria Pimentel and MILANI, EDISON JOSE and Cesero, PEDRO DE and Ussami, Naomi and Chaves, Carlos Alberto Moreno and Marques, Leila Soares and Ernesto, Marcia and Heidbach, O. and Rajabi, M. and Reiter, K. and Ziegler, M. and Cordani, Renato and Shukowsky, Wladimir and Agarwal, V K and Huang, C H and Physics, Engineering and Clemson, J. and Cartwright, J. and Booth, J. and Daly, M. C. and Chorowicz, J. and Fairhead, J. D. and I, John a Dunbar and Sawyer, Dale S and Gibson, G. M. and Totterdell, J. M. and White, L. T. and Mitchell, C. H. and Stacey, A. R. and Morse, M. P. and Whitaker, A. and Modisi, M. P. and Atekwana, E. A. and Kampunzu, A. B. and Ngwisanyi, T. H. and Moulin, Maryline and Aslanian, Daniel and Scheck, Magdalena and Bayer, Ulf and Otto, Volker and Lamarche, Juliette and Banka, Dirk and Pharaoh, T. Tim and van Wijk, J. W. and Ashby, D and MACHADO JÚNIOR, DELZIO DE LIMA and Srivastava, Rajesh K. and ALMEIDA, Fernando FLÁVIO MARQUES De E and Carneiro, CELSO DAL RÉ and Bellieni, Giuliano and Comín, P and Marques, Leila Soares and Melfi, A. J. and Piccirillo, Enzo M. and Nardy, Antonio José Ranalli and Rosemberg, A and McNulty, Judiann and Clark, David A. and Coelho, Raphael Martins and Chaves, Alexandre de Oliveira and Buckingham, David and Sefton-Green, Julian and Vianna Coutinho, José Moacyr and Almeida, Julio C.H. Júlio Julio and Dios, Fatima and Mohriak, Webster Ueipass and Valeriano, Claudio de Morisson CLÁUDIO DE MORISSON Claudio De Morisson and Heilbron, MONICA and Eirado, Luiz Guilherme and Tomazzoli, Edison Ramos and Wl, G and Cordani, U. G. and Kawaseita{\textasciitilde}, K and Reynolds{\textasciitilde}, J H and Brazil, S\&o Paulo and Bennio, L. and Brotzu, P. and D'Antonio, M. and Feraud, G. and Gomes, Celso B. de Barros and Marzoli, Andrea and Melluso, Leone and Morbidelli, L. and Morra, Vincenzo and Rapaille, Cedric and Ruberti, Excelso and SOUZA, B. T. de and Carvas, Karine Zuccolan and Chaves, Alexandre de Oliveira and de Oliveira Chaves, Alexandre and Neves, José Luiz Peixoto Marques Correia and Em, Mestrado and Pública, Saúde and Paper, Conference and Tupinamb, Miguel and Nordeste, Setores Centro-norte E and Enxame, D O and Da, D E Diques and Corval, Artur and Valente, S.C. Sérgio de Castro Sergio de Castro and Duarte, Beatriz Paschoal B.P. and Famelli, N. and Zanon, M. and Deckart, Katja and Féraud, Gilbert and Marques, Leila Soares and Bertrand, Hervé and Pós-graduação, Programa D E and Sciences, Atmospheric and State, North Carolina and Park, Menlo and Garda, Gianna Maria and Schorscher, Johann Hans Daniel and Do, Universidade and Do, Estado and Janeiro, Rio I O De E and Geologia, Faculdade D E and Guedes, Eliane and Heilbron, MONICA and Vasconcelos, Paulo M. and de Morisson Valeriano, Cláudio Claudio and Horta de Almeida, Júlio César and Teixeira, Wilson and Filho, André Danderfer Antonio Thomaz and {关华} and Guedes, Eliane and Heilbron, MONICA and de Morisson Valeriano, Cláudio Claudio and de Almeida, Julio Cesar Horta César Horta Cesar Horta and Szatmari, Peter and Magalhães, Joana and Matos, Lara and Nursalam, 2016, metode penelitian and Bhiwapurkar, Nitin and Rathi, Manoj and Mohan, Ned and NGONGE, EMMANUEL DONALD and Pires, Paulo Roberto and Geologia, Dissertação De Mestrado and Tomba, C.L.B. and Trotta, Mario Cesar and Turner, Simon P. and Regelous, Marcel and Kelley, Simon and Hawkesworth, Chris J. and Mantovani, Marta S.M. M.S. and Lobo, Janaína Teixeira and Duarte, Beatriz Paschoal B.P. and Szatmari, Peter and Valente, S.C. Sérgio de Castro Sergio de Castro and Corval, Artur and Duarte, Beatriz Paschoal B.P. and Ellam, Robert M. and Fallick, Anthony E. and Meighan, Ian Gordon and Dutra, Thiago and Ernesto, Marcia and Raposo, Maria I.B. Irene Bartolomeu and Marques, Leila Soares and Renne, Paul Randall and Diogo, L. A. and De Min, Angelo and Florisbal, Luana Moreira and Heaman, Larry M. and de Assis Janasi, Valdecir and de Fatima Bitencourt, Maria and Hergt, J. M. and Peate, David W. and Hawkesworth, Chris J. and Hou, Guiting and Jourdan, Fred and Féraud, Gilbert and Bertrand, Hervé and Watkeys, M. K. and Kampunzu, A. B. and Le Gall, B. and Kouamo, Noël Aimée Keutchafo and Tchaptchet, Depesquidoux Tchato and Ngueguim, Anne Laure Tezanou and Wambo, Nicole Armelle Simeni and Tchouankoue, Jean Pierre and Cucciniello, Ciro and Lare, C O C and Reland, I and Mcnamara, Maria E and Machado, Fábio Braz and Rocha-Júnior, Eduardo Reis Viana R.V. and Marques, Leila Soares and Nardy, Antonio José Ranalli and Zezzo, Larissa Vieira and Marteleto, Natasha Sarde and Ernesto, Marcia and Marteleto, Natasha Sarde and Mcmaster, Michael and Almeida, Julio C.H. Júlio Julio and Heilbron, MONICA and Guedes, Eliane and Mane, M. A. and Linus, J. H. and Motoki, Akihisa and Sichel, Susanna Eleonora and Petrakis, Giannis Hans and Renne, Paul Randall and Glen, Jonathon M Jonathan M. and Milner, Simon C. and Duncan, Andrew R. and Hall, Walton and Keynes, Milton and Peate, David W. and Urubici, Y and Gramado, Y and Raposo, Maria I.B. Irene Bartolomeu and Ernesto, Marcia and Renne, Paul Randall and Santiago, Raíssa and Caxito, Fabrício de Andrade and Neves, Mirna Aparecida and Dantas, Elton Luiz and Medeiros Júnior, Edgar Batista de and Queiroga, Gláucia Nascimento and Renne, Paul Randall and Ernesto, Marcia and Pacca, G and Coe, Robert S and Glen, Jonathon M Jonathan M. and Prevot, Michel and Perrin, Mireille and Deckart, Katja and Ernesto, Marcia and Féraud, Gilbert and Piccirillo, Enzo M. and Santos, Adjacente C. À Bacia D E and Brasil, S- S E D O and Agashev, A M and Takazawa, Eiichi and Albaréde, F and Modeling, Geochemical and York, New and Chester, Port and Pyle, David and Kumar, Santosh and Stewart, Kathy and Turner, Simon P. and Kelley, Simon and Hawkesworth, Chris J. and Kirstein, Linda A. and Mantovani, Marta S.M. M.S. and Thiede, David S. and Vasconcelos, Paulo M. and Vasconcelos, David L. and Bezerra, Francisco H.R. and Clausen, Ole R. and Medeiros, Walter E. and de Castro, David L. and Vital, Helenice and Barbosa, José A. Roberto and White, R. and McKenzie, D. and Will, Thomas M. and Frimmel, Hartwig E. and Alves, Costa F. and Ferrari, ANDRÉ LUIZ and Júnior, Cleber Peralta Gomes and Fossen, Haakon and Jackson, Chris and Rotevatn, Atle and Preto, Federal De Ouro and Zerfass, Henrique and Chemale Junior, Farid and Paulo Otávio Gomes, Bill Kilsdonk Jon Minken Tim Grow {and} Roberto Barragan and Granot, Roi and Dyment, JérÔme and Hartwig, Marcos Eduardo and Heilbron, MONICA and Machado, NUNO and Valeriano, Claudio de Morisson CLÁUDIO DE MORISSON Claudio De Morisson and Valladares, CLÁUDIA SAYÃO Claudia and Machado, NUNO and Pedrosa-soares, Antônio Carlos and Carlos, Luiz Luis and Allard, Rudolph and Trouw, Johannes and Mohriak, Webster Ueipass and Valerianol, M and Milani, J and Almeida, Julio C.H. Júlio Julio and Tupinambfil, M and Petreo, P and Tupinambá, Miguel and Valeriano, Claudio de Morisson CLÁUDIO DE MORISSON Claudio De Morisson and Armstrong, Richard and Do Eirado Siva, Luiz Guilherme and Melo, Renata Seibel and Simonetti, Antonio and Pedrosa Soares, Antonio Carlos and Machado, NUNO and Heine, C. and Zoethout, J. and Müller, R. Dietmar and Caldeira, Jefter and Kirstein, Linda A. and Kelley, Simon and Hawkesworth, Chris J. and Turner, Simon P. and Mantovani, Marta S.M. M.S. and Wijbrans, Jan and Kumar, Naresh and Danforth, A. Al and Nuttall, P. and Helwig, J. and Bird, D. E. and Venkatraman, S. and Acharya, Tapas and Nag, Sisir Kanti and Basumallik, Sukumar and Almeida, Julio C.H. Júlio Julio and Heilbron, MONICA and Valeriano, Claudio de Morisson CLÁUDIO DE MORISSON Claudio De Morisson and Acharya, Tapas and Mallik, Sukumar Basu and Ectono, E Volução T and Do, Strutural and Aréu, C Ampo D E X and Boyer, Robert E and Mcqueen, Jereld E and Morelli, M. and Piana, F. and Coward, M. P. and Daly, M. C. and Conceição, Rommulo Vieira Raimundo Almir Costa da and Ferreira, Joaquim M. and Bezerra, Francisco H.R. and Sousa, Maria O.L. and do Nascimento, Aderson F. and Sá, Jaziel M. and França, George S. and {شجاعی، رؤیا} and Gontijo-Pascutti, Ambrosina Helena Ferreira and Cesar, Julio and Almeida, Horta De and Bezerra, Francisco H.R. and Terra, Emanuele La and Almeida, Julio C.H. Júlio Julio and Hobbs, W. H. and Hodgson, Robert A and Jacques, Patricia Duringer and Machado, Rômulo and Nummer, Alexis Rosa and De Oliveira, Roberto Gusmão and Ferreira, Francisco José Fonseca and De Castro, Luís Gustavo and Nummer, Alexis Rosa and Justo, A N A Paula and Wild, J and Tirén, Sven and Anisimova, O. V. and Koronovsky, N. V. and Chiang, Liu Chan and Liu, Chan Chiang and Franco-Magalhães, Ana Olivia Barufi and Hackspacher, Peter Christian and Saad, Antonio Roberto and Estado, Capes-universidade D O and Rio, D O and Vol, Nature and Zealand, New and Island, North and Islands, Kurile and De Lima, Jose Valdeni and Nag, Sisir Kanti and Chakraborty, Surajit and Leary, D W O and Friedman, J D and Pohn, H A and Solomon, Semere and Ghebreab, Woldai and Gabrielsen, Roy H. and Braathen, Alvar and Dehis, John and Roberts, David and Hickman, M. H. and Nur, Amos and Cortés, A. L. and Soriano, M. A. and Maestro, A. and Casas, A. M. and Good, Nadine and De Wit, Maarten J. and Sarti, Therence Paoliello and Fisiograficos, Aspectos and Anteriores, Trabalhos and Geol, Contexto and Brasiliano, Evento and Ribeira, Faixa and Litoestratigr, Unidade and Estruturais, Aspectos and Geol, Contexto and Tect, Regimes and Continentais, Rifts and Meso-cenoz, Tecnonismo and Bibliogr, Pesquisa and Fundamentais, D E Conceitos and Estrutural, D E Geologia and Wheeler, Russell L. and Pinto, Victor Hugo and Wilson, J. T. and Misra, Achyuta Ayan and Mukherjee, Soumyajit and Edition, Second and Geraldes, Mauro Cesar César and Weizsacker, Von and Weizsacker, Von and {شجاعی، رؤیا} and Terrestre, Crosta and Paulista, Universidade Estadual and Em, Programa D E Pós-graduação and Biológicas, Ciências and Sr, Rb- and Pb, Pb and Pb, Pb and Th, Th and Unidos, Estados and Drilling, Scientific Ocean and Trouw, Rudolph A J and Mandl, Georg and Nursalam, 2016, metode penelitian and Machado, NUNO and Valladares, CLÁUDIA SAYÃO Claudia and Heilbron, MONICA and Valeriano, Claudio de Morisson CLÁUDIO DE MORISSON Claudio De Morisson and MACHADO JÚNIOR, DELZIO DE LIMA and Grande, Barra and Almeida, Julio C.H. Júlio Julio and Peternel, Rodrigo and Ramos, Renato R Cabral and Guimarães, Paulo V and Rodrigues, Sergio W De O and Rezende, Estevão M C De and Périco, Rafael and Costa, Vitor and Np, Em and Effektivwert, Echter and Marcén, M. and Casas-Sainz, A. M. and Román-Berdiel, T. and Griera, A. and Santanach, P. and Pocoví, A. and Gil-Imaz, A. and Aldega, L. and Izquierdo-Llavall, E. and Marques, Leila Soares and De Min, Angelo and Rocha-Júnior, Eduardo Reis Viana R.V. and Babinski, M. and Bellieni, Giuliano and Figueiredo, Antonio M.G. Manuel Ferreira and Merdith, Andrew S. and Williams, Simon E. and Brune, Sascha and Collins, Alan S. and Müller, R. Dietmar and Mohriak, Webster Ueipass and Danforth, A. Al and Post, Paul J. and Brown, David E. and Tari, Gabor C. and Nemčok, Michal and Sinha, Sudipta T. and Moulin, Maryline and Aslanian, Daniel and Unternehr, Patrick and Cai, Chen and Wiens, Douglas A. and Shen, Weisen and Eimer, Melody and {André Pires Negrão} and Nürnberg, Dirk and Müller, R. Dietmar and Gravim, Dados and Setentrional, Nordeste and Paulista, Universidade Estadual and Em, Programa D E Pós-graduação and Biológicas, Ciências and Elisa, Drª and Rocha, Soares and Ufg, Barbosa and Augusto, Pedro Prof and Silva, De Souza Da and Pequeno, Mônica Alves and Nursalam, 2016, metode penelitian and Mukasa, Samuel B. and Flower, Martin F.J. and Miklius, Asta and Marques, Leila Soares and Ulbrich, Mabel N.C. C. and Ruberti, Excelso and Tassinari, Colombo G. C.G. Celso Gaeta C.G. and Fodor, R. V. and Hanan, B. B. and Motoki, Akihisa and Vargas, Thais Thaís and Iwanuch, Woldemar and Da Silva, Samuel and Balmant, Alex and Gonçalves, Juliana and Sichel, Susanna Eleonora and Mello, Sidney Luiz Matos and Motoki, Kenji Freire and Aires, José Ribeiro and Siebel, W. and Becchio, R. and Volker, F. and Hansen, M. A.F. and Viramonte, J. and TRUMBULL, R. B. and Haase, G. and Zimmer, M. and Weaver, Barry L. and Saka, Hiroyasu and Weaver, Barry L. and Kar, Aditya and Davidson, Jon and Colucci, Mike and Cordani, U. G. and Teixeira, Wilson and D'Agrella-Filho, M. S. and Trindade, R. I. and Filho, André Danderfer Antonio Thomaz and de Cesero, Pedro and MIZUSAKI, Ana Maria Pimentel and Leão, Joana Gisbert and Santos, Rodrigo Bijani Rosana N. and Marques, Leila Soares and Sturm, R. and Grange, M. and Scharer, U. and Merle, R. and Girardeau, J. and Cornen, G. and Comin-Chiaramonti, PIERO and Junqueira-Brod, TEREZA CRISTINA and ROIG, HENRIQUE LLACER and Gaspar, JOSÉ CARLOS and Brod, JOSÉ AFFONSO and MENESES, PAULO ROBERTO and Aguirre, Luis and Controles, E and Dos, Gênese and Fósforo, Depósitos D E and Doutorado, Tese D E and Prof, Orientador and Carlos, José and Ig, Gaspar and Claudinei, Prof and Oliveira, Gouveia De and Unb, I G and Augusto, Pedro Prof and Bittencourt, Cesar and Ig, Pires and Maria, Profa and Motta, Cristina and Usp, De Toledo and Df, Brasília and Lopes, José Carrilho and Niu, Yaoling and Wilson, Marjorie and Humphreys, Emma R. and O'Hara, Michael J. and Hirano, Naoto and Machida, Shiki and Abe, Natsue and Morishita, Tomoaki and Tamura, Akihiro and Arai, Shoji and {USDA} and Letícia, Mara and Da, Torres and Aitchison, Jean and Huddlestone, Rodney and Pullum, Geoffrey K. and Estadual, Universidade and Microscopy, Carl Zeiss and Mata, João and Alves, Costa F. and Martins, Línia and Miranda, Rui and Madeira, José and Pimentel, N. and Martins, S. and Azevedo, Maria Rosário and Youbi, Nasrrddine and De Min, Angelo and Almeida, I. M. and Bensalah, Mohamed Khalil and Terrinha, Pedro and Motoki, Akihisa and Chaves Novais, Luiz Carlos and Motoki, Kenji Freire and Oliveira, Leonardo Costa De and De Fasolo, Ricardo Souza and Lima, Adrienne Brito and Putirka, Keith Daniel and Perfit, Michael and Ryerson, F. J. and Jackson, Matthew G. and The, Eochemistry O F Efinitions and The, Volution O F and Kawabata, Hiroshi and Hanyu, Takeshi and Chang, Qing and Kimura, Jun Ichi and Nichols, Alexander R.L. and Tatsumi, Yoshiyuki and Motoki, Akihisa and Novais, Luís Carlos Chaves and Sichel, Susanna Eleonora and Neves, José Luiz Peixoto Marques Correia and Aires, José Ribeiro and Jalowitzki, Tiago Luis Reis and Katz, Richard F. and Spiegelman, Marc and Langmuir, Charles H. and Jacobsen, S. B. and Wasserburg, G. J. and Leão, Zelinda M and Ahmadzadeh, Gholamreza and Zamani, Reza and Albarede, Francis and Aldanmaz, Ercan and Köprübaşi, N. and Gürer, Ö F. and Kaymakçi, N. and Gourgaud, A. and Allsgre, Claude J and Schiano, Pierre and Lewin, Eric and LARSEN, L. M. and Kogarko, L. N. and Onicas, O E S Tect and Eleonora, Susanna and Maria, Cristina and Campos, Pinheiro De and Alves, Costa F. and {CARMICHAEL ISE} and {NICHOLLS J} and {SMITH AL} and Cawthorn, R G and Collerson, K. D. and MORIMOTO, N and Nacional, Museo and Ciencias, De and Putirka, Keith Daniel and Perfit, Michael and Ryerson, F. J. and Jackson, Matthew G. and Haschke, M. and Siebel, W. and Günther, A. and Scheuber, E. and Exemplos, X and Anderson, Don L. and Armienti, Pietro and Gasperini, Daniela and Motoki, Akihisa and Paulista, Universidade Estadual and Em, Programa D E Pós-graduação and Biológicas, Ciências and Spinelli, Fernando Pelegrini and De Barros Gomes, Celso and Nursalam, 2016, metode penelitian and Douglass, Jill and Schilling, Jean-Guy and Fontignie, Denis and Xu, Yigang and Huang, Xiaolong and Menzies, Martin A. and Wang, Rucheng and Ersoy, Yalçin and Helvaci, Cahit and Geldmacher, Jörg and Van Den Bogaard, Paul and Hoernle, Kaj and Schmincke, Hans Ulrich and Hastie, Alan R. and Mitchell, Simon F. and Kerr, Andrew C. and Minifie, Matthew J. and Millar, Ian L. and Delimitá-lo, Almeidianas A O and Geraldes, Mauro Cesar César and Tassinari, Colombo G. C.G. Celso Gaeta C.G. and Babinski, M. and Martinelli, C. D. and Iyer, S. S. and Barboza, E. S. and Pinho, F. E.C. and Onoe, A. T. and Borisov, A. A. and Shapkin, A. I. and Hofmann, Albrecht W. and Jochum, K. P. and Seufert, M. and White, William M. and De, Séries and THOMPSON, ROBERT N. and Gibson, SALLY A. and Mitghell, J. G. and Dickin, A. P. and Leonardos, O. H. and Brod, JOSÉ AFFONSO and Greenwood, J. G. and Dada, O. A and Ashano, E. C and Karsli, Orhan and Aydin, Faruk and Sadiklar, M. Burhan and Rudnick, Roberta L and Gao, Shan and Matteini, Massimo and Dantas, Elton Luiz and Pimentel, Marcio M. and Bühn, Bernhard and Samoilov, Valery S. and Vapnik, Yevgeny and Phemister, James and Pilet, Sébastien and Sharp, Warren D. and Renne, Paul Randall and Shcherbakov, Vasily D. and Plechov, Pavel Yu and Izbekov, Pavel E. and Shipman, Jill S. and Smith, Douglas David C. and Atlantic, South East and Holanda dos Santos, Werlem and Bergamaschi, Sérgio and Rodrigues, René and Bezerra da Costa, Diego Felipe and Chaves, Hernani Aquini Fernandes and THOMPSON, ROBERT N. and Ottley, C. J. and Smith, P. M. and Pearson, D. G. and Dickin, A. P. and Morrison, M. A. and Leat, Philip T. and Gibson, SALLY A. and Clemente, Eliane de Paula Eliene de Paula and Schaefer, Carlos Ernesto G.R. and Oliveira, Fábio Soares and Albuquerque Filho, Manoel Ricardo and Alves, Ruy Válka and Sá, Mariana Médice Firme and Melo, Vander de Freitas and Corrêa, Guilherme Resende and Chakhmouradian, Anton R. and Reguir, Ekaterina P. and Kamenetsky, Vadim S. and Sharygin, Victor V. and Golovin, Alexander V. and Assumpçao, Marcelo and Schimmel, Martin and Escalante, Christian and Barbosa, José A. Roberto and Rocha, Marcelo Peres and Barros, Lucas V. and Cisalhamento, Zonas De and Azzone, Rogério Guitarrari and Ruberti, Excelso and Da Silva, Júlio César Lopes and De Barros Gomes, Celso and Rojas, Gaston Eduardo Enrich and De Hollanda, Maria Helena Bezerra Maia and Tassinari, Colombo G. C.G. Celso Gaeta C.G. and MacIntyre, Jane E. and MacIntyre, Jane E. and Bongiolo, Everton M. and Pires, Gomes L.C. and Geraldes, Mauro Cesar César and Santos, Adjacente C. À Bacia D E and Neumann, R. and Green, Nathan L and Usdansky, Steven I and Acevedo, Rogelio D. and Quartino, Bernabé J. and Ersoy, Emrah Yalçin and Beccaluva, Luigi and Bianchini, Gianluca and Natali, Claudio and Siena, Franca and Boomeri, M and Callegaro, Sara and Rapaille, Cedric and Marzoli, Andrea and Bertrand, Hervé and Chiaradia, Massimo and Reisberg, Laurie and Bellieni, Giuliano and Martins, Línia and Madeira, José and Mata, João and Youbi, Nasrrddine and De Min, Angelo and Azevedo, Maria Rosário and Bensalah, Mohamed Khalil and Comin-Chiaramonti, PIERO and De Min, Angelo and Girardi, Vicente A.V. and Gomes, Celso B. de Barros and Friedman, Irving and O'Neil, James R JR and Hwang, Jeong and Zheng, Xiangshen and Ripley, Edward M. and Lee, Jong Ik and Shin, Dongbok and Mitchell, Roger H. and Mysen, Bjorn and Turi, B. and Boettcher, AL and O'Neil, James R JR and Brod, JOSÉ AFFONSO and Gibson, SALLY A. and THOMPSON, ROBERT N. and Junqueira-Brod, TEREZA CRISTINA and SEER, HILDOR JOSÉ and MORAES, LÚCIA CASTANHEIRA DE and BOAVENTURA, GERALDO RESENDE and Burg, Jean Pierre and Bussweiler, Yannick and Foley, Stephen F. and Prelević, Dejan and Jacob, Dorrit E. and Analysis, Diopside and Ferrari, ANDRÉ LUIZ and Riccomini, CLÁUDIO Claudio and Campos, Lucia and Lavrado, Helena and Gamboa, L. and Souza, Kaiser De and Geotectônico, I Contexto and Formação, De and Sudeste, Margem and Troalen, Frédéric and Sedimentares, Bacias and Brasileira, Margem Continental and Sial, Alcides Nobrega and MCreath, Ian and GUSMÃO, LUIZ GUILHERME SÁ and Motoki, Akihisa and Motoki, Kenji Freire and Melo, Dean Preira de and Coltorti, M. and Bonadiman, Costanza and Hinton, R. W. and Siena, Franca and Upton, B. G.J. and Wagner, João and Castro, Alencar and Chang, Zhaoshan and Vervoort, Jeffery D. and McClelland, William C. and Knaack, Charles and Saunders, A. D. and Langmuir, Charles H. and Bézos, A. and Escrig, S. and Parman, S. W. and Sun, Shen Su and McDonough, William F. and Bryson, John M. and Zindler, A. and Hart, Stanley R. and MARTINSSON, ANDERS and BASSETT, MICHAEL G. and Chakhmouradian, Anton R. and Böhm, C O and Kressall, R D and Lenton, P G and Nursalam, 2016, metode penelitian and MARTINS-NETO, MARCELO A. and Frisch, Thomas and Colli, Lorenzo and Fichtner, Andreas and Bunge, Hans Peter and Mantle, G. W. and Collins, W. J. and Kushiro, Ikuo and Renda, Imposto D E and Fonte, N A and Verzegnassi, Emerson and Bizzi, L A and Schobbenhaus, C and Stocco, Luana Birchler and Conrad, Clinton P. and Lithgow-Bertelloni, Carolina and Rochas, Geoquimica das and Científica, Série and {鷹巣豊治} and Crosby, A. G. and McKenzie, D. and Sclater, John G. and Rahman, S and MacKenzie, W S and Wijbrans, C. H. and Klemme, S. and Berndt, Jasper and Vollmer, C. and Dalton, Colleen A. and Langmuir, Charles H. and Gale, Allison and Dasgupta, Rajdeep and Hirschmann, Marc M. and Smith, Neil D. and Stalker, Kathryn and Davis, F. A. and Hirschmann, Marc M. and Humayun, M. and Bas, Le and Zhang, Li and Meng, Yue and Yang, Wenge and Wang, Lin and Mao, Wendy L. and Zeng, Qiao Shi and Jeong, Jong Seok and Wagner, Andrew J. and Mkhoyan, K. Andre and Liu, Wenjun and Xu, Ruqing and Mao, Ho Kwang and Pesquisa, Pró-reitoria D E and Almeida, Vidyã Vieira de and Geologia, Programa D E Pós-graduação E M and Araçuaí, Orógeno and Salinas, A Oeste D E and Horizonte, Belo and Weigert, Stefan Cruz and Antoniassi, J. L and Gonçalves, Rhaine Matos and Webster, James D. and Botcharnikov, Roman E. and Dobosi, G. and Fodor, R. V. and Fonseca, Ariadne Do Carmo and Skolotnev, S. G. and Peyve, A. A. and Turko, N. N. and Keskin, Mehmet and Dombroski, Brian A. and Fodor, R. V. and Droop, G. T. R. and Ellam, Robert M. and Ellrs, Devro and WYLLIE, PETER and Ernst, Richard E. and Buchan, Kenneth L. and Davies, D. R. and Rawlinson, N. and Iaffaldano, G. and Campbell, I. H. and EWART, A. and Jalowitzki, Tiago Luis Reis and Conceição, Rommulo Vieira Raimundo Almir Costa da and Orihashi, Yuji and Bertotto, Gustavo Walter and Nakai, Shun'ichi and Schilling, Manuel Enrique and SOMMER, CARLOS AUGUSTO and Lima, EVANDRO FERNANDES DE and NARDI, LAURO VALENTIM STOLL and Sack, Richard O. and Walker, David and Carmichael, Ian S.E. and Reath, Kevin A. and Ramsey, Michael S. and Carlson, Richard W and Frost, D. J. and Langenhorst, F. and Van Aken, P. A. and Nekvasil, H. and I, Disciplina G E Mineralogia and Sari, Shinta Permata and Gao, Jian Feng and Zhou, Mei Fu and Robinson, Paul T. and Wang, Changming Christina Yan and Zhao, Jun Hong and Malpas, John and ALMEIDA, Fernando FLÁVIO MARQUES De E and Fávaro, Déborah Inês Teixeira and De Oliveira, Sonia Maria Barros and Damatto, Sandra Regina and Menor, Eldemar and Moraes, Alex Souza and Mazzilli, Barbara Paci and Ferrara, G. and Preite-Martinez, M. and Taylor, H. P. and Tonarini, S. and Turi, B. and THOMAZ FILHO, Antonio ANTÔNIO and Cesero, PEDRO DE and MIZUSAKI, Ana Maria Pimentel and Leão, Joana Gisbert and Da Costa Alves, Eliane and Maia, Márcia and Sichel, Susanna Eleonora and De Campos, Cristina Maria Pinheiro and Fodor, R. V. and Galar, P. and Island, Kahoolawe and Fodor, R. V. and Dobosi, G. and Sial, Alcides Nobrega and McKee, E. H. and Asmus, H. E. and Foley, Stephen F. and Prelevic, Dejan and Rehfeldt, Tatjana and Jacob, Dorrit E. and Crystallization, Fractional and Melting, Fractional and Gutiérrez-Alonso, Gabriel and Collins, Alan S. and Fernández-Suárez, Javier and Pastor-Galán, Daniel and González-Clavijo, Emilio and Jourdan, Fred and Weil, Arlo B. and Johnston, Stephen T. and Verati, Chrystele and Féraud, Gilbert and Fitton, J. Godfrey and Upton, B. G.J. and Frost, B. Ronald and Frost, Carol D. and Janoušek, Vojtěch and Farrow, C. M. and Erban, Vojtěch and França, Z. and Ulbrich, HORSTPETER H. G. J. and Demaiffe, D. and VLACH, Silvio R.F. F. and Ulbrich, Mabel N.C. C. and Kay, R W and Seifert, Karl and Brunotte, Dale and RIZZOTTO, GILMAR JOSÉ and Lima, EVANDRO FERNANDES DE and CHEMALE JR, FARID and Bonito, Rio and Geraldes, Mauro Cesar César and Motoki, Akihisa and Costa, Anderson and Mota, Carlos Eduardo Miranda and Mohriak, Webster Ueipass and {شجاعی، رؤیا} and Weaver, Barry L. and Wood, D. A. and Tarney, J. and Joron, J. L. and Leal, Angela Beatriz de Menezes and Paul, Debajyoti and Silveira, Walter Peixoto da and Leal, Luiz Rogério Bastos and Cruz, Simone Cerqueira Pereira and Santos, Joilma Prazeres and Lusa, Marcelo and Lusa, Marcelo and Philipp, Ruy Paulo and Valentim, Lauro and Nardi, Stoll and Skolotnev, S. G. and Peive, A. A. and Cushman, Buffy and Sinton, John M. and Ito, Garrett and Dixon, Jacqueline Eaby and Gibson, SALLY A. and THOMPSON, ROBERT N. and Day, J. A. and Humphris, Susan E. and Dickin, A. P. and Leonardos, O. H. and Dickin, A. P. and Mitchell, John G. and Gibson S.A., Thompson R.N., Weska R.K., Dickin A.P., Leonardos O.H. and Gibson, SALLY A. and THOMPSON, ROBERT N. and Weska, R. K. and Dickin, A. P. and Leonardos, O. H. and Drescher, Hermann and Gnev, N H and Green, D. H. and Ringwood, A. E. and Green, Trevor H. and Green, D. H. and Ringwood, A. E. and Grove, Timothy L. and Till, Christy B. and Krawczynski, Michael J. and Haase, Karsten M. and Beier, Christoph and Kemner, Fabian and Davies, Gareth R and Halliday, Alex N and Lee, Der-chuen and Tommasini, Simone and Paslick, Cassi R and Fitton, J. Godfrey and James, Dodie E and Hartmann, Gerald and Wedepohl, K. Hans and Wentworth, Chester K. and Macdonald, Gordon A. and Mäkipää, H. and Heinonen, Jussi S. and Jennings, Eleanor S. and Riley, Teal R. and Herz, Norman and Hirano, Naoto and Kawamura, K. and Hattori, M. and Saito, K. and Ogawa, Y. and Machida, Shiki and Abe, Natsue and Morishita, Tomoaki and Tamura, Akihiro and Arai, Shoji and De Hoog, Jan C.M. and Gall, Louise and Cornell, David H. and Humphris, Susan E. and Thompson, Geoffrey and Mota, Carlos Eduardo Miranda and Souza, F A L De and Vargas, Thais Thaís and Geraldes, Mauro Cesar César and Nursalam, 2016, metode penelitian and Resende, Flávio and THOMAZ FILHO, Antonio ANTÔNIO and MIZUSAKI, Ana Maria Pimentel and Antonioli, Luzia and Carbonin, C. and Dal Negro, A. and Ganeo, S. and Piccirillo, Enzo M. and Kronberg, Bengt and Holmberg, Krister and Lindman, Björn and Ishimaru, Satoko and Arai, Shoji and White, William M. and {Anelise Losangela Bertotti} and Regional, Geologia and Vergara, Mercedes Diaz and Torquato, Joaquim Raul and Widom, Elisabeth and Bindeman, Ilya and Keken, Van and Ablay, G. J. and Carroll, M. R. and Palmer, M. R. and Martí, J. and Sparks, R. S.J. and Kar, Aditya and Weaver, Barry L. and Davidson, Jon and Colucci, Mike and Le Roex, Anton P. and Lanyon, Ruth and Weis, D. and Frey, F. A. and Giret, A. and Cantagrel, J. M. and THOMPSON, ROBERT N. and Ottley, C. J. and Smith, P. M. and Pearson, D. G. and Dickin, A. P. and Morrison, M. A. and Leat, Philip T. and Gibson, SALLY A. and Weis, D. and Frey, F. A. and Giret, A. and Cantagrel, J. M. and Harris, Chris and Marsh, Julian S. and Milner, Simon C. and Peate, David W. and Hawkesworth, Chris J. and Mantovani, Marta S.M. M.S. and Rogers, Nick W. and Turner, Simon P. and Kamber, B. S. and Collerson, K. D. and Nekvasil, H. and Simon, A. and Lindsley, Donald H. and SPATH, A. and HELLEBRAND, Eric and Regelous, Marcel and TRUMBULL, R. B. and Jung, Caroline and Jung, Stefan and Hoffer, Edgar and Berndt, Jasper and Schuth, Stephan and Münker, Carsten and König, Stephan and Qopoto, Cromwell and Basi, Stanley and Garbe-Schönberg, Dieter and Ballhaus, Chris and Le Roex, Anton P. and Class, Cornelia and O'Connor, John M. and Jokat, Wilfried and Kawabata, Hiroshi and Hanyu, Takeshi and Chang, Qing and Kimura, Jun Ichi and Nichols, Alexander R.L. and Tatsumi, Yoshiyuki and Jackson, Matthew G. and Jellinek, A. Mark and Hart, Stanley R. and Konter, Jasper G. and Koppers, Anthony A.P. and Staudigel, Hubert and Kurz, Mark D. and Blusztajn, Jerzy S. and Sinton, John M. and Saal, Alberto E. and Shimizu, Nobumichi and Kurz, Mark D. and Blusztajn, Jerzy S. and Skovgaard, Anna C. and Ramo, Geologia and Coimbra, Universidade De and Paulista, Universidade Estadual and Em, Programa D E Pós-graduação and Biológicas, Ciências and Junqueira-Brod, TEREZA CRISTINA and Gaspar, JOSÉ CARLOS and Brod, JOSÉ AFFONSO and Jost, Hardy and Rocha Barbosa, Elisa Soares and Kafino, Camilla Vasconcelos and Oriental, Oriental E Paraguai and Kaur, Parampreet and Zeh, Armin and Chaudhri, Naveen and Eliyas, Nusrat and Kawabata, Hiroshi and Hanyu, Takeshi and Chang, Qing and Kimura, Jun Ichi and Nichols, Alexander R.L. and Tatsumi, Yoshiyuki and Kelley, Katherine A. and Plank, Terry and Grove, Timothy L. and Stolper, Edward M. and Newman, Sally and Hauri, Erik and Kempton, P.D. and Casey, J.F. and Aoki, Ken ichiro and Kushiro, Ikuo and Kent, Adam J.R. and Norman, Marc D. and Hutcheon, Ian D. and Stolper, Edward M. and Alibert, C and Ketchum, Kirsty Y. and Heaman, Larry M. and Bennett, Gerry and Hughes, David J. and Ganne, Jerome and Feng, Xiaojun and Stanley, J. R. and Flowers, R. M. and Klein, V. C. and Vieira, A. C. and Kostopoulos, Dimitris K. and James, Simon D. and Fowler, Reading and McKibbin, Seann J. and O'Neill, Hugh St C. and Mallmann, Guilherme and Halfpenny, Angela and Langmuir, Charles H. and Hanson, G. N. and Leake, Bernard E. and Woolley, Alan R. and Arps, Charles E.S. and Birch, William D. and Gilbert, M. Charles and Grice, Joel D. and Hawthorne, Frank C. and Kato, Akira and Kisch, Hanan J. and Krivovichev, Vladimir G. and Linthout, Kees and Laird, Jo and Mandarino, Joseph A. and Maresch, Walter V. and Nickel, Ernest H. and Rock, Nicholas M.S. and Schumacher, John C. and Smith, Douglas David C. and Stephenson, Nick C.N. and Ungaretti, Luciano and Whittaker, Eric J.W. and Youzhi, Guo and Bas, M. J.Le and Maitre, R. W.Le and Streckeisen, A. L. and Zanettin, B. and Fractionation, Crystal and Island, Ocean and Thursday, Basalts and Leeman, W P and ALMEIDA, Fernando FLÁVIO MARQUES De E and Carneiro, CELSO DAL RÉ and MIZUSAKI, Ana Maria Pimentel and Liou, Juhn G. and Tsujimori, Tatsuki and Paulista, Universidade Estadual and Em, Programa D E Pós-graduação and Biológicas, Ciências and Brasiliensis, Geochimica and Prazeres Filho, Helcio Jose and Harara, Ossama Mohamed and Stipp Basei, Miguel Angelo and Passarelli, Claudia Regina and Siga Jr., Oswaldo and Parsons, Marianne and MacQueen, Jeffrey and {Sverre Berstad} and Horstad, Idar and Nacional, Parque and Abrolhos, Marinho De and Marine, Abrolhos and Bárbara, Santa and These, Guarita and Paulo, São and Spinelli, Fernando Pelegrini and De Barros Gomes, Celso and ALMEIDA, Fernando FLÁVIO MARQUES De E and Carneiro, CELSO DAL RÉ and MIZUSAKI, Ana Maria Pimentel and Azzone, Rogério Guitarrari and Ruberti, Excelso and Rojas, Gaston Eduardo Enrich and De Barros Gomes, Celso and Mohriak, Webster Ueipass and Janeiro, Rio I O De E and Montes-Lauar, C. R. and Pacca, I. G. and Melfi, A. J. and KAWASHITA, KOJI and Agostinho, Santo and Ya, Vicente and Carmelita, Convento and Castelo, Forte and O'Connor, John M. and Duncan, R. A. and Tomazzoli, Edison Ramos and Pellerin, Jöel Robert George Marcel and Ulbrich, HORSTPETER H. G. J. and VLACH, Silvio R.F. F. and Ulbrich, Mabel N.C. C. and KAWASHITA, KOJI and White, William M. and Paulista, Universidade Estadual and Em, Programa D E Pós-graduação and Biológicas, Ciências and {شجاعی، رؤیا} and Merli, Marcello and Bonadiman, Costanza and Diella, Valeria and Pavese, Alessandro and Yuen, David A and Ulbrich, Mabel N.C. C. and Machida, Shiki and Kogiso, Tetsu and Hirano, Naoto and Velázquez, Victor Fernandez and Riccomini, CLÁUDIO Claudio and Gomes, Celso B. de Barros and Figueredo, Lucia de and Figueredo, Carlos and Workman, Rhea K. and Hart, Stanley R. and Martins, S. and Mendes, Marcos P. H and Mata, João and Munhá, J and Caldeira, R and Madureira, Pedro and Mata, João and Mattielli, Nadine and Queiroz, Gabriela and Silva, Pedro and Reich, C. J. and Ferrelli, Federico and Scientific, National and Scientific, National and Piccolo, Maria Cintia and Mota, Carlos Eduardo Miranda and Geraldes, Mauro Cesar César and de Almeida, Julio Cesar Horta César Horta Cesar Horta and Vargas, Thais Thaís and De Souza, Débora Marinho and De Oliveira Loureiro, Renata and Da Silva, Aline Pimentel and NASCIMENTO, Marcos Antonio Leite do and Vi, Capítulo and NASCIMENTO, Marcos Antonio Leite do and Pl, Tridimensional D O and Japi, T O N D E and M., Babinski and McCarter, Renee' L. and Fodor, R. V. and Trusdell, Frank and Melluso, Leone and Guarino, Vincenza and Lustrino, Michele and Morra, Vincenzo and de' Gennaro, Roberto and Linnen, Robert L. and Keppler, Hans and Kinzler, Rosamond J. and Ghiorso, Mark S and Gualda, Guilherme A R and Regelous, Marcel and Niu, Yaoling and Abouchami, Wafa and Castillo, Pat R. and Hémond, Christophe and Hofmann, Albrecht W. and Vlastélic, Ivan and Nauret, François and Motoki, Akihisa and Paul, Debajyoti and White, William M. and Turcotte, Donald L. and Motoki, Akihisa and Araújo, Ana Lúcia and Sichel, Susanna Eleonora and Geraldes, Mauro Cesar César and Jourdan, Fred and Motoki, Kenji Freire and Silva, Samuel and Campos, Thomas Ferreira da Costa and Srivastava, Narendra Kumar and Soares, Rodrigo and Stasiuk, M. V. and Barclay, J. and Carroll, M. R. and Jaupart, C. and Ratté, J. C. and Sparks, R. S.J. and Tait, S. R. and Müller, R. Dietmar and Roest, Walter R. and Royer, Jean-Yves Y. and Gahagan, Lisa M. and Sclater, John G. and Sdrolias, Maria and Gaina, Carmen and Roest, Walter R. and Leat, Philip T. and Day, Simon J. and Tate, Alex J. and Martin, Tara J. and Owen, Matthew J. and Tappin, David R. and Müller, R. Dietmar and Sdrolias, Maria and Gaina, Carmen and Roest, Walter R. and Muller, R. D. and Royer, Jean-Yves Y. and Lawver, L. A. and Hofmann, Albrecht W. and WASYLENKI, L. E. and Neumann, E R and Nielsen, Roger L. and Nishimura, Koshi and Niu, Yaoling and O'Hara, Michael J. and {Yaoling Niu} and Batiza, R. and Nude, Prosper M. and Shervais, John W. and Attoh, Kodjopa and Vetter, Scott K. and Barton, Corey and O'Connor, John M. and Jokat, Wilfried and Wijbrans, Jan and Colli, Lorenzo and Sclater, John G. and White, William M. and Rural, Universidade Federal and Enxame, Pesquisas and Rural, Universidade Federal and Valente, Castro and Markl, Gregor and Marks, Michael A.W. and Ronald Frost, B. and Gasperini, Daniela and Hernández, Agustín and Cruz, D E L A and Paulista, Universidade Estadual and Em, Programa D E Pós-graduação and Biológicas, Ciências and Variations, Obliquity and Kadik, A. A. and Campos, ROBERTO SACKS DE and Pandey, Ashutosh and Chalapathi Rao, N. V. and Pandit, Dinesh and Pankaj, Praveer and Pandey, Rohit and Sahoo, Samarendra and Kumar, Alok and Pang, Kwan Nang and Chung, Sun Lin and Zarrinkoub, Mohammad Hossein and Khatib, Mohammad Mahdi and Mohammadi, Seyyed Saeid and Chiu, Han Yi and Chu, Chiu Hong and Lee, Hao Yang and Lo, Ching Hua and Anderson, Don L. and Sammis, Charles and Pearce, Julian A. and Harris, Nigel B.W. and Tindle, Andrew G. and Pesaran, M. Hashem and Putirka, Keith Daniel and Durand-Charre, Madeleine and Putirka, Keith Daniel and Johnson, Marie C and Kinzler, Rosamond J. and Walker, David and Andersen, David J. and Lindsley, Donald H. and Davidson, Paula M. and Magnani, Marco and Fujii, Toshitsugu and Orihashi, Yuji and Yasuda, Atsushi and Hirata, Takafumi and Santo, Alba P. and Vaggelli, Gloria and Yang, Zong Feng and Zhou, Jun Hong and Galton, Francis and Figure, From and Ray, Jytisankar and Sen, Gautam and Kunzmann, Thomas and THOMPSON, ROBERT N. and Ottley, C. J. and Smith, P. M. and Pearson, D. G. and Dickin, A. P. and Morrison, M. A. and Leat, Philip T. and Gibson, SALLY A. and Pires, Gomes L.C. and Bongiolo, Everton M. and Geraldes, Mauro Cesar César and Renac, C. and Santos, Adjacente C. À Bacia D E and Jourdan, Fred and Neumann, R. and Lustrino, M; Dallai, L; Giordano, R; Gomes, C.B.; and Poli, Stefano and Schmidt, Max W. and Pfänder, J A and Münker, Carsten and Jung, Stefan and Mezger, K and Gibson, SALLY A. and THOMPSON, ROBERT N. and Weska, R. K. and Dickin, A. P. and Leonardos, O. H. and Lehky, Sidney R. and Tanaka, Keiji and Atentamente, Leia and Instruções, A S and Tese, Souza Bologna and Padilha, Antonio Lopes and Putirka, Keith Daniel and Quirk, David G. and Hertle, Michael and Jeppesen, Jon W. and Raven, Madeleine and Mohriak, Webster Ueipass and Kann, Dorthe J. and Nørgaard, Mette and Howe, Matthew J. and Hsu, Dan and Coffey, Brian and Mendes, Marcos P. H and Rampone, Elisabetta and Borghini, Giulio and Godard, Marguerite and Ildefonse, Benoit and Crispini, Laura and Fumagalli, Patrizia and Rasskazov, Sergei and Chuvashova, Irina and Turner, Simon P. and Caulfield, John and Turner, Michael and Van Keken, Peter and Maury, René and Sandiford, Mike and Prouteau, Gaelle and Alves, Leandro Soares Gouveia Martins; Ruy José Valka and White, William M. and Cheatham, M. M. and Duncan, R. A. and {شجاعی، رؤیا} and Ocean, South Atlantic and Ericsson, K. Anders and Rivalenti, Giorgio and Mazzucchelli, Maurizio and Girardi, Vicente A.V. and Vannucci, Riccardo and Barbieri, M. Adelaide and Zanetti, Alberto and Goldstein, Steve L. and Miranda, Rui and Terrinha, Pedro and Mata, João and Azevedo, Rosário and Chadwick, Jane and Lourenço, Nuno and Moreira, Mário and Rocha, Marcelo Peres and Schimmel, Martin and Assumpção, Marcelo and Liz, Joaquim Daniel De and Lima, EVANDRO FERNANDES DE and Valentim, Lauro and Nardi, Stoll and Le Bas, M. J. and Streckeisen, A. L. and Harangi, Szabolcs and Downes, Hilary and Thirlwall, Matthew and Gméling, Katalin and Rov, P L and Rojas, Gaston Eduardo Enrich and Ruberti, Excelso and Azzone, Rogério Guitarrari and De Barros Gomes, Celso and Rona, Peter a and Richardson, Evan S and Sanfilippo, Alessio and Tribuzio, Riccardo and Tiepolo, Massimo and Sato, Hiroaki and Saunders, A. D. and Norry, M. J. and Tarney, J. and Siebel, W. and Becchio, R. and Volker, F. and Hansen, M. A.F. and Viramonte, J. and TRUMBULL, R. B. and Haase, G. and Zimmer, M. and Rodgers, Mel and Roman, Diana C. and Geirsson, Halldor and LaFemina, Peter and Muñoz, Angelica and Guzman, Carlos and Tenorio, Virginia and Regelous, Marcel and Niu, Yaoling and Abouchami, Wafa and Castillo, Pat R. and Shorttle, Oliver and Moussallam, Yves and Hartley, Margaret E. and Maclennan, John and Edmonds, Marie and Murton, Bramley J. and ALMEIDA, Fernando FLÁVIO MARQUES De E and Siivola, J and Schmid, R and Dasgupta, Rajdeep and Hirschmann, Marc M. and Stalker, Kathryn and Paulo, Universidade D E S Ã O A O and Entologia, Sedim and José, Augusto and Pedreira, De C L and C, Augusto José De and Pedreira, L and Oor-rnc, Nrcrror- W A and Bonifacio, Juliana Fernandes and Skolotnev, S. G. and Peyve, A. A. and Turko, N. N. and Bylinskaya, M. E. and Golovina, L. A. and Ipat'eva, I. S. and Lindzus, Dipl Jennifer and Osnabrück, Universität and Tatistik, Ü Bung S and Datenanalyse, U N D and Sleep, Norman H and Sobolev, N. V. and Logvinova, A. M. and Zedgenizov, D. A. and Pokhilenko, N. P. and Malygina, E. V. and Kuzmin, D. V. and Sobolev, A. V. and Solomatov, V and Sonoki, I. K and Garda, Gianna Maria and Meckling, Jensen and and Anderson, Don L. and Jung, Stefan and Stracke, Andreas and Hofmann, Albrecht W. and Hart, Stanley R. and Wolff, John A and Tackley, Paul J. and Dias, Armando and Júnior, Tavares and Geraldes, Mauro Cesar César and Costa, Anderson and Holanda, Werlem and Herzberg, C. and Asimow, P. D. and Arndt, N. and Niu, Yaoling and Lesher, C. M. and Fitton, J. Godfrey and Cheadle, M. J. and Saunders, A. D. and Fuhrman, Miriam L. and Lindsley, Donald H. and Hunt, Heather K and Silveira, F V and Almeida, Ricardo Nascimento and Geologia, Curso D E and Gale, Allison and Langmuir, Charles H. and Dalton, Colleen A. and Möller, H and THOMAZ FILHO, Antonio ANTÔNIO and RODRIGUES, Aliotti ANA LÚCIA and Clemente, Eliane de Paula Eliene de Paula and Schaefer, Carlos Ernesto G.R. and Oliveira, Fábio Soares and Albuquerque Filho, Manoel Ricardo and Alves, Ruy Válka and Sá, Mariana Médice Firme and Melo, Vander de Freitas and Corrêa, Guilherme Resende and Adam, John and Green, Trevor H. and Marks, Michael A.W. and Halama, Ralf and Wenzel, Thomas and Markl, Gregor and {شجاعی، رؤیا} and Mattos, Wanessa Melchert and Rollinson, Hugh R. and Laurent, Antonin and Janoušek, Vojtěch and Magna, Tomáš and Schulmann, Karel and Míková, Jitka and Cagnoli, B. and Romano, G. P. and Haase, Karsten M. and Devey, Colin W. and Vilalva, Frederico Castro Jobim and VLACH, Silvio R.F. F. and Vogel, Thomas A. and Hidalgo, Paulo J. and Patino, Lina and Tefend, Karen S. and Ehrlich, Robert and Skolotnev, S. G. and Peyve, A. A. and Turko, N. N. and MEIRELLES, MARCELO R. and DARDENNE, MARCEL A. and Walters, A. L. and Phillips, J. C. and Brown, R. J. and Field, M. and Gernon, T. and Stripp, G. and Sparks, R. S.J. and Humphris, Susan E. and Thompson, Geoffrey and Wang, Changming Christina Yan and Chen, Liang and Bagas, Leon and Lu, Yongjun and He, Xinyu and Lai, Xiangru and Wass, Suzanne Y. and HEKINIAN, ROGER and Weaver, Barry L. and Kar, Aditya and Davidsont, J O N and Colucci, Mike and Welsch, Benoit and Hammer, Julia and Baronnet, Alain and Jacob, Samantha and HELLEBRAND, Eric and Sinton, John M. and White, William M. and Whitney, Donna L. and Evans, Bernard W. and Woolley, Alan R. and Garth Platt, R. and Wang, Zhi Hui and Ge, Wen Chun and Yang, Hao and Bi, Jun Hui and Ji, Zheng and Dong, Yu and Xu, Wen Liang and Publications, Peter J Wyllie and Yang, Gaoxue and Li, Yongjun and Safonova, Inna and Yi, Shanxin and Tong, Lili and Seltmann, Reimar and Zhang, Chuan Lin and Zou, Hai Bo and Wang, Hong Yan and Li, Huai Kun and Ye, Hai Min and Zhu, Yongfeng and Ogasawara, Yoshihide and Dmitry, Zozulya and Nelson, E B Y and David, K. and Schiano, Pierre and Allègre, C. J. and Laurentino, Lauro De Sena and Quirk, David G. and Hertle, Michael and Jeppesen, Jon W. and Raven, Madeleine and Mohriak, Webster Ueipass and Kann, Dorthe J. and Nørgaard, Mette and Howe, Matthew J. and Hsu, Dan and Coffey, Brian and Mendes, Marcos P. H and de Melo, Aline Cristina and Santos, Rodrigo Bijani Rosana N. and Carlos, Dionísio Uendro and Neto, Cosme F. Ponte and Barbosa, Valéria C. F. and Lins, Carlos Alberto C and Jacques, Ana Paula R and Riccomini, CLÁUDIO Claudio and Gomes, Leonardo Correa Lucy and Anna, Sant and Rigoti, Caesar Augusto and Bento dos Santos, Telmo M. and Tassinari, Colombo G. C.G. Celso Gaeta C.G. and Fonseca, Paulo E. and Siedner, Gerard and Mitchell, John G. and Salomon, Eric and Passchier, Cees and Koehn, Daniel and Tupinambá, Miguel and Heilbron, MONICA and Duarte, Beatriz Paschoal B.P. and Nogueira, José Renato and Valladares, CLÁUDIA SAYÃO Claudia and Almeida, Julio C.H. Júlio Julio and Silva, Luiz Guilherme do Eirado and Medeiros, Silvia Regina de and Almeida, Clayton Guia de and Miranda, Alan and Ragatky, Célia Diana and Mendes, Julio Cezar Júlio and Ludka, Isabel and Teixeira, Wilson and Heilbron, MONICA and Valeriano, Claudio de Morisson CLÁUDIO DE MORISSON Claudio De Morisson and Tupinambá, Miguel and Simonetti, Antonio and Heilbron, MONICA and de Almeida, Julio Cesar Horta César Horta Cesar Horta and do Eirado, Luiz Guilherme and Zalán, Pedro Victor and de Oliveira, João Alberto Bach and Valeriano, Claudio de Morisson CLÁUDIO DE MORISSON Claudio De Morisson and Mendes, Julio Cezar Júlio and Tupinambá, Miguel and Bongiolo, Everton M. and Heilbron, MONICA and Junho, Maria do Carmo Bustamante and Meek, Christopher and Hfiana, Sara Ali and Muñoz Nuñez, Daniel Gerardo and Ji, Guanzhou and Mohler, Rick and Omar, Osama Mohamed El-said and Wael, Arch and Adel, Mohamed and Civil, Faculdade D E Engenharia and Urbanismo, Arquitetura E and Das, Avaliação and Em, Janelas and Escolares, Edifícios and Elaine, Flávia and RODRIGUES, Aliotti ANA LÚCIA and Das, Avaliação and Em, Janelas and Escolares, Edifícios and Rahimzadeh, Shahab Din and Peterson, Nicole L and White, Jonathan Robert and Tapia Zeas, Cristian Eduardo and {Ministerio de Electricidad y Energía Renovable} and Meneses, Edgar and Rojas Cortorreal, Gilkauris María and Karpicka, Edyta and Brembilla, Eleonora and Aguilar Sánchez, Alexis and Confort, L U Z Y and Anand Athalye, Rahul and Boyce, Peter and Hunter, Claudia and Howlett, Owen and Eriksson, Sara and Waldenström, Lovisa},
	date = {2015},
	pmid = {25246403},
	eprinttype = {arxiv},
	eprint = {1011.1669v3},
	note = {{ISBN}: 9788578110796},
	keywords = {(), ({UMI}){AAI}10293775, - basalt, -picrite, 0514:School administration, 0729:Architecture, 1, 10, 10.1002/2014JB011561 and Santos Basin, 1007, 12691, 132ma, 1999, 2, 2 apresentam razões la, 2004 para prospec-, 2006, 2013, 238U and 232Th series, 238U-230Th-226Ra systematics, 3-18, 3He/4He, 4044 derring hall, 40Ar/39Ar, 40Ar/39Ar age dating, 40Ar/39Ar ages, 40Ar/39Ar dating, 40Ar/39Ar geochronology, 41-46, 70 km, 87Sr/86Sr isotopes, 9998:No {ProQuest} subject assigned, A-type granites, {AAPG} Bulletin, {ABSTRACT}, {AFC}, {AFC} processes, {AIFC}, {ANALYTICAL} {TECHNIQUES}, Abrolhos continental shelf, Absolute age, Accommodation zone, Accretionary wedge, Activity Based Costing, Adakite, Adjoint method, Aegirine, Aeromagnetic data, Aeromagnetics, Aeromagnetism, Africa, African plate motion, Agitation gradient, Agpaitic rocks, Alcalina, Alkali basalt, Alkali basalts, Alkali-basalt, Alkalic, Alkaline and tholeiitic magmatism, Alkaline core, Alkaline intrusions, Alkaline magmatism, Alkaline rocks, Alkaline volcanic, Alkaline-carbonatite complex, Aluminous A-type granite, Amambay regions, Amazonian Craton, Ambient mantle temperatures, America, American Society for Photogrammetry, Amphibole, Amphibolite partial melting, Ampère Seamount, Analyse chimique, Andesite, Anisotropy of magnetic susceptibility, Ankaramite, Annual Daylight Metrics, Anorogenic granite, Antarctica, Análise estrutural. Deformação rúptil. Lineamentos, Análisis químico, Apatite fission track, Approaching, Ar-40/Ar-39, Ar-Ar dating, Ar-Ar geochronology, Ar/Ar, Arc magmatism, Arc-type volcanic rocks, Architecture, Argentina, Arizona rhyolite compositions, Arraial do Cabo peninsula, Ascencion fracture zone, Ascension Island, Asia, Assimilation, Asthenosphere, Asthenosphere flow, Atlantic, Atlantic continental margin, Atlantic margin, Atlantic ocean opening, Augite, Azores, Bacia de ({SP}), Bacia de Campos, Bacia do Parnaíba, Baddeleyite U-Pb dating, Balanced restoration, Basalt, Basaltic dikes, Basalts, Basanite, Base erosion, Basement, Basement control, Basement inheritance, Basin analysis, Basin development, Bezymianny Volcano, Bibliographies, Binary mixing, Biology, Body waves, Bohemian Massif, Borborea province, Botswana, Boudinage, Brasiliano orogeny, Brasiliano-Panafrican, Brazil, Brazil magmatism, Brazilian margin, Brazilian rifted margin, Breakup unconformity, Brittle structures, Building Performance Simulation, C, {CENTROS} {INFANTILES}, {CESifo} Working Paper no. 7303, Calc-alkaline magmatism, Calcite, Caldera, Cambrian, Campos Basin, Campos and Santos basins, Campos basin, Cananéia, Carbonate-rich rocks, Carbonated eclogite, Carbonated peridotite, Carbonatite, Carbonatites, Carpathian-Pannonian region, Cataclasis, Cenozoic, Cenozoic rifts, Cenozoic tectonics, Central Atlantic magmatic province, Chain silicates, Chemical composition, Chemical ratios, Chemistry, Chemostratigraphy, Chile trench, Chondrites, Chronology, Classification, Classrooms, Climate-Based Daylight Modelling, Clinopyroxene, Colorado Plateau, Columbia, Columbia seamount, Communication and the arts, Compatibility, Complex, Composition diagrams, Conjugate Atlantic margin, Conjugate margins, Continental Geology, Continental breakup, Continental collision, Continental crust, Continental flood basalt, Continental flood basalts, Continental volcanism, Core-mantle boundary, Corporate income tax, Corvo, Cost management, Cpx, Craton, Cretaceous Serra do Mar, Cretaceous hingeline, Crust, Crustal anatexis, Crustal architecture, Crustal assimilation, Crustal boundary, Crustal composition, Crustal contamination, Crustal modeling, Crustal recycling, Crustal stretching, Crustal structure, Crustal thickness, Crustal thinning, Crystal fractionation, Crystal isotope stratigraphy, Crystallization, Cumulate, Cumulate texture, Cunhaporanga Batholith, {DES}, {DISEÑO} {DE} {ILUMINACION}, {DIVA}, {DMM}, Dacite, Damage zone, Damage zones, Data analysis, Daylight, Daylight factor, Daylighting, Debris flows, Deep seismic, Deep seismic data, Deep seismic reflection and refraction, Deep water, Deformation bands, Degree of melting, Dendrite, Density, Depleted upper mantle, Depositionol element, Desertas Islands, Dharwar, Diagrama equilibrio, Diagramme équilibre, Diamond, Didactic, Different target prices, Digital isochrons, Dike Swarm, Dike swarm, Dike swarms, Dilatational fractures, Diorite, Dual model, Dupal Anomaly, Dupal mantle anomaly, Dyke swarm, Dyke swarms, Dykes, Dynamics of lithosphere and mantle, D″, {EM}-1, Early Jurassic, Earth history, Eastern Pontides, Eclogite, Ecotipos, Education, Educational institutions, Efeito Banco, Eifel, Elbe Fault System, Element partitioning, Endemic soils, Enriched mantle source, Enrichment, Enxame de Diques de Florianópolis, Eocene, Eocene-Oligocene, Eopaleozoic, Equatorial Atlantic, Equilibrium, Eritrea, Eruption transitions, Eruptions, Espírito Santo basin, Etendeka, Experimental petrology, Experimental phase equilibria, {FOZO}, Factor analysis, Faroes, Fault growth, Fault interaction, Fault reactivation, Fault stepovers, Fault surfaces, Fault tip, Faults, Faults and fractures, Fe-Ti oxides, Feldspars, Felsic magmatism, Fernando Poo fracture zone, Fernando de Noronha, Financial performance, Flood basalt, Flood basalts, Florianópolis Dyke Swarm, Fluid flow, Fluid inclusion, Foliation, Foreign direct investment, Formação Serra Geral, Fractional crystallization, Fractionation, Fractionation crystallization, Fracture, Fracture networks, Fracture zone, Fracture-frequency, Fractures, Free-air gravity, Full waveform inversion, Gabbro, Gabbroic xenoliths, Gabbronorite, Galapagos plume, Garnet peridotite, Geobarometry, Geochemical modeling, Geochemical modelling, Geochemistry, Geochemistry of boreholes, Geochronology, Geodynamic, Geodynamics, Geofisica, Geologia estrutural, Geology, Geometry, Geophysical, Geophysical anomaly, Geophysics, Geoprocessing, Geoquímica, Geotectonics, Geothermal, Geothermal gradient, Geothermobarometry, Geothermometry, Germany, Glass inclusions, Gondwana, Gondwana amalgamation, Gondwana breakup, Graciosa Province, Granite, Granitoid-related mineralisation, Granitoids, Graphs, Grasshopper, Gravitational collapse of orogenic crust, Gravity inversion., Gravity settling, Green-core clinopyroxene, Groupe pyroxène, Grupo piroxeno, {HIMU}, {HIMU} {OIB}, Hafnium, Hawaii, Hawaii petrology, Hb3gr hornblende, Heterogeneity, Hidroacústica, High field strength elements, High pressure, High resolution aerotnagnetnometry, Hole model, {HoneyBee}, Hot spot, Hot spots, Hotspot track, Hotspots, Hydraulically significant fracture, Hydrology, Hydrothermal alteration, Hydrothermalism, Hydrous, Hyper-extension, Hyperspectral, {ILUMINACION} {NATURAL}, {INTRODUCTION}, {IRAN}, {IRON} {SKARN} {DEPOSIT}, Iberia, Igneous intrusions, Igneous rocks, Ilha de Santa Catrina, Illumination, Ilmenite, Imperfect fractional crystallization, In situ crystallization, In-situ sr isotopes, Inclusion, India, Industry, Inequality, Interaction, Interaction between Rodinian plume and plate subdu, Intercalibration, Interpretation of magnetic surveys, Intersection, Intra-plate, Intra-plate basalts, Intracontinental deformation, Intraplate deformation, Intraplate seismicity, Inverse theory, Investment, Iran, Irtysh-Zaisan and Junggar-Balkhash Oceans, Isostatic compensation, Isotope, Isotope geochemistry, Isotopic chemistry, Israel, Itaboraí basin, Jacupiranga - Brazil, Jacuípe Basin, Japi Surface, Joints, Juan Fernandez hotspot, July 1964, Jurassic-Cretaceous transition, Karoo, Katungite, Kilauea Iki, Kimberlite, Kinematic analysis, Kinematic indicators, Kinematic reconstruction and constraints, Kinematics, Kink band, Kokchetav, {LA}-{ICP}-{MS}, {LA}-{MC}-{ICP}-{MS}, {LLSVP}, {LadyBug}, Lages Dome, Lagoa da Viraçāo, Lamprophyre, Landsat {TM}, Laptev Sea, Large igneous province, Large igneous provinces, Laser ablation, Late Aptian-Early Albian transtension, Late Cretaceous highlands, Late Palaeoproterozoic, Late-Variscan fault, Laterite, Lava lake, Lava stratigraphy, Left-lateral shear zone, Light, Light distribution, Lighting systems, Lineament, Lineamentos, Linear concentration relationship, Linkage, Liquid immiscibility, Lithosphere, Lithosphere plate, Lithosphere thinning, Lithospheric extension, Lithospheric mantle, Lithospheric strength, Lithospheric subcontinental mantle, Lithospheric subcontinental mantle en, Loihi seamount, Low-frequency seismicity, Lower continental crust, Lower crust, Lu-Hf, Lut, Luz natural, {MORB}, {MS} Windows, Madeira, Mafic dyke swarms, Mafic-felsic magma interaction, Mafic-ultramafic cumulates, Mafurite, Magma chamber, Magma evolution, Magma genesis, Magma mixing, Magma types, Magma-poor margin, Magmas, Magmatic Arc, Magmatic petrology, Magnesian olivine, Magnesiowüstite, Magnetic anomalies, Magnetic petrology, Magnetic petrophysics, Magnetic sector {ICP}-{MS}, Major element, Major elements, Makran, Manjo, Mantle, Mantle convection, Mantle {fO}{\textless}inf{\textgreater}2{\textless}/inf{\textgreater}, Mantle geochemistry, Mantle heterogeneity, Mantle isochron, Mantle melting, Mantle metasomatism, Mantle metasomatism en, Mantle plume, Mantle plumes, Mantle processes, Mantle sources, Marquesas, Martin Vaz island, Mauna Kea, Mauna Kea volcano, Mauna Loa, Mechanics, Melt, Melt inclusions, Melt metasomatism, Melt/ solid reaction, Melt/rock reaction, Melting, Melting model, Mesozoic, Messum complex, Metasomatised lithosphere, Metasomatism, Metasomatized mantle source, Microanalysis, Mid ocean ridge basalt, Mid-Atlantic Ridge, Mid-Polish Basin, Mid-atlantic ridge, Mid-ocean ridge, Mid-ocean ridge basalts, Mid-ocean ridges, Mineral abbreviations, Mineral chemistry, Mineral compositions, Mineral-melt trace element partitioning, Mineralization, Mineralogy, Miocene volcanism, Misra2015, Mixing, Mode of emplacement, Model, Modelo Tese {UERJ}, Morphostructural, Morro Redondo Complex, Morro Vermelho Formation age, Morro de são joão, Muddy system, Multiplets, Mylonite, Møre Margin, {NE} Paraguay, {NW} Iran, Namibia, Nazca Plate, Nb/Ta decoupling, Nb/Ta decoupling en, Nd and Pb isotopes, Nd-144/Nd-143, Neoproterozoic, Neoproterozoic bimodal intrusive complex, Neotectonics, Nepheline syenite, Nephelinite, Nephelinite suite, Nephelinite–phonolite magmatism, Networks, New Zealand, Ni enrichment, Niobium ore deposits, No {ProQuest} subject assigned, Nomenclatura, Nomenclature, Non-volcanic passive continental margin, Norms, North China Craton, North German Basin, Northeastern China, Northern Aravalli orogen, {OIB}, {OIB} genesis, Ocean Drilling, Ocean Island Basalts, Ocean crust, Ocean drilling, Ocean floor, Ocean island basalt, Ocean island basalts, Ocean island magmatism, Ocean islands, Ocean-continent transition, Ocean-island basalts, Oceanic basalts, Oceanic fracture zone, Oceanic islands volcanism, Oceanic lithosphere, Offshore Santos Basin, Okavango, Olivine, Olivine trace element chemistry, Onshore-offshore correlation, Ophiolites, Ore-forming fluids, Orocline, Orogenic collapse, Orthopyroxene, Oscillatory zoning, Oxybarometry, Oxygen, Oxygen fugacity, {PHEM}, Palaeogeomorphology, Paleo-Pacific Ocean, Paleobreccias, Paleocene, Paleocontinent rifting and breakup, Paleozoic, Pan-African Dahomeyide orogen, Parallel dyke swarm, Parameterization, Parana, Parana Basin, Parana Brazil, Parana flood basalts, Parana-Etendeka volcanism, Paraná Basin, Paraná Continental Flood Basalts, Paraná Magmatic Province, Paraná-Etendeka, Paraná-Etendeka Large Igneous Province, Paraná-Etendeka magmatic province, Parental magma, Partial Melting, Partial mantle melting, Partial melting, Partition coefficients, Passive continental margin, Passive margin, Passive margins, Patagonia, Patagonia en, Patagônia (Argentina e Chile), Pb isotope, Pb isotopes, Pb paradox, Pb-207/Pb-204, Pb-208/Pb-204, Pb-Sr-Hf isotopes, Peralkaline melts, Peridotite, Permeable, Perovskite, Petit-spot, Petrogenesis, Petrogenetic modelling, Petrography, Petrologic modeling, Petrology, Phase equilibria, Phonolite, Photogrammetric Engineering, Phreatic eruption, Picrite, Picrites, Pigeonite, Piroxeno, Plagioclase, Plagioclase phenocrysts, Plagioclase zoning patterns, Plate kinematic, Plate motions, Plate tectonics, Plate-wide stress, Platinum group elements, Plug model, Plume-ridge interaction, Plumes, Portugal, Post-collisional, Post-collisional extension, Post-orogenic magmatism, Post-perovskite, Potassic and sodic series, Precambrian, Pressure, Pressure-driven asthenospheric flow, Profit shifting, Programa {REVIZEE}, Proterozoic, Proterozoic granite, Província magmática Paraná, Pré-sal, Pseudo color, Psychology--Abstracting, Pyrochlore, Pyroclastic flows, Pyroxene, Pyroxenite, Pyroxène, {QUIIF}, Quartz, Quaternary, Quaternary dating, Quiet zone, R language, {REE} ratios, Radiance, Radiating dyke swarm, Radiogenic isotopes, Raohe accretionary complex, Rare earth element modeling, Reactivation, Reactivation of structures, Real-estate, Recycled oceanic crust, Recycling, Relay ramps, Relay structures, Remote sensing, Representación arquitectónica, Rhinoceros, Rhyolite, Rhyolite mineral compositions, Ribeira Belt, Ribeira belt, Riedel structures, Rift, Rift stage, Rifting, Rifting deformation, Rifting evolution, Ringwoodite, Rio Apa, Rio Doce formation, Rio bonito, Rock avalanches, Rock magnetism, Rodinia, Russia, {SANGAN}, {SANTOS} Basin, {SCHEM}, {SE} Brazil, {SEBASS}, {STABLE} {ISOTOPES}, Saint Peter Saint Paul Islets, Salite, Salt basin, Salt tectonics, Salton Sea, Samoa, Sandy system, Santa Catarina Shield, Santas Basin, Santos, Santos Basin, Sao Paulo Plateau, Satellite-derived gravimetry, School administration, Seafloor spreading, Seamount, Seawater, Section restoration, Sector zoning, Sedimentary basin, Segmentation, Segregation veins, Seismic, Seismic stratigraphy, Seismic tomography, Sensoriamento remoto, Sergipe-Aalagoas Basin, Serra Geral, Serra do Mar Province, Serra do Mar alkaline province, Shear, Shear fractures, Shear zone, Shear zones, Shear-zone, Shona, Shona plume, Silica activity, Simulación virtual., Sistan, Slab, Slab break up, Slab window, Slickensides, Small-scale mantle heterogeneity, Social sciences, Society, Sodic and potassic, Software, Solomon Islands, Solution model, South, South Africa, South America, South American continent, South Atlantic, South Atlantic Ocean, South Atlantic opening, South China Sea, South Gondwana breakup, South and Equatorial Atlantic Oceans, Southeast Brazil, Southeast atlantic ocean, Southeastern Brazil, Southeastern Brazilian margin, Southeastern Ghana, Southern Brazilian occurrences, Southern Brazilian rifted margin, Southwestern Angola, Spinel, Spreading ridge, Spreadsheet program, Sr, Sr isotopes, Sr-87/Sr-86, Sr-Nd isotopes, Sr-Nd-Os isotopes, Sr-Nd-Pb isotopes, Sr-Nd-Pb-Os isotopes, Sr–Nd–Hf isotopes, St. Helena, Standards, Statistic, Strain, Strain localization, Stress, Stress variability, Strike-slip faults, Strike-slip reactivation, Strontium, Structural analysis, Structural geology, Structural inheritance, Structural lineaments, Sub-continental lithospheric mantle, Sub-continental mantle, Subaqueous eruption, Subcontinental lithospheric mantle ({SCLM}), Subduction, Subduction zone, Submarine volcanism, Subsalt imaging, Subsidence, Subsidence modeling, Sudetic Granite Belt, Suites, Sulfide melt, Summit level map, Supercontinent breakup, Supercontinent cycle, Supercontinents, Supergene ores, Superimposed slickenlines, Supplementary tables, São Paulo Plateau, Taishanmiao, Tanguá, Target costing, Tarim in {NW} China, Taubaté Basin, Tax avoidance, Tax havens, Tectonic breccias, Tectonic evolution northeastern Brazil, Tectonic reactivations, Tectonic setting, Tectonic uplift, Tectonics, Tectono-stratigraphic evolution, Tectônica, Telica, Temperature, Tengchong, Tephrite, Terceira Island, Tese de doutorado, The Czech Republic, Thermal boundary layer, Thermal infrared, Thermal subsidence, Thermometry, Thessalon volcanic rocks, Thinning processes, Tholeiites, Tholeiitic dyke swarm, Tholeiitic magmatism, Tholeitiic, Ti-magnetite, Tibetan plateau, Tonalite, Tonga Trench, Topography, Topology, Trace element enrichment, Trace element mapping, Trace element modeling, Trace element variation, Trace elements, Trace-element partitioning, Trachybasalt, Transfer zone, Transform margin, Transition metals, Transition zone, Transitional domain, Transport model, Travertine, Tres Corregos, Trindade Island, Trindade hotspot, Trindade island, Trindade plume, Triple junction, Tristan, Tristan plume, Troctolites, Tuffisite, Turkey, U'Pb ages, U-Pb, U-Pb dating, U-Pb geochronology, U-value, U/Pb, {UAV} geological mapping, Ugandite, Ultra-low velocity zone, Ultramafic mantle xenoliths, Ultramafic mantle xenoliths en, Ultramafic xenoliths, Ultrapotassic lavas, Upper mantle convective planform, Upper plate magma-poor rifted margins, Upper-mantle tomography, Uruguay, Value engineering, Vent, Vertical segregation, Vesicularity, Viscosity, Vitória-trindade chain, Vol 30, Volatiles, Volcanic arc, Volcanic rifted margin, Volcanic rocks, Volcanic successions, Volcanism, Volcanology, Wave-cut bench, West Bengal, West Gondwana, West Iberian Margin magmatism, Western Cameroon, Western Gondwana, Western {USA}, Wide-angle seismic, Window size, Windows, {XANES}, Xenoliths, Xinjiang Autonomous Region, Yunnan, Zigana Granitoid, Zircon, Zircon U-Pb-Hf isotopes, Zircon U–Pb dating, Zircon ages, Zircon dating, Zirconium, Zona Econômica Exclusiva das regiões Central e Nor, a, a análise do padrão, a atividade vulcânica nesta, a avaliação de dados, a dada, a evolução de tais, a paleoproterozoic unit constituted, a potential petrogenetic and, a preservação parcial, a província contém algumas, a-si, absolute age, abstract geochemistry and petrology, abstract investigation of the, abyssal peridotites, acampamento velho, acumulação de hidrocarbonetos, además se realiza una, adquiridos e processados em, agent-based modelling, aires, al, alcalinas situada no sudeste, alcalino da porção sul, alcalinos na porção norte, algumas evidências geológicas, aliados a dados geológicos, alkaline, alkaline basalts, alkaline complexes, alkaline earth metals, alkaline rock, alkaline rocks, alkaline syenite, alkaline-carbonatite complexes, almacenamiento, amphibole, amphibole composition, amphibole nomenclature, amphibolite, amphibolites, an evolution of active, analisar, and dykes, and e, and pb isotope signatures, and percentage of, angola-etendeka province, análise multi - escala, ao litoral do, ao longo do plano, aos metapsamitos, apip, application in structural analysis, apresenta resultados de análises, apresentando níveis intrusivos, ar, ar geochronology, ar-ar age, araújo, arc basin, arizona rhyolite compositions, arm's length principle, arraial do cabo, article, as bacias sedimentares, as intrusões e extrusões, as principais, asset management, associadas, associação vulcanossedimentar arqueana do, atlântica do continente tem, attitude sensors ângulos de, {bachelorThesis}, bacias da margem continental, bacias sedimentares, back\&hyphen, baixo-tio 2 e alto-tio, balizada pela idade das, basaltic dikes, basaltic vulcanism, basalto, basaltos e andesitos basálticos, basalts, basanite, basin analysis, basin development, batholiths, bloco em fase explorató-, borda sudeste da bacia, boundary layer instabilities, brasil, brasileira, brazil, brazilian alkaline complexes, brazilian sedimentary basins, bulk silicate Earth, by tonalitic, c ashano, ca, cabo, cabo frio, cabo frio island, cambrian, carbonatite, catapleiite, central and southern brazil, ceo inferior, cerca de 250 x, cerca de 5000 m, cf, chegado ao limite leste, chemical stratigraphy - magma, chilwa, chondrite, cite this article, classification, classroom spaces, coastal basins, coastal region, com, com contribuições locais de, com rochas básicas-ultrabásicas vulcanoclásticas, comin-chiaramonti et, como, como geossinclinal e tectônica, comparando-o com o das, comparison of energy consumption, complex, composition of amphiboles from, composição química e evolução, compõem as serras do, conhe-, contains supplementary, continental crust, continental extension, continental flood basalts -, continental margin, continental margin in the, continental rifting, convection, copyright 2005 by svetlana, corpo intrusivo da ilha, corporate tax, correlations, corresponding author, cosmogenic nuclides, cretaceous, cretaceous flood tholeiites, cretaceous until eocene times, cristalinos mais ricos em, crossite, crust, crustal anatexis, crustal assimilation, crustal contamination, crustal recycling, crustal structure, cuja resolução é sobretudo, cummingtonite, d, da crosta para o, da ilha de cabo, da passagem da placa, da textura e mineralogia, dados sísmicos tridimensionais de, dados th-u-pb total por, dannemorite, das mais extensas exposições, data science, dates, daylight, daylight factor, daylight level, daylighting, daylighting architecture, daylighting calculation, daylighting design, daylighting tools, de, de afinidade tholeiitica com, de campo e consideração, de carajás, de contato dos diques, de diques plio-pleistocênicos da, de falhas naturais, de goiás, de idade cretáceo superior, de iguape, de janeiro, de lineamentos na, de natureza vulcânica alcalina, de orientação do enxame, de placas, de profundidade, de rochas alcalinas, de rochas magmáticas predo, de um grande edifício, decay seri, decay series, delaminação térmica ou fraturamento, department of geological, depleted, depth-dependent thermal expansivity, desarrollo de nuevos productos, despertado questões controvertidas, detachment, diabásio, diferenciação do magma de, dike swarm, diques, diques da serra do, direção de sh max, direção wnw-ese, do continente, do estado de são, do parnaíba, do plano de contato, do platô do taquarembo, doi, doi:10., doi:10.1029/2000GL012426, doi:10.1029/2004GL022192, doi:10.1029/2005JB003732, doi:10.1029/9, doi:10.1029/91JB01933, doi:10.1029/94JB00661, doi:10.1029/98JB02642, doi:10.1029/{JB}073i002p00619, dom pedrito, domenico m, doronzo, dos diques máficos da, dos metagabros é o, dr, duas suítes, durante a, durante o éoceno, dyke swarm, e, e ambientes associados, e arraial do cabo, e condicionamento tectônico da, e la, e n8, e paleomagnéticos, e vulcânicas na porção, early stages of design, early-, editorial handling, electronic supplementary material the, elementos de simetria, em adotar critérios e, em condições de baixa, em função da necessidade, em parte, emi-himu signature, encantadas complex, energy, energy loads, entre otros, esse hot spot teria, esta unidade neoproterozóica é, este artigo apresenta a, este artigo apresenta descrições, estes basaltos, estes dados sugerem, euler, euler angles, evaluación de las, evento metamórfïco-deformacional progressivo e, everton marques bongiolo, evolução dos conhecimentos sobre, exhumation, exibem características estruturais e, experimental petrology, explosive eruption, extended kalman filter, extrusive, f, fault-plane markings, feições ímpares da borda, filtro de, first author, flood basalts, fluid, fluidisation, fonolitos, foreign direct investment, fozo, fractionation, fractionation crystallization, fracture zone, fracture zones, frequentemente negligenciada, frio, from early, g, gabbro xe-, gap, garnet, genesis, genéticas para os xenólitos, geobarometry, geochemical modeling, geochemistry, geochronology, geocronológicos k, geodynamics, geological structures, geomatics 1, geomorfológicas e geocronológicas, geoprocessing, geoquímica, geotectonics, geothermobarometric indicator, geothermobarometry, geothermometry, glass - vesicles, glazing, glazing to wall area, glazing type, grandense, granites, grupo grão-pará, gwar, h, hawaiian tholeiite composition -, hc.mk, heat flow, heterogéneo, hidráulico de cizalhamento, high temperature alteration, hot spot, hot spots, hotspot, hotspots, hotspots and melting anomalies, http://dx.doi.org/10.1029/2000GL012426, http://dx.doi.org/10.1029/2004GL022192, http://dx.doi.org/10.1029/2005JB003732, http://dx.doi.org/10.1029/91JB01933, http://dx.doi.org/10.1029/94JB00240, http://dx.doi.org/10.1029/94JB00661, http://dx.doi.org/10.1029/98JB02642, http://dx.doi.org/10.1029/{JB}073i002p00619, http://dx.doi.org/10.1029/{JB}082i005p00803, http://dx.doi.org/10.1029/{JB}089iB07p05835, http://dx.doi.org/10.1029/{JB}094iB06p07278, https, https://dspace.lboro.ac.uk/2134/20417, hydraulic shear fracturing, hydrothermal alteration, icle, igneous, igneous rocks, illuminance, impact structures, incluem-se numa série transicional, incluindo desde complexos máfico-ultramáficos, indicators, influenciados por teorias geotectônicas, infrastructure, inicialmente, initial kinematic reconstruction, integram o enxame de, integrated, internet of things, interpretation, intraplate deformation, intraplate volcanism, introduction and geological setting, introduction the paraná basin, introdução este trabalho sintetiza, introdução estudos realizados na, intrusions, intrusiva de pariqüera açu, intrusive, intrínseca de se ordenar, inverse-forward modelling, inúmeras, is characterized by early, isochron, isotope, isotope and trace element, isotope fractionation, isotope geochemistry, isotope ratios, isotopes, isotópicas rb-sr de interesse, ito and van keken, iv simpósio de vulcanismo, iwanuch, j, japi, jgg-1-1-7, jourdan, journal of geosciences and, k, k-ar disponíveis na literatura, kalman estendido, kamafugite, kimberlite pipe, l, l stratigraphic chart, large igneous province, large igneous provinces, lava flow, lead, lineages, linear volcanic rises, lip, lithochemistry, lithogeochemistry, lithosphere, litogeoquímica, litosférica sul-americana por sobre, lo, location and orientation in, los campos de la, lower paleozoic, luminance, líquidos, m, mafic dyke, mafic dyke swarms, mafic dyke swarms of, magma, magma chamber, magmatic lineaments, magmatic sources, magmatic sources of post-collisional, magmatic volatiles, magmatism, magmática das rochas alcalinas, magnetic anomalies, maior parte dos casos, manjo, mantle, mantle evolution, mantle geochemistry, mantle lithosphere, mantle melting, mantle plumes, mantle source, mantle sources, mantle temperatures, mantle wedge, manufacturing and processing, mar e da mantiqueira, mar no sudeste do, margem con-, marginal basins, mello, melt, melt petrogenesis, melting anomaly, mesozoic, mesozoic-cenozoic magmatism, metagabbros, metagabros da sequência metavulcanossedimentar, metagranites of encantadas, metals, metapelitos, methodology, metrics, mica, microcrystalline silicon, microquímicos, microssonda, mid-atlantic ridge, minas gerais, mineral chemistry, mineral composition, mineral compositions - rhyolitic, mineral redox buffer, minous magmatism occurred in, miocene volcanism, missing Ar, mobile belt, morb, motoki, multinational enterprise, mush, mylonites, máficos da área de, n, na, na região do baixo, nb, nd, nd model ages, near-solidus, nefelina sienito com assimilação, neodymium, neoproterozoic, nepheline syenite, nephelinite, no, no caso correspondente a, no eocretáceo pré-aptiano, no estado do rio, noliths - magma convection, nonchondritic Earth, normal-reverse magnetizations, normalization, northcentral nigeria, nova brasilândia, numerical basin modeling, numerical modeling, nw, o, o artigo, o conhecimento, o de anomalias magn, o magmatismo cretáceo-terciário nas, o pacote vulcânico, o remanescente na interpretac, o vulcanismo basáltico é, o-c-sr-nd isotopes, ocean island basalts, ocorre ao longo de, of porphyritic and equigranular, of post-collisional alkaline, oib, olbina, olivine, online version of this, orbitais e aéreos aplicados, org, oria-trindade chain, os valores de idades, ou seja, p e alta t, p-morb, pa-, paep, pago, paleogene, paleomagnetic virtual poles, paleomagnetism, paleoprotero-, paleoproterozoic, panafrican, para desarrollar aplicaciones en, para os, parana, parentais da suíte de, part of the paraná-, partial melting, partially layered convection, particle size distribution, passive margin, pattern, pau-, pequena exposição de rochas, peridotite, permite limitar mais adequada-, permitiu a determinação da, pertencentes ao distrito ferrifero, pesquisas que têm sido, petrakis - xenólitos tabulares, petrography, petroleum exploration, petrology, petrology.exp, petrolíferos da, petrolíferos e modelos de, ph, phlogopite, phonolite, phonolites and nepheline syenites, pico viejo, piroclásticas e vulcanoclásticas denominada, placa litosférica, plate divergence, pleistocene, pliocene, plugs fonolíticos, plume, plumes, plutonic rocks, ponta grossa arch, por uma, porção nordeste da ilha, porção oeste do escudo, post-collisional magmatism, post-perovskite phase transition, poços de caldas, poços de caldas-cabo frio, pp 630-635, predicted bathymetry, predictive maintenance, predominantes na porção central, present address, pressure, primitive mantle, principalmente em func, procesamiento de alimentos, producción primaria, prof, propor esquemas para classificar, propõe-se que o alinhamento, proterozoic granite, pv module, pyroxene, pyroxene group, que se estende na, r, radiative heat transfer in, radioactive disequilibrium, radioactive isotopes, radiogenic isotopes, ran, rare earth element modeling, rare earths, ratio, rb, reactivation, realizadas no sentido de, redox reactions, redox zonations, referida geralmente como formação, reflexão, região, remanent magnetization, remonta provavelmente ao paleoceno, resultado da maior competência, resumo, resumo - motoki, resumo a ilha da, resumo a origem dos, resumo a província alcalina, resumo ao longo da, resumo com base em, resumo o maciço alcalino, resumo o presente artigo, resumo o vulcanismo ácido, resumo os anfíbolitos e, resumo os corpos n4, resumo sintetizam-se informações sobre, rheology, rhyolite mineral compositions, rhyolites, rhyolites -, ribeira belt, rift, rifting, rj, rochas alcalinas sub-vulcânicas, rochas félsicas, rochas piroclásticas, ron fodor, rosenbuschite, rs, rs está localizado na, s, s institution, s12517-019-4424-y, santa catarina state, santana da boa vista, santos basin, santos basin l stratigraphy, sciences, sedimentary basins, sedimentation, segundo n60w, seguridad alimentaria, seismic interpretation, seismic stratigraphy, sendo representada nesta região, sensores de atitude, sequence, sequência vulcânica ácida, serra do mar dyke, serra dos carajás, shear heating, shoshonitic association as the, shoshonitic association is related, shoshonitic associations, shoshonitic series based upon, sichel, sidelighting, silva, simulation, sistemas montanhosos subparalelos que, sliding normalization, sliding normalization technique using, south atlantic, south-american plate, southeast Brazil, southeast atlantic ocean, southern brazil, southern brazilian rifted margin, sp-mg, sr, sr and k, sr-nd-pb isotopes, stability, stable, stable isotopes, stereographic projection, stress-field, strontium, structural geology, structure, subcontinental lithospheric mantle, subduction, subduction zone, submarine, submersa a, subvulcânicos e nefelina sienitos, sucessão de rochas efusivas, sudeste do brasil, suites, sul, sul-rio-, sul-rio-grandense shield, sunlight, superfícies de erosão do, sustainability, sva, sw amazonian craton, swarm, syenite, são analisados e comparados, são constituídos por uma, t, t - continental break-up, tabular xenolith, tabulares observados ao longo, também do alongamento de, teaching-learning, tectonic evolution, tectonic reactivation, tectonics, tectonism, tectono-magmatic activation, tectono-metamorphic evolution, teide, temperature- and pressure-dependent viscosity, tenerife, tenha se implantado quando, terræ didatica 2, texturais que refletem um, the, the encantadas complex is, the lavras do sul, thermal delamination, thermal transmittance, thermochronology, thermometry, this includes the extensive, tholeiites, tholeiitic basalts, tholeiitic series, tijucas terrane, tims, tinental brasileira e nas, tinguaitos, tirodite, to the early stages, toro diorite, toro dioritic complex, transfer pricing, transparent blinds, trazabilidad, treatise on geophysics, trindade, trindade island, tristan da cunha plume, trondhjemitic, types - boreholes -, ultrapotassic, um hot spot, uma faixa de direção, universidade federal do rio, upper Precambrian, upwelling rates, vale do rio ribeira, vargas, venite, vertical foreclosure, virginia polytechnic institute and, vit, vitória-trindade chain, vogelsberg, volcanic rock, volcanism, volcano, volu-, volume, vulcânico cuja base encontra-se, vários autores se preocuparam, w, water, western cameroon, wide-angle seismic, with regard to type, xenoliths, yb, zircon, à, à compreensão da origem, à deformação em relação, área de cabo frio, áreas emersas adjacentes, áreas oceânicas adjacentes per-, ção petrolífera de um, é a porção emersa, é composto essencialmente por, é formado principalmente por, época de formação, σ 1, Киари},
}

@article{burns_sampling-based_2006,
	title = {Sampling-Based Motion Planning Using Uncertain Knowledge},
	abstract = {Sampling-based algorithms have dramatically improved the state of the art in robotic motion planning. However, these approaches still make significant assumptions that limit their applicability for real-world planning. This work describes how one of these assumptions: that the world is perfectly known, can be removed. We propose a predictive roadmap planner that incorporates uncertainty directly into the planning process. This enables the planner to identify configuration space paths that minimize the risk due to uncertainty and, when necessary, directs sensing to reduce uncertainty. Experimental results in several domains indicate that the predictive roadmap is adept at planning despite uncertainty in its perception of the workspace. 1},
	pages = {1--16},
	author = {Burns, Brendan and Brock, Oliver},
	date = {2006},
}

@article{barsim_unsupervised_2014,
	title = {Unsupervised Adaptive Event Detection for Building-Level Energy Disaggregation},
	abstract = {… The system is tested on the publicly available Building-Level {fUlly} labeled Electricity … M. Berges, “{BLUED}: A Fully Labeled Public Dataset for Event-Based Non-Intrusive Load … appliances via analysis of power consumption,” in Uni- versities Power Engineering Conference ({UPEC} …},
	pages = {1--6},
	author = {Barsim, Karim Said and Streubel, Roman and Yang, Bin},
	date = {2014},
}

@article{deter_simulating_2021,
	title = {Simulating the Autonomous Future},
	pages = {111--121},
	issue = {January},
	author = {Deter, Dean and Wang, Chieh Ross},
	date = {2021},
}

@article{wang_few-shot_2019,
	title = {Few-shot Video-to-Video Synthesis},
	issue = {{NeurIPS}},
	author = {Wang, Ting-chun and Liu, Ming-yu and Tao, Andrew and Liu, Guilin and Kautz, Jan and Catanzaro, Bryan},
	date = {2019},
}

@article{castiganani_achieving_2018,
	title = {Achieving autonomous driving with simulation \& testing},
	pages = {6--9},
	author = {Castiganani, Luca},
	date = {2018},
}

@article{thielman_expert_1999,
	title = {Expert System for Crash Data collection},
	pages = {8--72},
	author = {Thielman, Carol Y.},
	date = {1999},
}

@book{eskandarian_autonomous_2012,
	title = {Autonomous Driving: Context and State-of-the-Art},
	volume = {1-2},
	isbn = {978-0-85729-085-4},
	abstract = {The Handbook of Intelligent Vehicles provides a complete coverage of the fundamentals, new technologies, and sub-areas essential to the development of intelligent vehicles; it also includes advances made to date, challenges, and future trends. Significant strides in the field have been made to date; however, so far there has been no single book or volume which captures these advances in a comprehensive format, addressing all essential components and subspecialties of intelligent vehicles, as this book does. Since the intended users are engineering practitioners, as well as researchers and graduate students, the book chapters do not only cover fundamentals, methods, and algorithms but also include how software/hardware are implemented, and demonstrate the advances along with their present challenges. Research at both component and systems levels are required to advance the functionality of intelligent vehicles. This volume covers both of these aspects in addition to the fundamentals listed above.},
	pagetotal = {1-1599},
	author = {Eskandarian, Azim},
	date = {2012},
	doi = {10.1007/978-0-85729-085-4},
	note = {Publication Title: Handbook of Intelligent Vehicles
Issue: October 2017},
}

@article{johnsen_literature_2017,
	title = {Literature review on the acceptance and road safety, ethical, legal, social and economic implications of automated vehicles},
	pages = {1--76},
	number = {723021},
	author = {Johnsen, Annika},
	date = {2017},
	keywords = {Automated driving, acceptance, autonomous driving, economic aspects, ethical aspects, legal aspects, organised stakeholders, public opinion, road safety, social aspects},
}

@article{rao_deep_2018,
	title = {Deep Learning for Self-Driving Cars : Chances and Challenges},
	pages = {35--38},
	author = {Rao, Qing and Group, B M W and Frtunikj, Jelena and Group, B M W},
	date = {2018},
	note = {{ISBN}: 9781450357395},
	keywords = {2018, acm reference format, automotive, cars, deep learning, deep learning for self-driving, frtunikj, functional safety, qing rao and jelena},
}

@article{asher_towards_2014,
	title = {Towards an Autonomous World: Making Sense of the Potential Impacts of Autonomous Vehicles},
	url = {https://smartech.gatech.edu/handle/1853/51944},
	abstract = {Planning as a profession covers many different facets even though the traditional role of a planner is thought of as dealing with land use and zoning. Taking this narrow view of planning misses many of the roles planners perform. One important role planners can have is that of an informer and consensus builder. By using the broad array of skills that planners have, a planner can help policymakers and the citizenry understand the impacts certain changes will have on the built environment and society. Further, planners can help develop a vision for embracing change while limiting the negative impacts of change. This paper will seek to inform planners, policymakers, and the public about the potential benefits and impacts autonomous vehicles will have on the urban environment and society in the coming decades. This paper will lay a foundation for the issues planners and policymakers should begin to consider when deciding whether or not to embrace autonomous vehicles.},
	issue = {May},
	author = {Asher, Isaac},
	date = {2014},
	keywords = {{AV}, future},
}

@article{schmidt_entwicklung_2015,
	title = {Entwicklung einer Methodik zur Bewertung bestehender Infrastruktur hinsichtlich der Nutzbarkeit für autonome Busse als Zubringer zum {SPNV}},
	issue = {November},
	author = {Schmidt, Frederic},
	date = {2015},
	keywords = {ma},
}

@article{european_commission_connected_2018,
	title = {Connected and Automated Transport - Studies and reports},
	url = {http://ec.europa.eu/programmes/horizon2020/en/news/connected-and-automated-transport-expert-group-report},
	author = {{European Commission}},
	date = {2018},
}

@article{international_transport_forum__oecd_automated_2015,
	title = {Automated and Autonomous Driving. Regulation under uncertainty},
	abstract = {Many cars sold today are already capable of some level of automated operation, and prototype cars capable of driving autonomously have been - and continue to be - tested on public roads in Europe, Japan and the United States. These technologies have arrived rapidly on the market and their future deployment is expected to accelerate. Autonomous driving promises many benefits: improved safety, reduced congestion and lower stress for car occupants, among others. Authorities will have to adapt existing rules and create new ones in order to ensure the full compatibility of these vehicles with the public’s expectations regarding safety, legal responsibility and privacy. This report explores the strategic issues that will have to be considered by authorities as more fully automated and ultimately autonomous vehicles arrive on our streets and roads. It was drafted on the basis of expert input and discussions amongst project partners in addition to a review of relevant published research and position papers.},
	pages = {1--32},
	author = {{International Transport Forum / OECD}},
	date = {2015},
	note = {{ISBN}: 1100100011001},
}

@article{lee_autonomous_2020,
	title = {Autonomous Vehicle Implementation Predictions: Implications for Transport Planning},
	volume = {42},
	issn = {10769757},
	abstract = {This report explores the impacts of autonomous (also called self-driving, driverless or robotic) vehicles, and their implications for transportation planning. It investigates how quickly such vehicles are likely to develop and be deployed based on experience with previous vehicle technologies; their likely benefits and costs; how they will affect travel activity; and their impacts on planning decisions such as optimal road, parking and public transit supply. This analysis indicates that Level 5 autonomous vehicles, able to operate without a driver, may be commercially available and legal to use in some jurisdictions by the late 2020s, but will initially have high costs and limited performance. Some benefits, such as independent mobility for affluent non-drivers, may begin in the 2030s or 2040s, but most impacts, including reduced traffic and parking congestion, independent mobility for low-income people (and therefore reduced need for public transit), increased safety, energy conservation and pollution reductions, will only be significant when autonomous vehicles become common and affordable, probably in the 2050s to 2060s, and some benefits may require prohibiting human-driven vehicles on certain roadways, which could take even longer.},
	pages = {1--39},
	number = {5},
	journaltitle = {Transportation Research Board Annual Meeting},
	author = {Lee, Dave},
	date = {2020},
	pmid = {17255001},
	eprinttype = {arxiv},
	eprint = {cs/9605103},
	note = {{ISBN}: 0-7803-3213-X},
}